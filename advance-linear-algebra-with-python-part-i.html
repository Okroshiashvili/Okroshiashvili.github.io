<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://dsfabric.org/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://dsfabric.org/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Nodar Okroshiashvili" />

        <meta name="description" content="This is the first part of advance linear algebra with Python
" />
        <meta name="twitter:creator" content="@N_Okroshiashvil">
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Linear Algebra, Advance Topics, Mathematics, Mathematics, eigenvectors and eigenvalues in python, gauss and gauss-jordan elimination with python, matrix image and kernel, advances of linear algebra with python, linear algebra advances with python, numpy, scipy" />

<meta property="og:title" content="Advance Linear Algebra with Python - Part I "/>
<meta property="og:url" content="https://dsfabric.org/advance-linear-algebra-with-python-part-i" />
<meta property="og:description" content="This is the first part of advance linear algebra with Python" />
<meta property="og:site_name" content="Data Science Fabric" />
<meta property="og:article:author" content="Nodar Okroshiashvili" />
<meta property="og:article:published_time" content="2019-03-04T12:14:00+04:00" />
<meta name="twitter:title" content="Advance Linear Algebra with Python - Part I ">
<meta name="twitter:description" content="This is the first part of advance linear algebra with Python">

        <title>Advance Linear Algebra with Python - Part I  Â· Data Science Fabric
</title>
        <link rel="shortcut icon" href="https://dsfabric.org/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="https://dsfabric.org/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://dsfabric.org/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://dsfabric.org/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://dsfabric.org/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://dsfabric.org/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://dsfabric.org/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://dsfabric.org/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://dsfabric.org/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://dsfabric.org/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://dsfabric.org/theme/images/apple-touch-icon-180x180.png" type="image/png" />
        <link href="https://dsfabric.org/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Data Science Fabric - Full Atom Feed" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-136307659-1', 'auto');
    ga('send', 'pageview');
</script>


    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://dsfabric.org/"><span class=site-name>Data Science Fabric</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://dsfabric.org
                                    >Home</a>
                                </li>
                                <li ><a href="https://dsfabric.org/categories">Categories</a></li>
                                <li ><a href="https://dsfabric.org/tags">Tags</a></li>
                                <li ><a href="https://dsfabric.org/archives">Archives</a></li>
                                <li><form class="navbar-search" action="https://dsfabric.org/search" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://dsfabric.org/advance-linear-algebra-with-python-part-i">
                Advance Linear Algebra with Python - Part I
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <p>This is the first part of the fourth and last post in blog series about linear algebra. </p>
<ol>
<li>Introduction</li>
<li>Basics of linear algebra</li>
<li>Intermediate linear algebra</li>
<li><strong>Advances in linear algebra</strong>: <strong>Part I</strong> and Part II</li>
</ol>
<p>In this post I will introduce you to the advances of linear algebra, which in turn includes the following:</p>
<ul>
<li><a href="#vector">Vector</a></li>
<li><a href="#basis-vectors">Basis Vectors</a></li>
<li><a href="#matrix">Matrix</a></li>
<li><a href="#gaussian-elimination-of-a-matrix">Gaussian Elimination of a Matrix</a></li>
<li><a href="#gauss-jordan-elimination-of-a-matrix">Gauss-Jordan Elimination of a Matrix</a></li>
<li><a href="#the-inverse-of-a-matrix-using-gauss-jordan-elimination">The Inverse of a Matrix Using Gauss-Jordan Elimination</a></li>
<li><a href="#image-of-a-matrix">Image of a Matrix</a></li>
<li><a href="#kernel-of-a-matrix">Kernel of a Matrix</a></li>
<li><a href="#rank-of-a-matrix">Rank of a Matrix</a></li>
<li><a href="#find-the-basis-of-a-matrix">Find the Basis of a Matrix</a></li>
<li><a href="#transformations">Transformations</a><ul>
<li><a href="#linear-transformation">Linear Transformation</a></li>
<li><a href="#transformations-of-magnitude-and-amplitude">Transformations of Magnitude and Amplitude</a></li>
<li><a href="#affine-transformation">Affine Transformation</a></li>
</ul>
</li>
<li><a href="#eigenvalues">Eigenvalues</a></li>
<li><a href="#eigenvectors">Eigenvectors</a></li>
<li><a href="#spectrum-and-spectral-radius">Spectrum and Spectral Radius</a></li>
<li><a href="#numerical-representation">Numerical Representation</a><ul>
<li><a href="#kernel-or-null-space-of-a-matrix">Kernel or Null Space of a Matrix</a></li>
<li><a href="#linear-transformations">Linear Transformations</a></li>
<li><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></li>
</ul>
</li>
<li><a href="#conclusion-for-part-i">Conclusion for part I</a></li>
<li><a href="#references">References</a><ul>
<li><a href="#vector-1">Vector</a></li>
<li><a href="#matrix-1">Matrix</a></li>
</ul>
</li>
</ul>
<h2 id="vector">Vector<a class="headerlink" href="#vector" title="Permanent link">&para;</a></h2>
<p><a id="Vector"></a></p>
<h3 id="basis-vectors">Basis Vectors<a class="headerlink" href="#basis-vectors" title="Permanent link">&para;</a></h3>
<p><a id="Basis_Vectors"></a></p>
<hr>
<p>In the <a href="https://dsfabric.org/articles/mathematics/basics-of-linear-algebra.html">basics</a>, we saw what is a unit vector. 
To refresh, the unit vector is the vector with length 1 and the formula is</p>
<div class="math">$$
\hat{X} = \frac{X}{\|X\|}
$$</div>
<p>For farther explanation, unit vectors can be used to represent the axes of a <a href="https://en.wikipedia.org/wiki/Cartesian_coordinate_system">Cartesian coordinate system</a>. 
For example in a three-dimensional Cartesian coordinate system such vectors are:</p>
<div class="math">$$
\hat{i} =
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
\quad
\hat{j} =
\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}
\quad
\hat{k} =
\begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}
$$</div>
<p>which represents, <span class="math">\(x\)</span>, <span class="math">\(y\)</span>, and <span class="math">\(z\)</span> axes, respectively.</p>
<p>For two dimensional space we have</p>
<div class="math">$$
\hat{i} =
\begin{bmatrix}
1 \\
0
\end{bmatrix}
\quad
\hat{j} =
\begin{bmatrix}
0 \\
1
\end{bmatrix}
$$</div>
<p>Let deal with two-dimensional space to catch the idea of basis easily and then generalize this idea for higher dimensions. 
Imagine, we have vector space or collection of vectors <span class="math">\(\vec{V}\)</span> over the Cartesian coordinate system. 
This space includes all two-dimensional vectors, or in other words, vectors with only two elements, <span class="math">\(x\)</span>, and <span class="math">\(y\)</span>.</p>
<blockquote>
<p>A <strong>basis</strong>, call it <span class="math">\(B\)</span>, of vector space <span class="math">\(V\)</span> over the Cartesian coordinate system is a linearly independent subset of <span class="math">\(V\)</span> that spans whole vector space <span class="math">\(V\)</span>. To be precise, basis <span class="math">\(B\)</span> to be the basis it must satisfy two conditions:</p>
</blockquote>
<ul>
<li>
<p>Linearly independence property - states that all vectors in <span class="math">\(B\)</span> are linearly independent</p>
</li>
<li>
<p>The spanning property - states that <span class="math">\(B\)</span> spans whole <span class="math">\(V\)</span></p>
</li>
</ul>
<p>We can combine these two conditions in one sentence. <span class="math">\(B\)</span> is the basis if its all elements are linearly independent and every 
element of <span class="math">\(V\)</span> is a linear combination of elements of <span class="math">\(B\)</span>.</p>
<p>From these conditions, we can conclude that unit vectors <span class="math">\(\hat{i}\)</span> and <span class="math">\(\hat{j}\)</span> are the basis of <span class="math">\(\mathbb{R^2}\)</span>. 
This kind of bases are also called <strong>standard basis</strong> or <strong>natural basis</strong>. The standard basis are denoted 
by <span class="math">\(e_{1}\)</span>, <span class="math">\(e_{2}\)</span>, <span class="math">\(e_{3}\)</span> and so on. I will be consistent and use the later notation for standard basis 
and <span class="math">\(\hat{i}\)</span>, <span class="math">\(\hat{j}\)</span> and <span class="math">\(\hat{k}\)</span> for unit vectors.</p>
<p>These standard basis vectors are the basis in the sense that any other vector in <span class="math">\(V\)</span> can be expressed uniquely 
as a linear combination of these unit vectors. For example, every vector <span class="math">\(v\)</span> in two-dimensional space can be written as</p>
<div class="math">$$
x\ e_{1} + y\ e_{2}
$$</div>
<p>where <span class="math">\(e_{1}\)</span> and <span class="math">\(e_{2}\)</span> are unit vectors and <span class="math">\(x\)</span> and <span class="math">\(y\)</span> are scalar components or elements of the vector <span class="math">\(v\)</span>.</p>
<p>Now, to generalize the idea for higher dimensions we just have to apply the same logic as above, 
for <span class="math">\(\mathbb{R^3}\)</span> and more. In <span class="math">\(\mathbb{R^3}\)</span> we have standard basis vectors <span class="math">\(e_{1}\)</span>, <span class="math">\(e_{2}\)</span>, <span class="math">\(e_{3}\)</span>, 
and generally for <span class="math">\(\mathbb{R^n}\)</span> we have standard basis vector space</p>
<div class="math">$$
E = 
\begin{bmatrix}
e_{1} \\
e_{2} \\
\cdots \\
e_{n}
\end{bmatrix}
$$</div>
<p>To generalize the definition of basis further let consider the following:</p>
<p><strong>If elements <span class="math">\(\{v_{1}, v_{2},\cdots,v_{n}\}\)</span> of <span class="math">\(V\)</span> generate <span class="math">\(V\)</span> and in addition they are linearly independent, then <span class="math">\(\{v_{1}, v_{2},\cdots,v_{n}\}\)</span> is called a basis of <span class="math">\(V\)</span>. We shall say that the elements <span class="math">\(v_{1}, v_{2},\cdots,v_{n}\)</span> constitute or form a basis of V.</strong> 
Vector space <span class="math">\(V\)</span> can have several basis.</p>
<p>At this stage, the notion of basis seems very abstract even for me, and believe me it was totally unclear for me until I 
solved some examples by hand. I'll show you how to compute basis after explaining row-echelon and reduced row-echelon 
forms and you'll understand it. However, it's not enough only to know how to row-reduce the given matrix. 
It's necessary to know which basis you want. Either column space or row space basis or the basis for nullspace. 
These notions are explained below and after that, we can find the basis for each of them.</p>
<h2 id="matrix">Matrix<a class="headerlink" href="#matrix" title="Permanent link">&para;</a></h2>
<p><a id="Matrix"></a></p>
<h3 id="gaussian-elimination-of-a-matrix">Gaussian Elimination of a Matrix<a class="headerlink" href="#gaussian-elimination-of-a-matrix" title="Permanent link">&para;</a></h3>
<p><a id="Gaussian_Elimination_of_a_Matrix"></a></p>
<hr>
<p>In linear algebra, Gaussian Elimination is the method to solve the system of linear equations. 
This method is the sequence of operations performed on the coefficient matrix of the system. 
Except for solving the linear systems, the method can be used to find the rank of a matrix, the determinant as well 
as the inverse of a square invertible matrix. </p>
<p>And what is the sequence of operations?</p>
<p>Under this notion, elementary row operations are meant. We've covered it in the previous post but for the refresher, ERO's are:</p>
<ul>
<li>
<p>Interchange rows</p>
</li>
<li>
<p>Multiply each element in a row by a non-zero number</p>
</li>
<li>
<p>Multiply a row by a non-zero number and add the result to another row</p>
</li>
</ul>
<p>Performing Gaussian elimination results in the matrix in <strong>Row Echelon Form</strong> (ref). 
The matrix is said to be in row echelon form if it satisfies the following conditions:</p>
<ul>
<li>
<p>The first non-zero element in each row, called the leading entry, is a 1</p>
</li>
<li>
<p>Each leading entry is in a column, which is the right side of the leading entry in the previous row</p>
</li>
<li>
<p>Below the leading entry in a column, all other entries are zero</p>
</li>
</ul>
<p>To catch the idea of this process, let consider the example. Actually, we have no matrix (not necessarily true), 
we have the system of linear equations in the following form:</p>
<div class="math">$$
\begin{cases}
x + 2y - z = 5\\
3x + y - 2z = 9\\
-x + 4y + 2z = 0
\end{cases}
$$</div>
<p>Based on these equations we can form the following matrix</p>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 \\
3 &amp; 1 &amp; -2 \\
-1 &amp; 4 &amp; 2
\end{bmatrix}
$$</div>
<p>This matrix is called <strong>coefficient matrix</strong> as it contains the coefficients of the linear equations. 
Having the coefficient matrix, we can rewrite our system in the following form:</p>
<div class="math">$$
Ax = b
$$</div>
<p>Where <span class="math">\(A\)</span> is the coefficient matrix, <span class="math">\(x\)</span> is the vector of the unknowns, and <span class="math">\(b\)</span> is the vector of the right-hand side components</p>
<p>To solve this simultaneous system, the coefficients matrix is not enough. We need something more, on which we can perform ELO's. This matrix is:</p>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
3 &amp; 1 &amp; -2 &amp; |&amp; 9 \\
-1 &amp; 4 &amp; 2 &amp; |&amp; 0
\end{bmatrix} = [A | b]
$$</div>
<p>which is called <strong>augmented matrix</strong>, which in turn gives us the possibility to perform ELO's, in other words, 
we do Gaussian elimination and the resulted matrix will be in row echelon form. Using back substitution on the 
resulted matrix gives the solution to our system of equations.</p>
<p>Let do it by hand. We have the initial system</p>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
3 &amp; 1 &amp; -2 &amp; |&amp; 9 \\
-1 &amp; 4 &amp; 2 &amp; |&amp; 0
\end{bmatrix}
\equiv
\begin{cases}
x + 2y - z = 5 \\
3x + y - 2z = 9 \\
-x + 4y + 2z = 0
\end{cases}
$$</div>
<p>Then, using ERO's</p>
<ol>
<li><span class="math">\(R3 \rightarrow R3 + R1\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
3 &amp; 1 &amp; -2 &amp; |&amp; 9 \\
0 &amp; 6 &amp; 1 &amp; |&amp; 5
\end{bmatrix}
\equiv
\begin{cases}
x + 2y - z = 5 \\
3x + y - 2z = 9 \\
6y + z = 5
\end{cases}
$$</div>
<ol>
<li><span class="math">\(R2 \rightarrow R2 - 3R1\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
0 &amp; -5 &amp; 1 &amp; |&amp; -6 \\
0 &amp; 6 &amp; 1 &amp; |&amp; 5
\end{bmatrix}
\equiv
\begin{cases}
x + 2y - z = 5 \\
-5y + z = -6 \\
6y + z = 5
\end{cases}
$$</div>
<ol>
<li><span class="math">\(R2 \rightarrow R2 + R3\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
0 &amp; 1 &amp; 2 &amp; |&amp; -1 \\
0 &amp; 6 &amp; 1 &amp; |&amp; 5
\end{bmatrix}
\equiv
\begin{cases}
x + 2y - z = 5 \\
y + 2z = -1 \\
6y + z = 5
\end{cases}
$$</div>
<ol>
<li><span class="math">\(R3 \rightarrow R3 - 6R2\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
0 &amp; 1 &amp; 2 &amp; |&amp; -1 \\
0 &amp; 0 &amp; -11 &amp; |&amp; 11
\end{bmatrix}
\equiv
\begin{cases}
x + 2y - z = 5 \\
y + 2z = -1 \\
-11z = 11
\end{cases}
$$</div>
<ol>
<li><span class="math">\(R3 \rightarrow R3 \cdot -\frac{1}{11}\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
0 &amp; 1 &amp; 2 &amp; |&amp; -1 \\
0 &amp; 0 &amp; 1 &amp; |&amp; -1
\end{bmatrix}
\equiv
\begin{cases}
x + 2y - z = 5 \quad (A)\\
y + 2z = -1 \quad (B)\\
z = -1 \quad (C)
\end{cases}
$$</div>
<ol>
<li>Back substitution</li>
</ol>
<div class="math">$$
\begin{cases}
(C) \quad z = -1 \\
(B) \quad y = -1 - 2z \quad \Rightarrow \quad y = -1 - 2(-1) = 1 \\
(A) \quad x = 5 - 2y + z \quad \Rightarrow \quad x = 5 - 2(1) + (-1) = 2
\end{cases}
$$</div>
<ol>
<li>Solution</li>
</ol>
<div class="math">$$
x = 2 \\
y = 1 \\
x = -1
$$</div>
<p>This is the solution of the initial system, as well as the last system and every intermediate system. 
The matrix obtained in step 5 above is in Row-Echelon form as it satisfied above-mentioned conditions.</p>
<p><strong>Note that, starting with a particular matrix, a different sequence of ERO's can lead to different row echelon form</strong></p>
<h3 id="gauss-jordan-elimination-of-a-matrix">Gauss-Jordan Elimination of a Matrix<a class="headerlink" href="#gauss-jordan-elimination-of-a-matrix" title="Permanent link">&para;</a></h3>
<p><a id="Gauss_Jordan_Elimination_of_a_Matrix"></a></p>
<hr>
<p>Gaussian elimination performs row operations to produce zeros below the main diagonal of the coefficient matrix to 
reduce it to row echelon form. Once it's done we perform back substitution to find the solution. However, we can 
continue performing ERO's to reduce coefficient matrix farther, to produce <strong>Reduced Row Echelon Form</strong> (rref). 
The matrix is in reduced row echelon form if it satisfies the following conditions:</p>
<ul>
<li>
<p>It is in row echelon form</p>
</li>
<li>
<p>The leading entry in each row is the only non-zero entry in its column</p>
</li>
</ul>
<p>Gauss-Jordan elimination starts when Gauss elimination left off. Loosely speaking, Gaussian elimination works from the 
top down and when it stops, we start Gauss-Jordan elimination from the bottom up. The Reduced Row Echelon 
Form matrix is the result of Gauss-Jordan Elimination process.</p>
<p>We can continue our example from step 5 and see what is Gauss-Jordan elimination. At step 5 we had</p>
<ol>
<li><span class="math">\(R3 \rightarrow R3 \cdot -\frac{1}{11}\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
0 &amp; 1 &amp; 2 &amp; |&amp; -1 \\
0 &amp; 0 &amp; 1 &amp; |&amp; -1
\end{bmatrix}
\equiv
\begin{cases}
x + 2y - z = 5 \\
y + 2z = -1 \\
z = -1
\end{cases}
$$</div>
<p>Now, from bottom to up we perform the following ERO's</p>
<ol>
<li><span class="math">\(R2 \rightarrow R2 - 2R3\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 5 \\
0 &amp; 1 &amp; 0 &amp; |&amp; 1 \\
0 &amp; 0 &amp; 1 &amp; |&amp; -1
\end{bmatrix}
\equiv
\begin{cases}
x + 2y - z = 5 \\
y  = 1 \\
z = -1
\end{cases}
$$</div>
<ol>
<li><span class="math">\(R1 \rightarrow R1 + R3\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; |&amp; 4 \\
0 &amp; 1 &amp; 0 &amp; |&amp; 1 \\
0 &amp; 0 &amp; 1 &amp; |&amp; -1
\end{bmatrix}
\equiv
\begin{cases}
x + 2y = 4 \\
y  = 1 \\
z = -1
\end{cases}
$$</div>
<ol>
<li><span class="math">\(R1 \rightarrow R1 - 2R2\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; |&amp; 2 \\
0 &amp; 1 &amp; 0 &amp; |&amp; 1 \\
0 &amp; 0 &amp; 1 &amp; |&amp; -1
\end{bmatrix}
\equiv
\begin{cases}
x = 2 \\
y = 1 \\
z = -1
\end{cases}
$$</div>
<p>The solution is</p>
<div class="math">$$
x = 2 \\
y = 1 \\
x = -1
$$</div>
<p>and this is the same as the solution of the Gauss elimination. The matrix in step 8 is the 
Reduced Row Echelon Form of our initial coefficient matrix <span class="math">\(A\)</span>.</p>
<h3 id="the-inverse-of-a-matrix-using-gauss-jordan-elimination">The Inverse of a Matrix Using Gauss-Jordan Elimination<a class="headerlink" href="#the-inverse-of-a-matrix-using-gauss-jordan-elimination" title="Permanent link">&para;</a></h3>
<p><a id="The_Inverse_of_a_Matrix_Using_Gauss_Jordan_Elimination"></a></p>
<hr>
<p>Suppose, we have given the matrix and want to find its inverse but do not want to use the technique mentioned 
in the intermediate post. We can use Gauss-Jordan elimination with little modification to find the inverse of a matrix. 
To be consistent, I use the above coefficient matrix but not the right-hand side of the system. So, our matrix is</p>
<div class="math">$$
A =
\begin{bmatrix}
1 &amp; 2 &amp; -1 \\
3 &amp; 1 &amp; -2 \\
-1 &amp; 4 &amp; 2
\end{bmatrix}
$$</div>
<p>To find the inverse of <span class="math">\(A\)</span>, we need to augment <span class="math">\(A\)</span> by the identity matrix <span class="math">\(I\)</span> which has the same dimensions as <span class="math">\(A\)</span>. 
It is a must the identity to have the same dimensions. After augmentation we have</p>
<div class="math">$$
[A | I] =
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 1 &amp; 0 &amp; 0 \\
3 &amp; 1 &amp; -2 &amp; |&amp; 0 &amp; 1 &amp; 0 \\
-1 &amp; 4 &amp; 2 &amp; |&amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
$$</div>
<p>We have to perform elementary row operations in the same way as we did in the above example. Particularly,</p>
<ol>
<li><span class="math">\(R3 \rightarrow R3 + R1\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 1 &amp; 0 &amp; 0\\
3 &amp; 1 &amp; -2 &amp; |&amp; 0 &amp; 1 &amp; 0\\
0 &amp; 6 &amp; 1 &amp; |&amp; 1 &amp; 0 &amp; 1
\end{bmatrix}
$$</div>
<ol>
<li><span class="math">\(R2 \rightarrow R2 - 3R1\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 1 &amp; 0 &amp; 0\\
0 &amp; -5 &amp; 1 &amp; |&amp; -3 &amp; 1 &amp; -3\\
0 &amp; 6 &amp; 1 &amp; |&amp; 1 &amp; 0 &amp; 1
\end{bmatrix}
$$</div>
<ol>
<li><span class="math">\(R2 \rightarrow R2 + R3\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 2 &amp; |&amp; -2 &amp; 1 &amp; -2\\
0 &amp; 6 &amp; 1 &amp; |&amp; 1 &amp; 0 &amp; 1
\end{bmatrix}
$$</div>
<ol>
<li><span class="math">\(R3 \rightarrow R3 - 6R2\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 2 &amp; |&amp; -2 &amp; 1 &amp; -2\\
0 &amp; 0 &amp; -11 &amp; |&amp; 13 &amp; -6 &amp; 13
\end{bmatrix}
$$</div>
<ol>
<li><span class="math">\(R3 \rightarrow R3 \cdot -\frac{1}{11}\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 2 &amp; |&amp; -2 &amp; 1 &amp; -2\\
0 &amp; 0 &amp; 1 &amp; |&amp; -\frac{13}{11} &amp; \frac{6}{11} &amp; -\frac{13}{11}
\end{bmatrix}
$$</div>
<ol>
<li><span class="math">\(R2 \rightarrow R2 - 2R3\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; -1 &amp; |&amp; 1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; |&amp; \frac{4}{11} &amp; -\frac{9}{100} &amp; \frac{4}{11}\\
0 &amp; 0 &amp; 1 &amp; |&amp; -\frac{13}{11} &amp; \frac{6}{11} &amp; -\frac{13}{11}
\end{bmatrix}
$$</div>
<ol>
<li><span class="math">\(R1 \rightarrow R1 + R3\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; |&amp; -\frac{2}{11} &amp; \frac{6}{11} &amp; -\frac{13}{11}\\
0 &amp; 1 &amp; 0 &amp; |&amp; \frac{4}{11} &amp; -\frac{9}{100} &amp; \frac{4}{11}\\
0 &amp; 0 &amp; 1 &amp; |&amp; -\frac{13}{11} &amp; \frac{6}{11} &amp; -\frac{13}{11}
\end{bmatrix}
$$</div>
<ol>
<li><span class="math">\(R1 \rightarrow R1 - 2R2\)</span></li>
</ol>
<div class="math">$$
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; |&amp; -\frac{10}{11} &amp; \frac{18}{25} &amp; -\frac{21}{11}\\
0 &amp; 1 &amp; 0 &amp; |&amp; \frac{4}{11} &amp; -\frac{9}{100} &amp; \frac{4}{11}\\
0 &amp; 0 &amp; 1 &amp; |&amp; -\frac{13}{11} &amp; \frac{6}{11} &amp; -\frac{13}{11}
\end{bmatrix}
$$</div>
<p>Our inverse of <span class="math">\(A\)</span> is</p>
<div class="math">$$
A^{-1} =
\begin{bmatrix}
-\frac{10}{11} &amp; \frac{18}{25} &amp; -\frac{21}{11}\\
\frac{4}{11} &amp; -\frac{9}{100} &amp; \frac{4}{11}\\
-\frac{13}{11} &amp; \frac{6}{11} &amp; -\frac{13}{11}
\end{bmatrix}
$$</div>
<h3 id="image-of-a-matrix">Image of a Matrix<a class="headerlink" href="#image-of-a-matrix" title="Permanent link">&para;</a></h3>
<p><a id="Image_of_a_Matrix"></a></p>
<hr>
<p>Let <span class="math">\(A\)</span> be <span class="math">\(m\times n\)</span> matrix. Space spanned by its column vectors are called range, image, or column space of a matrix <span class="math">\(A\)</span>. 
The row space is defined similarly. I only consider column space as all the logic is the same for row space.</p>
<p>The precise definition is the following:</p>
<p>Let <span class="math">\(A\)</span> be an <span class="math">\(m\times n\)</span> matrix, with column vectors <span class="math">\(v_{1}, v_{2}, \cdots, v_{n}\)</span>. A linear combination of these 
vectors is any vector of the following form: <span class="math">\(c_{1}v_{1} + c_{2}v_{2} + \cdots + c_{n}v_{n}\)</span>, where <span class="math">\(c_{1}, c_{2}, \cdots , c_{n}\)</span> are scalars. 
The set of all possible linear combinations of <span class="math">\(v_{1}, v_{2}, \cdots , v_{n}\)</span> is called the column space of <span class="math">\(A\)</span>.</p>
<p>For example:</p>
<div class="math">$$
A =
\begin{bmatrix}
1 &amp; 0\\
0 &amp; 1\\
2 &amp; 0
\end{bmatrix}
$$</div>
<p>Column vectors are:</p>
<div class="math">$$
v_{1} =
\begin{bmatrix}
1\\
0\\
2
\end{bmatrix}
\quad
v_{2} =
\begin{bmatrix}
0\\
1\\
0
\end{bmatrix}
$$</div>
<p>A linear combination of <span class="math">\(v_{1}\)</span> and <span class="math">\(v_{2}\)</span> is any vector of the form</p>
<div class="math">$$
c_{1}
\begin{bmatrix}
1\\
0\\
2
\end{bmatrix} + c_{2}
\begin{bmatrix}
0\\
1\\
0
\end{bmatrix} = 
\begin{bmatrix}
c_{1}\\
c_{2}\\
2c_{1}
\end{bmatrix}
$$</div>
<p>The set of all such vectors is the column space of <span class="math">\(A\)</span>.</p>
<h3 id="kernel-of-a-matrix">Kernel of a Matrix<a class="headerlink" href="#kernel-of-a-matrix" title="Permanent link">&para;</a></h3>
<p><a id="Kernel_of_a_Matrix"></a></p>
<hr>
<p>In linear algebra, the kernel or a.k.a null space is the solution of the following homogeneous system:</p>
<div class="math">$$A\cdot X = 0$$</div>
<p>where <span class="math">\(A\)</span> is a <span class="math">\(m\times n\)</span> matrix and <span class="math">\(X\)</span> is a <span class="math">\(m\times 1\)</span> vector and is denoted by <span class="math">\(Ker(A)\)</span>.</p>
<p>For more clarity, let consider the numerical example. Lat our matrix <span class="math">\(A\)</span> be the following:</p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 7 &amp; 1 &amp; 3 \\
-4 &amp; -2 &amp; 2 &amp; -2 \\
-1 &amp; 7 &amp; 3 &amp; 2 \\
-2 &amp; 2 &amp; 2 &amp; 0 \\
\end{bmatrix}
$$</div>
<p>and our <span class="math">\(X\)</span> is</p>
<div class="math">$$
X =
\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4} \\
\end{bmatrix}
$$</div>
<p>We have to form the following system:</p>
<div class="math">$$
A\cdot X =
\begin{bmatrix}
2 &amp; 7 &amp; 1 &amp; 3 \\
-4 &amp; -2 &amp; 2 &amp; -2 \\
-1 &amp; 7 &amp; 3 &amp; 2 \\
-2 &amp; 2 &amp; 2 &amp; 0 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4} \\
\end{bmatrix} = 
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}
$$</div>
<p>After that, we have to put this system into row-echelon or reduced row-echelon form. 
Let skip detailed calculation and present only results, which is the last matrix.</p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 7 &amp; 1 &amp; 3\\
-4 &amp; -2 &amp; 2 &amp; -2\\
-1 &amp; 7 &amp; 3 &amp; 2\\
-2 &amp; 2 &amp; 2 &amp; 0
\end{bmatrix}
\rightarrow
\begin{bmatrix}
-1 &amp; 7 &amp; 3 &amp; 2\\
-4 &amp; -2 &amp; 2 &amp; -2\\
2 &amp; 7 &amp; 1 &amp; 3\\
-2 &amp; 2 &amp; 2 &amp; 0
\end{bmatrix} 
\rightarrow
\begin{bmatrix}
-1 &amp; 7 &amp; 3 &amp; 2\\
0 &amp; -30 &amp; -10 &amp; -10\\
0 &amp; 21 &amp; 7 &amp; 7\\
0 &amp; -12 &amp; -4 &amp; -4
\end{bmatrix}
\rightarrow
\begin{bmatrix}
-1 &amp; 7 &amp; 3 &amp; 2\\
0 &amp; 3 &amp; 1 &amp; 1\\
0 &amp; 3 &amp; 1 &amp; 1\\
0 &amp; 3 &amp; 1 &amp; 1
\end{bmatrix}
\rightarrow
\begin{bmatrix}
-1 &amp; 7 &amp; 3 &amp; 2\\
0 &amp; 3 &amp; 1 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}
$$</div>
<p>Now, to find the kernel of the original matrix <span class="math">\(A\)</span>, we have to solve the following system of equations:</p>
<div class="math">$$
\begin{cases}
x_{1} - 7x_{2} - 3x_{3} - 2x_{4} = 0 \\
        3x_{2} + x_{3} + x_{4} = 0
\end{cases}
\rightarrow
\begin{cases}
x_{1} = \frac{2}{3}x_{3} - \frac{1}{3}x_{4}\\
x_{2} = -\frac{1}{3}x_{3} - \frac{1}{3}x_{4}
\end{cases}
$$</div>
<p>From this solution we conclude that the kernel of <span class="math">\(A\)</span> is</p>
<div class="math">$$
Ker(A) = 
\begin{bmatrix}
x_{1} = \frac{2}{3}x_{3} - \frac{1}{3}x_{4} \\
x_{2} = -\frac{1}{3}x_{3} - \frac{1}{3}x_{4} \\
x_{3} \\
x_{4}
\end{bmatrix}
$$</div>
<p>Where, <span class="math">\(x_{3}\)</span> and <span class="math">\(x_{4}\)</span> are free variables and can be any number in <span class="math">\(R\)</span></p>
<p>Note, that both original matrix <span class="math">\(A\)</span> and its row-echelon for has the same kernel. This means that row reduction preserves the kernel or null space.</p>
<h3 id="rank-of-a-matrix">Rank of a Matrix<a class="headerlink" href="#rank-of-a-matrix" title="Permanent link">&para;</a></h3>
<p><a id="Rank_of_a_Matrix"></a></p>
<hr>
<p>In the intermediate tutorial, you saw how to calculate the determinant of a matrix and also saw that any non zero determinant 
of sub-matrix of the original matrix shows its non-degenerateness. In other words, nonzero determinant gives us information 
about the rank of the matrix. Also, I said that there was not only one way to find the rank of a matrix. 
After reviewing Gauss and Gauss-Jordan Elimination and Row-Echelon and Reduced Row-Echelon forms you know that 
indeed there are other ways to find the determinant of a matrix as well as the rank of the matrix. 
To keep <a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY</a>, here I only consider a numerical example. The code is provided in the intermediate tutorial.</p>
<p>Suppose we have matrix <span class="math">\(A\)</span> in the following form:</p>
<div class="math">$$
A =
\begin{bmatrix}
3 &amp; 2 &amp; -1\\
2 &amp; -3 &amp; -5\\
-1 &amp; -4 &amp;- 3
\end{bmatrix}
$$</div>
<p>Perform Elementary Row Operations we get reduced-echelon form:</p>
<div class="math">$$
A = 
\begin{bmatrix}
3 &amp; 2 &amp; -1\\
2 &amp; -3 &amp; -5\\
-1 &amp; -4 &amp; -3
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 &amp; 4 &amp; 3\\
3 &amp; 2 &amp; -1\\
2 &amp; -3 &amp; -5
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 &amp; 4 &amp; 3\\
0 &amp; -10 &amp; -10\\
0 &amp; -11 &amp; -11
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 &amp; 4 &amp; 3\\
0 &amp; 1 &amp; 1\\
0 &amp; -11 &amp; -11
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 &amp; 4 &amp; 3\\
0 &amp; 1 &amp; 1\\
0 &amp; 0 &amp; 0
\end{bmatrix}
$$</div>
<p>From the last matrix we see that the nonzero determinant only exists in <span class="math">\(2\times2\)</span> sub-matrices, hence rank of the matrix <span class="math">\(A\)</span> is 2.</p>
<h3 id="find-the-basis-of-a-matrix">Find the Basis of a Matrix<a class="headerlink" href="#find-the-basis-of-a-matrix" title="Permanent link">&para;</a></h3>
<p><a id="Find_the_Basis_of_a_Matrix"></a></p>
<hr>
<p>Now we are able to find the basis for column space and row space as well as the basis for the kernel. 
The columns of a matrix <span class="math">\(A\)</span> span the column space but they may not form a basis if the column vectors are linearly dependent. 
If this is the case, only some subset of these vectors forms the basis. To find the basis for column space we reduce matrix <span class="math">\(A\)</span> to reduced row-echelon form.</p>
<p>For example:</p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 7 &amp; 1 &amp; 3 \\
-4 &amp; -2 &amp; 2 &amp; -2 \\
-1 &amp; 7 &amp; 3 &amp; 2 \\
-2 &amp; 2 &amp; 2 &amp; 0 \\
\end{bmatrix}
$$</div>
<p>Row reduced form of <span class="math">\(A\)</span> is:</p>
<div class="math">$$
B =
\begin{bmatrix}
-1 &amp; 7 &amp; 3 &amp; 2\\
0 &amp; 3 &amp; 1 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}
$$</div>
<p>We see that only column 1 and column 2 are linearly independent in reduced form and hence column 1 and column 2 of the original matrix <span class="math">\(A\)</span> form the basis, which is:</p>
<div class="math">$$
\begin{bmatrix}
2 \\-4\\-1\\-2
\end{bmatrix}
\quad
\text{and}
\quad
\begin{bmatrix}
7\\-2 \\ 7 \\ 2
\end{bmatrix}
$$</div>
<p>To find the basis for row space, let consider different matrix and again let it be <span class="math">\(A\)</span>.</p>
<div class="math">$$
A =
\begin{bmatrix}
1 &amp; 3 &amp; 2 \\
2 &amp; 7 &amp; 4 \\
1 &amp; 5 &amp; 2
\end{bmatrix}
$$</div>
<p>To reduce <span class="math">\(A\)</span> to reduced row-echelon form we have:</p>
<div class="math">$$
B =
\begin{bmatrix}
1 &amp; 0 &amp; 2 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{bmatrix}
$$</div>
<p>As in column space case, we see that linearly independent, nonzero row vectors are</p>
<div class="math">$$
\begin{bmatrix}
1 \\0\\2
\end{bmatrix}
\quad
\text{and}
\quad
\begin{bmatrix}
0\\1 \\ 0
\end{bmatrix}
$$</div>
<p>To find the basis for kernel let consider our old example. In this case our matrix is:</p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 7 &amp; 1 &amp; 3 \\
-4 &amp; -2 &amp; 2 &amp; -2 \\
-1 &amp; 7 &amp; 3 &amp; 2 \\
-2 &amp; 2 &amp; 2 &amp; 0 \\
\end{bmatrix}
$$</div>
<p>And its row reduced form is</p>
<div class="math">$$
B =
\begin{bmatrix}
-1 &amp; 7 &amp; 3 &amp; 2\\
0 &amp; 3 &amp; 1 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}
$$</div>
<p>We solved this and got the following result:</p>
<div class="math">$$
Ker(A) = 
\begin{bmatrix}
x_{1} = \frac{2}{3}x_{3} - \frac{1}{3}x_{4} \\
x_{2} = -\frac{1}{3}x_{3} - \frac{1}{3}x_{4} \\
x_{3} \\
x_{4}
\end{bmatrix}
$$</div>
<p>Now to have basis for null space just plug values for <span class="math">\(x_{3} = 1\)</span> and <span class="math">\(x_{4} = 0\)</span>, resulted vector is</p>
<div class="math">$$
\begin{bmatrix}
\frac{2}{3} \\
-\frac{1}{3} \\
1 \\
0
\end{bmatrix}
$$</div>
<p>The resulted vector is one set of the basis for kernel space. The values for <span class="math">\(x_{3}\)</span> and <span class="math">\(x_{4}\)</span> are up to you as they are free variables.</p>
<h3 id="transformations">Transformations<a class="headerlink" href="#transformations" title="Permanent link">&para;</a></h3>
<p><a id="Transformations"></a></p>
<hr>
<p>Matrices and vectors are used together to manipulate spatial dimensions. This has a lot of applications, 
including the mathematical generation of 3D computer graphics, geometric modeling, and the training and optimization of 
machine learning algorithms. Here, I present some types of transformation. However, the list will not be exhaustive. 
Firstly, define linear transformation:</p>
<blockquote>
<p>Linear transformation or linear map, is a mapping (function) between two vector spaces that preserves addition and scalar multiplication operations</p>
</blockquote>
<h4 id="linear-transformation">Linear Transformation<a class="headerlink" href="#linear-transformation" title="Permanent link">&para;</a></h4>
<p>You can manipulate a vector by multiplying it with a matrix. The matrix acts like a function that operates on an input 
vector to produce a vector output. Specifically, matrix multiplications of vectors are <em>linear transformations</em> that 
transform the input vector into the output vector.</p>
<p>For example, consider a matrix <span class="math">\(A\)</span> and vector <span class="math">\(v\)</span></p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 3 \\
5 &amp; 2
\end{bmatrix}
\quad
\vec{v} =
\begin{bmatrix}
1 \\
2
\end{bmatrix}
$$</div>
<p>Define transformation <span class="math">\(T\)</span> to be:</p>
<div class="math">$$
T(\vec{v}) = A \vec{v}
$$</div>
<p>This transformation is simply dot or inner product and give the following result:</p>
<div class="math">$$
T(\vec{v}) = A \vec{v} =
\begin{bmatrix}
8 \\
9
\end{bmatrix}
$$</div>
<p>In this case, both the input and output vector has 2 components. In other words, the transformation takes a 2-dimensional 
vector and produces a new 2-dimensional vector. Formally we can write this in the following way:</p>
<div class="math">$$
T: \rm I\!R^{2} \to \rm I\!R^{2}
$$</div>
<p>The transformation does not necessarily have to be <span class="math">\(n \times n\)</span>. The dimension of the output vector and the input vector may differ. Rewrite our matrix <span class="math">\(A\)</span> and vector <span class="math">\(v\)</span>.</p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 3 \\
5 &amp; 2 \\
1 &amp; 1
\end{bmatrix}
\quad
\vec{v} =
\begin{bmatrix}
1 \\
2
\end{bmatrix}
$$</div>
<p>Apply above transformation gives,</p>
<div class="math">$$
T(\vec{v}) = A \vec{v} =
\begin{bmatrix}
8 \\
9 \\
3
\end{bmatrix}
$$</div>
<p>Now, our transformation transforms a vector from 2-dimensional space into 3-dimensional space. We can rite this transformation as</p>
<div class="math">$$
T: \rm I\!R^{2} \to \rm I\!R^{3}
$$</div>
<h4 id="transformations-of-magnitude-and-amplitude">Transformations of Magnitude and Amplitude<a class="headerlink" href="#transformations-of-magnitude-and-amplitude" title="Permanent link">&para;</a></h4>
<p>When we multiply a vector by a matrix we transform it in at least one of the following two ways</p>
<ul>
<li>
<p>Scale the length (Magnitude)</p>
</li>
<li>
<p>Change the direction (Amplitude)</p>
</li>
</ul>
<p><em>Change in length (Magnitude), but not change in direction (Amplitude)</em></p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 0 \\
0 &amp; 2
\end{bmatrix}
\quad
\vec{v} =
\begin{bmatrix}
1 \\
0
\end{bmatrix}
$$</div>
<p>transformation gives,</p>
<div class="math">$$
T(\vec{v}) = A \vec{v} =
\begin{bmatrix}
2 \\
0
\end{bmatrix}
$$</div>
<p>In this case, the resulted vector changed in length but not changed in direction. See code and visualization in Numerical Representation part.</p>
<p><em>Change in direction (Amplitude), but not change in length (Magnitude)</em></p>
<div class="math">$$
A =
\begin{bmatrix}
0 &amp; -1 \\
1 &amp; 0
\end{bmatrix}
\quad
\vec{v} =
\begin{bmatrix}
1 \\
0
\end{bmatrix}
$$</div>
<p>transformation gives,</p>
<div class="math">$$
T(\vec{v}) = A \vec{v} =
\begin{bmatrix}
0 \\
1
\end{bmatrix}
$$</div>
<p>This time, resulted vector changed in direction but has the same length.</p>
<p><em>Change in direction (Amplitude) and in length (Magnitude)</em></p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 1 \\
1 &amp; 2
\end{bmatrix}
\quad
\vec{v} =
\begin{bmatrix}
1 \\
0
\end{bmatrix}
$$</div>
<p>transformation gives,</p>
<div class="math">$$
T(\vec{v}) = A \vec{v} =
\begin{bmatrix}
2 \\
1
\end{bmatrix}
$$</div>
<p>This time the resulted vector changed in the direction as well as the length.</p>
<h4 id="affine-transformation">Affine Transformation<a class="headerlink" href="#affine-transformation" title="Permanent link">&para;</a></h4>
<p>An Affine transformation multiplies a vector by a matrix and adds an offset vector, sometimes referred to as <em>bias</em>.</p>
<div class="math">$$
T(\vec{v}) = A\vec{v} + \vec{b}
$$</div>
<p>Consider following example</p>
<div class="math">$$
T(\vec{v}) = A\vec{v} + \vec{b} = 
\begin{bmatrix}
5 &amp; 2\\
3 &amp; 1
\end{bmatrix} \cdot
\begin{bmatrix}
1\\
1
\end{bmatrix} + 
\begin{bmatrix}
-2\\
-6
\end{bmatrix} = 
\begin{bmatrix}
5\\
-2
\end{bmatrix}
$$</div>
<p>This kind of transformation is actually the basic block of linear regression, which is a core foundation for machine learning. However, these concepts are out of the scope of this tutorial. Python code is below for this transformation.</p>
<h3 id="eigenvalues">Eigenvalues<a class="headerlink" href="#eigenvalues" title="Permanent link">&para;</a></h3>
<p><a id="Eigenvalues"></a></p>
<hr>
<p>Let consider matrix <span class="math">\(A\)</span></p>
<div class="math">$$
A =
\begin{bmatrix}
2 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 4 \\
0 &amp; 4 &amp; 9
\end{bmatrix}
$$</div>
<p>Now, let multiply this matrix with vector
</p>
<div class="math">$$
\vec{v} =
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
$$</div>
<p>We have the following:</p>
<div class="math">$$
A \cdot v =
\begin{bmatrix}
2 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 4 \\
0 &amp; 4 &amp; 9
\end{bmatrix}
\cdot
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix} =
\begin{bmatrix}
2 \\
0 \\
0
\end{bmatrix} =
2 \cdot v
$$</div>
<p>That's the beautiful relationship yes? To prove this is not the only one vector, which can do this try this 
vector <span class="math">\(\vec{v} = [0\quad 1\quad 2]\)</span> instead of old <span class="math">\(v\)</span>. You should get <span class="math">\(11\cdot \vec{v}\)</span></p>
<p>This beautiful relationship comes from the notion of eigenvalues and eigenvectors. In this case <span class="math">\(2\)</span> and <span class="math">\(11\)</span> are eigenvalues of the matrix <span class="math">\(A\)</span>.</p>
<p>Let formalize the notion of eigenvalue and eigenvector:</p>
<blockquote>
<p>Let <span class="math">\(A\)</span> be an <span class="math">\(n\times n\)</span> <strong>square</strong> matrix. If <span class="math">\(\lambda\)</span> is a scalar and <span class="math">\(v\)</span> is non-zero vector in <span class="math">\(\mathbb{R^n}\)</span> such that <span class="math">\(Av = \lambda v\)</span> then we say that <span class="math">\(\lambda\)</span> is an <em>eigenvalue</em> and <span class="math">\(v\)</span> is <em>eigenvector</em></p>
</blockquote>
<p>I believe you are interested in how to find eigenvalues. Consider again our matrix <span class="math">\(A\)</span> and follow steps to find eigenvalues. 
Given that our matrix <span class="math">\(A\)</span> is a square matrix, the condition that characterizes an eigenvalue <span class="math">\(\lambda\)</span> is the existence of a 
nonzero vector <span class="math">\(v\)</span> such that <span class="math">\(Av = \lambda v\)</span>. We can rewrite this equation in the following way:</p>
<div class="math">$$
Av = \lambda v
\\
Av - \lambda v = 0
\\
Av - \lambda I v = 0
\\
(A - \lambda I)v = 0
$$</div>
<p>The final form of this equation makes it clear that <span class="math">\(v\)</span> is the solution of a square, homogeneous system. 
To have the nonzero solution(we required it in above definition), then the determinant of 
the <strong>coefficient matrix</strong> - <span class="math">\((A - \lambda I)\)</span> must be zero. This is achieved when the columns of the coefficient matrix are 
linearly dependent. In other words, to find eigenvalues we have to choose <span class="math">\(\lambda\)</span> such that to solve the following equation:</p>
<div class="math">$$
det(A - \lambda I) = 0
$$</div>
<p>This equation is called <strong>characteristic equation</strong></p>
<p>For more clarity, let solve it with a particular example. We have square matrix <span class="math">\(A\)</span> and follow the above equation gives us:</p>
<div class="math">$$
det(A - \lambda I) = det\Bigg(
\begin{bmatrix}
2 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 4 \\
0 &amp; 4 &amp; 9
\end{bmatrix} -
\lambda \cdot
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\Bigg) =
det\Bigg(
\begin{bmatrix}
2 - \lambda &amp; 0 &amp; 0 \\
0 &amp; 3 - \lambda &amp; 4 \\
0 &amp; 4 &amp; 9 - \lambda
\end{bmatrix}
\Bigg)
\Rightarrow
\\ \quad
\\
\Rightarrow
(2 - \lambda)[(3 - \lambda)(9 - \lambda) - 16] =
-\lambda^3 + 14\lambda^2 - 35\lambda + 22
$$</div>
<p>The equation <span class="math">\(-\lambda^3 + 14\lambda^2 - 35\lambda + 22\)</span> is called  <strong>characteristic polynomial</strong> of the matrix <span class="math">\(A\)</span> 
and will be of degree <span class="math">\(n\)</span> if <span class="math">\(A\)</span> is <span class="math">\(n\times n\)</span></p>
<p>The zeros or roots of this characteristic polynomial are the eigenvalues of the original matrix <span class="math">\(A\)</span>. 
In this case the roots are <span class="math">\(2\)</span>, <span class="math">\(1\)</span>, and <span class="math">\(11\)</span>. Surprise! Our matrix <span class="math">\(A\)</span> have three eigenvalues and two of them are 
already known for us from above example.</p>
<p>Eigenvalues of a square matrix <span class="math">\(A\)</span> have some nice features:</p>
<ul>
<li>
<p>The determinant of <span class="math">\(A\)</span> equals to the product of the eigenvalues</p>
</li>
<li>
<p>The trace of <span class="math">\(A\)</span> (The sum of the elements on the principal diagonal) equal the sum of the eigenvalues</p>
</li>
<li>
<p>If <span class="math">\(A\)</span> is symmetric matrix, then all of its eigenvalues are real</p>
</li>
<li>
<p>If <span class="math">\(A\)</span> is invertible (The determinant of <span class="math">\(A\)</span> is not zero) and <span class="math">\(\lambda_{1}, \cdots, \lambda_{n}\)</span> are its eigenvalues, then the eigenvalues of <span class="math">\(A^{-1}\)</span> are <span class="math">\(1 / \lambda_{1}, \cdots, 1 / \lambda_{n}\)</span></p>
</li>
</ul>
<p>From first feature we have that the matrix is invertible if and only if all its eigenvalues are nonzero.</p>
<h3 id="eigenvectors">Eigenvectors<a class="headerlink" href="#eigenvectors" title="Permanent link">&para;</a></h3>
<p><a id="Eigenvectors"></a></p>
<hr>
<p>It's time to calculate eigenvectors, but let firstly define what is eigenvector and how it relates to the eigenvalue.</p>
<blockquote>
<p>Any nonzero vector <span class="math">\(v\)</span> which satisfies characteristic equation is said to be an eigenvector of <span class="math">\(A\)</span> corresponding to <span class="math">\(\lambda\)</span></p>
</blockquote>
<p>Continue above example and see what are eigenvectors corresponding to eigenvalues <span class="math">\(\lambda = 2\)</span>, <span class="math">\(\lambda = 1\)</span>, and <span class="math">\(\lambda = 11\)</span>, respectively.</p>
<p>Eigenvector for <span class="math">\(\lambda = 1\)</span></p>
<div class="math">$$
(A - 1I)\cdot
\begin{bmatrix}
v_{1} \\
v_{2} \\
v_{3}
\end{bmatrix} =\Bigg(
\begin{bmatrix}
2 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 4 \\
0 &amp; 4 &amp; 9
\end{bmatrix} -
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}\Bigg)\cdot
\begin{bmatrix}
v_{1} \\
v_{2} \\
v_{3}
\end{bmatrix} =
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 4 \\
0 &amp; 4 &amp; 8
\end{bmatrix}\cdot
\begin{bmatrix}
v_{1} \\
v_{2} \\
v_{3}
\end{bmatrix}=
\begin{bmatrix}
0 \\
0 \\
0
\end{bmatrix}
$$</div>
<p>Rewrite this as a system of equations, we'll get</p>
<div class="math">$$
\begin{cases}
v_{1} = 0\\
2v_{2} + 4v_{3} = 0\\
4v_{2} + 8v{3} = 0
\end{cases}\rightarrow
\begin{cases}
v_{1} = 0 \\
v_{2} = -2v_{3}\\
v_{3} = 1
\end{cases}
\rightarrow
\begin{cases}
v_{1} = 0 \\
v_{2} = -2\\
v_{3} = 1
\end{cases}
$$</div>
<p>So, our eigenvector corresponding to eigenvalue <span class="math">\(\lambda = 1\)</span> is</p>
<div class="math">$$
v_{\lambda = 1} =
\begin{bmatrix}
0 \\
-2 \\
1
\end{bmatrix}
$$</div>
<p>Finding eigenvectors for <span class="math">\(\lambda = 2\)</span> and <span class="math">\(\lambda = 11\)</span> is up to you.</p>
<h3 id="spectrum-and-spectral-radius">Spectrum and Spectral Radius<a class="headerlink" href="#spectrum-and-spectral-radius" title="Permanent link">&para;</a></h3>
<p><a id="Spectrum_and_Spectral_Radius"></a></p>
<hr>
<p>The <strong>Spectral Radius</strong> of a square matrix <span class="math">\(A\)</span> is the largest absolute values of its eigenvalues and is denoted by <span class="math">\(\rho(A)\)</span>. More formally,</p>
<blockquote>
<p>Spectral radius of a <span class="math">\(n \times n\)</span> matrix <span class="math">\(A\)</span> is:</p>
</blockquote>
<div class="math">$$
\rho(A) = max
\Big\{
\mid \lambda
\mid \ :
\lambda \ is \ an \ eigenvalue \ of \ A
\Big\}
$$</div>
<p>Stated otherwise, we have</p>
<div class="math">$$
\rho(A) = max
\Big\{
\mid \lambda_{1}
\mid,
\cdots,
\mid \lambda_{n}
\mid
\Big\}
$$</div>
<p>It's noteworthy that the set of all eigenvalues</p>
<div class="math">$$
\Big\{ \lambda : \lambda \in \lambda(A)
\Big\}
$$</div>
<p>is called the <strong>Spectrum</strong></p>
<p>From above example we had three eigenvalues, <span class="math">\(\lambda = 2\)</span>, <span class="math">\(\lambda = 1\)</span> and <span class="math">\(\lambda = 11\)</span> which are spectrum 
of <span class="math">\(A\)</span> and spectral radius for our matrix <span class="math">\(A\)</span> is <span class="math">\(\lambda = 11\)</span></p>
<h3 id="numerical-representation">Numerical Representation<a class="headerlink" href="#numerical-representation" title="Permanent link">&para;</a></h3>
<p><a id="Numerical_Representation_Matrix"></a></p>
<h4 id="kernel-or-null-space-of-a-matrix">Kernel or Null Space of a Matrix<a class="headerlink" href="#kernel-or-null-space-of-a-matrix" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">null_space</span>


<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>


<span class="n">kernel_A</span> <span class="o">=</span> <span class="n">null_space</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Normalized Kernel&quot;</span><span class="p">,</span> <span class="n">kernel_A</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span> <span class="c1"># This matrix is normalized, meaning that it has unit length</span>


<span class="c1"># To find unnormalized kernel we have to do the following:</span>


<span class="c1"># Import sympy</span>
<span class="kn">from</span> <span class="nn">sympy</span> <span class="kn">import</span> <span class="n">Matrix</span>


<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span>

<span class="n">B</span> <span class="o">=</span> <span class="n">Matrix</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

<span class="n">kernel_B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">nullspace</span><span class="p">()</span>

<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Unnormalized Kernel&quot;</span><span class="p">,</span> <span class="n">kernel_B</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="c1"># In unnormilized case, we clearly see that sympy automatically choose values for our free variables. </span>
<span class="c1"># In first case x_3 = 1; x_4 = 0 and in the second case x_3 = 0; x_4 = 1</span>
<span class="c1"># Resulted vector(s) are basis for the null space for our matrix A</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Normalized</span> <span class="n">Kernel</span>
<span class="p">[[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">59408621</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">00166</span>   <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">09787364</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">40852336</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">69195985</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">41018336</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">39833892</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">81538673</span><span class="p">]]</span>

<span class="n">Unnormalized</span> <span class="n">Kernel</span>
<span class="p">[</span><span class="n">Matrix</span><span class="p">([</span>
<span class="p">[</span> <span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="p">],</span>
<span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">],</span>
<span class="p">[</span>   <span class="mi">1</span><span class="p">],</span>
<span class="p">[</span>   <span class="mi">0</span><span class="p">]]),</span> <span class="n">Matrix</span><span class="p">([</span>
<span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">],</span>
<span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">],</span>
<span class="p">[</span>   <span class="mi">0</span><span class="p">],</span>
<span class="p">[</span>   <span class="mi">1</span><span class="p">]])]</span>
</pre></div>


<h4 id="linear-transformations">Linear Transformations<a class="headerlink" href="#linear-transformations" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>


<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">A</span><span class="nd">@v</span> <span class="c1"># dot product</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Resulted vector is: t =&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="c1"># Plot v and t</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">t</span><span class="p">,</span><span class="n">v</span><span class="p">])</span>
<span class="n">origin</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="n">vecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">vecs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># Original vector v is green and transformed vector t is blue.</span>
<span class="c1"># Vector t has same direction as v but greater magnitude</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">Resulted vector is: t = [2 0]</span>
</pre></div>


<p><img alt="picture" src="https://dsfabric.org/images/Linear_Algebra_Advance_Part_I_figure2_1.png"></p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">A</span><span class="nd">@v</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Resulted vector is: t =&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="c1"># Plot v and t</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">v</span><span class="p">,</span><span class="n">t</span><span class="p">])</span>
<span class="n">origin</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="n">vecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">vecs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># Resulted vector change the direction but has the same length</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">Resulted vector is: t = [0 1]</span>
</pre></div>


<p><img alt="picture" src="https://dsfabric.org/images/Linear_Algebra_Advance_Part_I_figure3_1.png"></p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">A</span><span class="nd">@v</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Resulted vector is: t =&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="c1"># Plot v and t</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">v</span><span class="p">,</span><span class="n">t</span><span class="p">])</span>
<span class="n">origin</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="n">vecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">vecs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># Resulted vector changed the direction, as well as the length</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">Resulted vector is: t = [2 1]</span>
</pre></div>


<p><img alt="picture" src="https://dsfabric.org/images/Linear_Algebra_Advance_Part_I_figure4_1.png"></p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">6</span><span class="p">])</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">A</span><span class="nd">@v</span> <span class="o">+</span> <span class="n">b</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Resulted vector is: t =&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="c1"># Plot v and t</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">v</span><span class="p">,</span><span class="n">t</span><span class="p">])</span>
<span class="n">origin</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="n">vecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">vecs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># The resulted vector t is blue</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">Resulted vector is: t = [ 5 -2]</span>
</pre></div>


<p><img alt="picture" src="https://dsfabric.org/images/Linear_Algebra_Advance_Part_I_figure5_1.png"></p>
<h4 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors<a class="headerlink" href="#eigenvalues-and-eigenvectors" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>


<span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvalues are: &quot;</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvectors are: &quot;</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="c1"># Note that this eigenvectors seems different from my calculation. However they are not different.</span>
<span class="c1"># They are normalized to have unit length</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Eigenvalues</span> <span class="k">are</span><span class="p">:</span>  <span class="p">[</span><span class="mi">11</span><span class="p">.</span>  <span class="mi">1</span><span class="p">.</span>  <span class="mi">2</span><span class="p">.]</span>

<span class="n">Eigenvectors</span> <span class="k">are</span><span class="p">:</span>
<span class="p">[[</span> <span class="mi">0</span><span class="p">.</span>          <span class="mi">0</span><span class="p">.</span>          <span class="mi">1</span><span class="p">.</span>        <span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">4472136</span>   <span class="mi">0</span><span class="p">.</span><span class="mi">89442719</span>  <span class="mi">0</span><span class="p">.</span>        <span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">89442719</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">4472136</span>   <span class="mi">0</span><span class="p">.</span>        <span class="p">]]</span>
</pre></div>


<h3 id="conclusion-for-part-i">Conclusion for part I<a class="headerlink" href="#conclusion-for-part-i" title="Permanent link">&para;</a></h3>
<p>In conclusion, part one is relatively heavy but as it contains lots of calculations. That's why we use computers to solve this kind of problems. Despite Numpy's build in functions there is big avenue to write algorithm to compute eigenvalues for instance. That would be very helpful for practicing linear algebra and python simultaneously.</p>
<p>The second part is devoted solely for matrix decompositions.</p>
<h3 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h3>
<h4 id="vector_1">Vector<a class="headerlink" href="#vector_1" title="Permanent link">&para;</a></h4>
<ul>
<li><a href="http://linear.axler.net/">Linear Algebra Done Right</a></li>
</ul>
<h4 id="matrix_1">Matrix<a class="headerlink" href="#matrix_1" title="Permanent link">&para;</a></h4>
<ul>
<li><a href="https://en.wikipedia.org/wiki/List_of_linear_algebra_topics">Linear Algebra Topics</a></li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             


<div class="applause_button">
    <applause-button url=https://dsfabric.org/advance-linear-algebra-with-python-part-i> </applause-button>
</div>

 
                <p id="post-share-links">
    Like this post? Share on:
      <a href="https://twitter.com/intent/tweet?text=Advance%20Linear%20Algebra%20with%20Python%20-%20Part%20I&url=https%3A//dsfabric.org/advance-linear-algebra-with-python-part-i&via=N_Okroshiashvil&hashtags=linear-algebra,advance-topics,mathematics" target="_blank" rel="nofollow noopener noreferrer" title="Share on Twitter">Twitter</a>
 â       <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//dsfabric.org/advance-linear-algebra-with-python-part-i" target="_blank" rel="nofollow noopener noreferrer" title="Share on Facebook">Facebook</a>
 â       <a href="mailto:?subject=Advance%20Linear%20Algebra%20with%20Python%20-%20Part%20I&amp;body=https%3A//dsfabric.org/advance-linear-algebra-with-python-part-i" target="_blank" rel="nofollow noopener noreferrer" title="Share via Email">Email</a>

            
            







<section>
    <h6 style="display:none;">Comments</h6>
    <p id="comment-message">So what do you think? Did I miss something? Is any part unclear? Leave your comments below. </p>

    <div class="accordion" id="accordion2">
        <div class="accordion-group">
            <div class="accordion-heading">
                <a class="accordion-toggle disqus-comment-count comment-count collapsed"
                   data-toggle="collapse"
                   data-parent="#accordion2"
                   data-disqus-identifier="https://dsfabric.org/advance-linear-algebra-with-python-part-i"
                   href="https://dsfabric.org/advance-linear-algebra-with-python-part-i#comment_thread"
                   id="comment-accordion-toggle">
                    Comments
                </a>
            </div>
            <div id="comment_thread" class="accordion-body collapse">
                <div class="accordion-inner">
                    <div class="comments">
                        <div id="disqus_thread"></div>
                        <script>
    var disqus_shortname = 'dsfabric';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());

    var disqus_identifier = 'https://dsfabric.org/advance-linear-algebra-with-python-part-i';
    var disqus_url = 'https://dsfabric.org/advance-linear-algebra-with-python-part-i';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>




                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

            <hr/>
<section>
    <h2>Keep Reading</h2>
<ul class="related-posts-list">
<li><a href="https://dsfabric.org/introduction-to-linear-algebra-with-python" title="Introduction to Linear Algebra with Python">Introduction to Linear Algebra with Python</a></li>
<li><a href="https://dsfabric.org/basic-linear-algebra-with-python" title="Basic Linear Algebra with Python">Basic Linear Algebra with Python</a></li>
<li><a href="https://dsfabric.org/intermediate-linear-algebra-with-python-part-i" title="Intermediate Linear Algebra with Python - Part I">Intermediate Linear Algebra with Python - Part I</a></li>
<li><a href="https://dsfabric.org/advance-linear-algebra-with-python-part-ii" title="Advance Linear Algebra with Python - Part II">Advance Linear Algebra with Python - Part II</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">Â« <a href="https://dsfabric.org/intermediate-linear-algebra-with-python-part-i" title="Previous: Intermediate Linear Algebra with Python - Part I">Intermediate Linear Algebra with Python - Part I</a></li>
                <li class="next-article"><a href="https://dsfabric.org/advance-linear-algebra-with-python-part-ii" title="Next: Advance Linear Algebra with Python - Part II">Advance Linear Algebra with Python - Part II</a> Â»</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2019-03-04T12:14:00+04:00">áá á¨ 04 ááá á¢á 2019</time>
            <h4>Category</h4>
            <a class="category-link" href="https://dsfabric.org/categories#mathematics-ref">Mathematics</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://dsfabric.org/tags#advance-topics-ref">Advance Topics
                    <span class="superscript">2</span>
</a></li>
                <li><a href="https://dsfabric.org/tags#linear-algebra-ref">Linear Algebra
                    <span class="superscript">5</span>
</a></li>
                <li><a href="https://dsfabric.org/tags#mathematics-ref">Mathematics
</a></li>
            </ul>
<h4>Stay in Touch</h4>
<div id="sidebar-social-link">
    <a href="https://www.linkedin.com/in/nodar-okroshiashvili/" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="LinkedIn" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%" fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
    </a>
    <a href="https://github.com/Okroshiashvili" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://twitter.com/N_Okroshiashvil" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>
    <div>
        Content licensed under <a rel="license nofollow noopener noreferrer"
    href="http://creativecommons.org/licenses/by/4.0/" target="_blank">
    Creative Commons Attribution 4.0 International License</a>.
    </div>

    <div>
        <span class="site-name">Data Science Fabric</span> - Torture the data, and it will confess to anything. Ronald Coase
    </div>



    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://dsfabric.org/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>