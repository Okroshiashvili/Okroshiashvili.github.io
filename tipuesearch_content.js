var tipuesearch = {"pages":[{"title":"Search · Data Science Fabric\n","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch · Data Science Fabric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n    ga('create', 'UA-136307659-1', 'auto');\n    ga('send', 'pageview');\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Fabric\n\n\n\nHome\n\nCategories\nTags\nArchives\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        Content licensed under \n    Creative Commons Attribution 4.0 International License.\n    \n\nData Science Fabric - Torture the data, and it will confess to anything. Ronald Coase\n    \n\n        Powered by: Pelican\n        Theme: Elegant\n\n \n\n\n\n\n            function validateForm(query)\n            {\n                return (query.length > 0);\n            }\n        \n\n\n","tags":"","url":"https://dsfabric.org/search.html"},{"title":"Lost in Pandas: #1","text":"Many have been told and written about Pandas and its capabilities. I could not imagine data scientist or data analyst who had not heard about Pandas or had not used it at least once. We all use it. Every day, every week. It does not matter how many times. It's a great tool. I use it all the time when I want to do data analysis, either it is simple calculations or complex data transformations, and it surprises me. Pandas is so simple in its form. However, imagine, how much you can do with some simple method chaining. Saying all of these, this blog aims to share my experience and amazment with Pandas. This series is not meant for beginner users and will not be short in length. These series will be based on my experience and I will try to give a detailed explanation for every step from problem definition to solving it. That's enough for now. Let get down to business. Problem Statement We have data. This data comes from the HR department of the company. The data contains two columns, company name, and information about its employees. Each row of the employee information column is a list of lists. The lists inside the outer list can be duplicated. It also can have duplicate values, and inner lists have at most two values. Disclaimer: Any name, phone, email, and the title is a pure coincidence. Data is random and fake. Here is our data. from collections import defaultdict import pandas as pd import numpy as np data = { \"company_name\" : [ 'A' , 'B' , 'C' , 'D' , 'E' , 'F' ], \"info\" : [ [[ 'Name' , 'David Jones' ],[ 'Title' , 'CEO' ],[ 'Phone' , '207-685-1626' ],[ 'Email' , 'djones@example.org' ]], [[ 'Name' , 'Kate Brown' ],[ 'Title' , 'Senior Lawyer' ],[ 'Phone' , '316-978-7791' ], [ 'Email' , 'Kate.Brown@example.edu' ],[ 'Name' , 'Darin White' ],[ 'Title' , 'Associate Vice President' ], [ 'Phone' , '316-978-3887' ],[ 'Email' , 'Darin.White@example.edu' ]], [[ 'Name' , 'Scott Lamb' ],[ 'Title' , 'Actuary' ],[ 'Phone' , '316-978-3804' ], [ 'Email' , 'scott.lamb@example.edu' ],[ 'Name' , 'Scott Lamb' ],[ 'Title' , 'Senior Officer' ], [ 'Title' , 'Application Developer' ],[ 'Title' , 'Blockchain Architect' ],[ 'Title' , 'Director of External Affairs' ],[ 'Name' , 'Scott' ], [ 'Name' , 'Scott' ],[ 'Title' , 'Director of Medicine' ],[ 'Title' , 'Product Owner' ], [ 'Name' , 'Mike' ],[ 'Title' , 'Domain Expert' ],[ 'Title' , 'Growth Hacker' ], [ 'Title' , 'Engineering Head' ],[ 'Title' , 'Event Manager' ],[ 'Name' , 'Joe' ], [ 'Name' , 'Mike' ],[ 'Title' , 'Fundraising' ],[ 'Title' , 'VP of Customers' ], [ 'Name' , 'Mike' ],[ 'Title' , 'Venture Capital Analyst' ],[ 'Title' , 'UX Designer' ], [ 'Name' , 'Mike' ],[ 'Name' , 'Susan' ],[ 'Name' , 'Bryan' ],[ 'Name' , 'Mia' ],[ 'Title' , 'Songwriter' ]], [[ 'Name' , 'Rose Smith Rose Smith' ],[ 'Title' , 'Vice President' ], [ 'Title' , 'Finance and Operations Head' ],[ 'Phone' , '316-978-3810' ], [ 'Email' , 'rose.smith@example.edu' ],[ 'Name' , 'Rose Smith' ], [ 'Title' , 'Foundation' ],[ 'Name' , 'Susan' ],[ 'Title' , 'Foundation' ], [ 'Title' , 'Accountant' ],[ 'Title' , 'Accountant' ],[ 'Title' , 'Executive' ], [ 'Title' , 'Director' ],[ 'Title' , 'Executive' ],[ 'Name' , 'Ray' ], [ 'Title' , 'Strategic Planning' ],[ 'Title' , 'Financial Analyst' ],[ 'Title' , 'Foundation' ], [ 'Title' , 'Foundation' ],[ 'Name' , 'Susan' ],[ 'Title' , 'member of the board' ], [ 'Title' , 'board of directors' ],[ 'Title' , 'president' ],[ 'Title' , 'board of directors' ]], [[ 'Name' , 'Carl Clark' ],[ 'Title' , 'Chief ' ],[ 'Title' , 'Operating Officer' ], [ 'Title' , 'PhD' ],[ 'Phone' , '413-534-2745' ],[ 'Email' , 'Clark_Carl@example.com' ]], [[ 'Title' , 'Board Member' ],[ 'Name' , 'Taylor Garcia' ],[ 'Phone' , '307-733-2164' ], [ 'Phone' , '307-733-4568' ],[ 'Email' , 'Garcia@example.org' ]] ] } Let convert this dictionary into Pandas DataFrame and see what data we have. df = pd . DataFrame ( data ) # Print DataFrame head df . head () company_name info 0 A [[Name, David Jones], [Title, CEO], [Phone, 20... 1 B [[Name, Kate Brown], [Title, Senior Lawyer], [... 2 C [[Name, Scott Lamb], [Title, Actuary], [Phone,... 3 D [[Name, Rose Smith Rose Smith], [Title, Vice P... 4 E [[Name, Carl Clark], [Title, Chief ], [Title, ... We see that the first column seems okay, but the second one not. Here we have one big list containing smaller two-element lists. df [ 'info' ] . iloc [ 2 ] [['Name', 'Scott Lamb'], ['Title', 'Actuary'], ['Phone', '316-978-3804'], ['Email', 'scott.lamb@example.edu'], ['Name', 'Scott Lamb'], ['Title', 'Senior Officer'], ['Title', 'Application Developer'], ['Title', 'Blockchain Architect'], ['Title', 'Director of External Affairs'], ['Name', 'Scott'], ['Name', 'Scott'], ['Title', 'Director of Medicine'], ['Title', 'Product Owner'], ['Name', 'Mike'], ['Title', 'Domain Expert'], ['Title', 'Growth Hacker'], ['Title', 'Engineering Head'], ['Title', 'Event Manager'], ['Name', 'Joe'], ['Name', 'Mike'], ['Title', 'Fundraising'], ['Title', 'VP of Customers'], ['Name', 'Mike'], ['Title', 'Venture Capital Analyst'], ['Title', 'UX Designer'], ['Name', 'Mike'], ['Name', 'Susan'], ['Name', 'Bryan'], ['Name', 'Mia'], ['Title', 'Songwriter']] As we figured out the data structure, let define the aim. We need to unpack lists from the second column and flatten them in tabular format in the way to preserve the order. Meaning that, from the above example, Scott Lamb has to have title Actuary and not other titles are allowed. Long story short, we need to flatten list of list and make proper DataFrame from it. How I approached this problem The first idea that came to my mind was to use Pandas DataFrame .explode() method to unpack list of lists, which returned lists containing two elements. After that, I extracted these two elements into two different columns. df_exploded = df . explode ( 'info' ) df_exploded . head () company_name info 0 A [Name, David Jones] 0 A [Title, CEO] 0 A [Phone, 207-685-1626] 0 A [Email, djones@example.org] 1 B [Name, Kate Brown] # Add two new columns df_exploded . loc [:, 'tag' ] = df_exploded [ 'info' ] . map ( lambda x : x [ 0 ]) df_exploded . loc [:, 'result' ] = df_exploded [ 'info' ] . map ( lambda x : x [ 1 ]) df_exploded . head () company_name info tag result 0 A [Name, David Jones] Name David Jones 0 A [Title, CEO] Title CEO 0 A [Phone, 207-685-1626] Phone 207-685-1626 0 A [Email, djones@example.org] Email djones@example.org 1 B [Name, Kate Brown] Name Kate Brown Despite unpacking the list of lists, that is not the format I wanted. So, I need to do an extra transformation. df_exploded_final = ( df_exploded . groupby ([ 'company_name' , 'tag' ])[ 'result' ] . apply ( lambda x : pd . Series ( x . values )) . unstack ( 1 ) . reset_index () . drop ([ 'level_1' ], axis = 1 )) df_exploded_final . head () tag company_name Email Name Phone tag 0 A djones@example.org David Jones 207-685-1626 CEO 1 B Kate.Brown@example.edu Kate Brown 316-978-7791 Senior Lawyer 2 B Darin.White@example.edu Darin White 316-978-3887 Associate Vice President 3 C scott.lamb@example.edu Scott Lamb 316-978-3804 Actuary 4 C NaN Scott Lamb NaN Senior Officer It seems we did a good job. However, this approach is prone to errors. Namely, it does not preserve the order of the list values and may assign a different email to a different person. That was not what I need. So, I decided to use another way to solve this problem. Notably, as the data contained millions of rows, it seemed impossible to be too precise, but I wanted to reduce the error of non-matching cases. To achieve this, I iterated over the values of info column and converted it to dict of lists, where keys are tags (identifiers) and values are actual employee information. out = [] for x in df [ 'info' ] . tolist (): groups = defaultdict ( list ) for g , v in x : groups [ g ] . append ( v ) out . append ( dict ( groups )) df . loc [:, 'new_info' ] = out df [ 'new_info' ] . iloc [ 0 ] {'Name': ['David Jones'], 'Title': ['CEO'], 'Phone': ['207-685-1626'], 'Email': ['djones@example.org']} That's a step forward. After this, I was interested in counting the values for each key in dicts for each row. I made small changes in the above code and applied it to the new_info column. out = [] for x in df [ 'new_info' ]: groups = defaultdict ( int ) for g , v in x . items (): groups [ g ] = len ( list ( filter ( None , v ))) out . append ( dict ( groups )) df . loc [:, 'new_info_stats' ] = out df [ 'new_info_stats' ] . iloc [ 0 ] {'Name': 1, 'Title': 1, 'Phone': 1, 'Email': 1} As we calculated value counts for each dict, now we need to add three helper columns to the dataset for further usage. These helper columns will help to differentiate matching cases and non-matching cases. df [ '_max' ] = df [ 'new_info_stats' ] . apply ( lambda x : max ( x . values ())) df [ '_min' ] = df [ 'new_info_stats' ] . apply ( lambda x : min ( x . values ())) df . loc [:, 'max_equal_min' ] = pd . Series ( np . where (( df [ '_max' ] == df [ '_min' ]), 1 , 0 )) The column max_equal_min is a dummy variable and helps us to differentiate matching and non-matching cases. The value 1 indicates we have a matching case and value 0 - non-matching case. According to this column, I split data into two parts. The first only contains matching examples, and the second will have only non-matching cases. df_first = ( df [ df [ 'max_equal_min' ] > 0 ] . reset_index ( drop = True )) df_second = ( df [ df [ 'max_equal_min' ] == 0 ] . reset_index ( drop = True )) The pre-processing of the first DataFrame is over and is ready to flatten. To do so, I iterate over new_info column and transform each row into Pandas DataFrame. After this step, data will be flat. new_data = [] for j in df_first [ 'new_info' ]: new_data . append ( pd . DataFrame ( j )) df_first_final_i = ( pd . concat ( new_data , axis = 0 , sort = False ) . drop_duplicates () . dropna ( subset = [ 'Name' ]) . reset_index ( drop = True )) df_first_final_i . head () Name Title Phone Email 0 David Jones CEO 207-685-1626 djones@example.org 1 Kate Brown Senior Lawyer 316-978-7791 Kate.Brown@example.edu 2 Darin White Associate Vice President 316-978-3887 Darin.White@example.edu Woohoo, it works! However, imagine having millions of rows how slow this approach will be. For this reason, I tried another method and found it much faster. Here it is. df_first_final_ii = ( df_first [ 'new_info' ] . apply ( pd . Series ) . apply ( lambda x : x . explode ()) . drop_duplicates () . reset_index ( drop = True )) df_first_final_ii . head () Name Title Phone Email 0 David Jones CEO 207-685-1626 djones@example.org 1 Kate Brown Senior Lawyer 316-978-7791 Kate.Brown@example.edu 2 Darin White Associate Vice President 316-978-3887 Darin.White@example.edu Faster and cleaner solution. But, what about the second DataFrame? It turned out that the above solution did not fit the second DataFrame and gave me an error. The error was ValueError: cannot reindex from a duplicate axis . Before finding the solution for this error, let take a look at the data. df_second . head () company_name info \\ 0 C [[ Name , Scott Lamb ], [ Title , Actuary ], [ Phone ,... 1 D [[ Name , Rose Smith Rose Smith ], [ Title , Vice P ... 2 E [[ Name , Carl Clark ], [ Title , Chief ], [ Title , ... 3 F [[ Title , Board Member ], [ Name , Taylor Garcia ],... new_info \\ 0 { 'Name' : [ 'Scott Lamb' , 'Scott Lamb' , 'Scott' ,... 1 { 'Name' : [ 'Rose Smith Rose Smith' , 'Rose Smith... 2 {' Name ': [' Carl Clark '], ' Title ': [' Chief ', ' ... 3 { 'Title' : [ 'Board Member' ], 'Name' : [ 'Taylor G... new_info_stats _max _min max_equal_min 0 {' Name ': 12, ' Title ': 16, ' Phone ': 1, ' Email ': 1} 16 1 0 1 {' Name ': 5, ' Title ': 17, ' Phone ': 1, ' Email ': 1} 17 1 0 2 {' Name ': 1, ' Title ': 3, ' Phone ': 1, ' Email ': 1} 3 1 0 3 {' Title ': 1, ' Name ': 1, ' Phone ': 2, ' Email ' : 1 } 2 1 0 In the second and third row, we have one Name and three Title and two Phone , respectively, for the new_info_stats column. This may be due to the data entry or extraction reason. Not 100% sure that this is the case, but the likelihood is very high. So, we have to handle this problem properly. One solution is to concatenate strings for the values of Title and Phone keys. def process_info ( record : dict ) -> dict : if ( len ( record . keys ()) == 4 and len ( record . get ( \"Name\" )) == 1 and len ( record . get ( 'Title' )) > 1 and len ( record . get ( 'Email' )) == 1 and len ( record . get ( 'Phone' )) == 1 ): record [ 'Title' ] = [ ' ' . join ( record . get ( 'Title' ))] elif ( len ( record . keys ()) == 4 and len ( record . get ( \"Name\" )) == 1 and len ( record . get ( 'Title' )) == 1 and len ( record . get ( 'Email' )) > 1 and len ( record . get ( 'Phone' )) == 1 ): record [ 'Email' ] = [ ',' . join ( record . get ( 'Email' ))] elif ( len ( record . keys ()) == 4 and len ( record . get ( \"Name\" )) == 1 and len ( record . get ( 'Title' )) == 1 and len ( record . get ( 'Email' )) == 1 and len ( record . get ( 'Phone' )) > 1 ): record [ 'Phone' ] = [ ',' . join ( record . get ( 'Phone' ))] else : pass return record df_second [ 'new_info' ] = df_second [ 'new_info' ] . apply ( process_info ) This is a simple logic to check if we are correctly concatenating string. After applying this function, the second DataFrame is ready to flatten. As I mentioned above the good old method did not give me the desired result for this case and then I came up to the following: def flatten ( df , column ): data = [] for i in df [ column ]: data . append ( pd . DataFrame ( dict ([( k , pd . Series ( v )) for k , v in i . items ()]))) new_df = ( pd . concat ( data , axis = 0 , sort = False ) . drop_duplicates () . dropna ( subset = [ 'Name' ]) . drop_duplicates ( subset = [ 'Name' , 'Title' , 'Phone' , 'Email' ]) . reset_index ( drop = True )) return new_df df_second_final = flatten ( df_second , 'new_info' ) df_second_final . head () Name Title Phone Email 0 Scott Lamb Actuary 316-978-3804 scott.lamb@example.edu 1 Scott Lamb Senior Officer NaN NaN 2 Scott Application Developer NaN NaN 3 Scott Blockchain Architect NaN NaN 4 Mike Director of External Affairs NaN NaN Applied this function to the second DataFrame flattened it, and combining first and second DataFrames will give the final result. To sum up, that was only a tiny part of this data pre-processing process. However, it was a great journey and lots to learn. What do you think? Did you find a more elegant solution? Please share it in the comments.","tags":"Data Science","url":"https://dsfabric.org/lost-in-pandas-1","loc":"https://dsfabric.org/lost-in-pandas-1"},{"title":"Modeling single good market in Python","text":"In this blog, I want to review simple one good market economy, where supply and demand are linear functions of price and agents of this economy are price takers. Those agents are divided into two categories: buyers and sellers. The price of a product is determined by the interaction of demand and supply or in other words buyers and sellers. Here, the buyer is associated with demand and seller is associated with supply. Henceforth, in this market, there are no frictions or market imperfections (more on this later) and market is competitive. So, this should be a very simple environment, where buyer demands product and seller sells this product at a given price. Before continue, it's worth to define some key terms and notions in our simple market model. First, let's start by defining what is economics. In simple words, economics is the social science which studies production, consumption, and distribution of goods and services. I used, three main terms above, which constitutes our simple economy: demand, supply, and price. Demand is all the quantities of good or service that buyers are willing and able to buy at all possible prices. Demand is based on needs and wants. From the consumer point of view, we all know the difference but from an economist's perspective, they are the same. Moreover, demand is based on the ability to pay, If the agent cannot afford to pay, they have no effective demand. The law of demand states that a higher price leads to a lower quantity demanded and a lower price leads to a higher quantity demanded Make it simple, low of demand says that there is an inverse relationship between price and quantity demanded and this relationship determines downward sloping demand curve. Supply is all the possible quantities that sellers are willing and able to produce at all possible prices. When an economist talks about to supply, they mean the amount of some good or service a producer is willing to supply at each price. Price is what the producer receives for selling one unit of good or service. The low of supply states that a higher price leads to a higher quantity and lower price leads to lower quantity supplied. Low of supply says that there is positive relationship between price and quantity supplied, leading to an upward sloping supply curve. I mentioned earlier that our market is competitive and in competitive market demand for good or service and supply will determine the price. This price is the equilibrium price. Equilibrium price is the price for which quantity demanded and quantity supplied are equal. In our market settings, equilibrium occurs when the price has adjusted until the quantity supplied is equal to quantity demanded. When this equality is violated market experiences disequilibrium. When a market is experiencing a disequilibrium, there will be either a shortage or a surplus. When the price is above the equilibrium price, this produce surplus , which encourages sellers to lower their prices to eliminate the surplus. At any price below the equilibrium price, the shortage will exist, which leads to the price of the good increasing. At that moment we defined all the necessary terms and notions in our simple economy to continue our analysis and modeling. Now, it's time to ask ourselves. Can we model mathematically this economy and see the dynamics of the buyers and sellers? Yes, we can do and we can go farther and make a simple Python script to make simulations in this economy. Let's start our modeling part with defining functions for demand and supply. Logically, demand and supply are generated from some very complicated function and its real anatomy is unknown, but as we are in simple economy we can assume that they are linear functions of price: A Liner demand curve: $$ Q = a - b\\cdot p $$ A linear supply curve: $$ Q = c + d\\cdot p $$ Here, \\(p\\) is the price paid by the consumer and \\(Q\\) is the quantity. \\(a\\) , \\(b\\) , \\(c\\) , and \\(d\\) are demand and supply parameters. Let first solve this mathematically. We have the equation for quantity and the parameters \\(a\\) and \\(b\\) are predetermined, or we can choose their value. We know that in equilibrium, quantity demanded and quantity supplied are equal. So, by equating our equations and doing some simple algebra we'll get equilibrium price. Let denote this price by \\(P&#94;{*}\\) . $$ a - b\\cdot p = c + d\\cdot p \\Rightarrow \\\\ \\\\ \\Rightarrow d\\cdot p + b\\cdot p = a - c \\\\ \\\\ \\Rightarrow P&#94;{*} = \\frac{a - c}{d + b} $$ By having \\(P&#94;{*}\\) we have equilibrium price. Now, let me introduce some new notions, which are very natural in this market economy, such as, consumer surplus, producer surplus, and social surplus. Consumer surplus is the gap between the price that consumers are willing to pay and equilibrium price. Producer surplus is the gap between the price for which producers are willing to sell a product and market equilibrium price. Social surplus is the sum of consumer and producer surplus. To calculate these values, first, we need to calculate inverse demand and supply function. Inverse demand function considers price as a function of quantity. We have quantity as a function of a price, hence we need to find its inverse. We have a linear demand curve, given by $$ Q = a - b\\cdot p $$ Let change the notation a little bit and denote demand function in the following way: $$ Q(p) = a - b\\cdot p $$ To find the inverse of this function we have to solve the above function for \\(p\\) . Doing simple algebra, yields $$ p = \\frac{a - Q}{b} $$ and this is the same to write $$ p(Q) = \\frac{a - Q}{b} $$ Doing the same for supply, we easily find inverse supply function, given by $$ p(Q) = \\frac{Q - c}{d} $$ Consumer surplus is the area under inverse demand function and producer surplus is the area above inverse supply function. This may not be clear but after plotting demand and supply curves it will be understandable, what is consumer and producer surplus and how to calculate them. For general sense, we need integrals of inverse demand and supply functions to calculate surplus. See the link for more information. As we have all the necessary information and equations, we can start to code these equations in Python. I'm going to write a simple Python class, which will calculate equilibrium price and quantity, consumer and producer surplus. from scipy.integrate import quad import numpy as np import matplotlib.pyplot as plt from matplotlib.path import Path from matplotlib.patches import PathPatch class Market : def __init__ ( self , a , b , c , d ): self . a = a self . b = b self . c = c self . d = d if a < c : raise ValueError ( 'Insufficient demand!' ) def price ( self ): # Returns equilibrium price return ( self . a - self . c ) / ( self . d + self . b ) def quantity ( self ): # Returns equilibrium quantity return self . a - self . b * self . price () def inverse_demand ( self , x ): return ( self . a - x ) / self . b def inverse_supply ( self , x ): return ( x - self . c ) / self . d def consumer_surplus ( self ): integrand = lambda x : ( self . a - x ) / self . b area , error = quad ( integrand , 0 , self . quantity ()) return area - self . price () * self . quantity () def producer_surplus ( self ): integrand = lambda x : ( x - self . c ) / self . d area , error = quad ( integrand , 0 , self . quantity ()) return self . price () * self . quantity () - area Let, test our model with some base line parameters. a = 15 b = 0.5 c = - 2 d = 0.5 m = Market ( a , b , c , d ) print ( \"Equilibrium Price = \" , m . price ()) print ( \"Equilibrium Quantity = \" , m . quantity ()) Equilibrium Price = 17.0 Equilibrium Quantity = 6.5 print ( 'Consumer Surplus =' , m . consumer_surplus ()) print ( 'Producer Surplus =' , m . producer_surplus ()) Consumer Surplus = 42.25 Producer Surplus = 42.25 Let plot inverse demand and inverse supply curves and shade consumer and producer surplus area. q_max = m . quantity () * 2 q_grid = np . linspace ( 0.0 , q_max , 100 ) pd = m . inverse_demand ( q_grid ) ps = m . inverse_supply ( q_grid ) fig , ax = plt . subplots ( figsize = ( 12 , 6 )) ax . plot ( q_grid , pd , lw = 2 , alpha = 0.6 , label = 'Demand' , color = 'red' ) ax . plot ( q_grid , ps , lw = 2 , alpha = 0.6 , label = 'Supply' , color = 'blue' ) ax . axhline ( 17 , 0 , 0.5 , linewidth = 2 , color = 'black' , linestyle = 'dashed' ) ax . axvline ( 6.5 , 0 , 0.5 , linewidth = 2 , color = 'black' , linestyle = 'dashed' ) path1 = Path ([[ 0 , 17 ],[ 0 , 30 ],[ 6.5 , 17 ],[ 0 , 17 ]]) patch1 = PathPatch ( path1 , facecolor = 'silver' ) ax . add_patch ( patch1 ) path2 = Path ([[ 0 , 4 ],[ 0 , 17 ],[ 6.5 , 17 ],[ 0 , 4 ]]) patch2 = PathPatch ( path2 , facecolor = 'plum' ) ax . add_patch ( patch2 ) ax . text ( 6.5 , 17 , 'Equilibrium Point' , fontsize = 12 ) ax . text ( 1 , 22 , 'Consumer Surplus' , fontsize = 12 ) ax . text ( 1 , 14 , 'Producer Surplus' , fontsize = 12 ) ax . text ( 0 , 17 , '$P&#94;{*}$' , fontsize = 14 ) ax . text ( 6.5 , 0 , '$Q&#94;{*}$' , fontsize = 14 ) ax . set_xlabel ( 'Quantity' ) ax . set_xlim ( 0 , q_max ) ax . set_ylabel ( 'Price' ) ax . legend ( loc = 'best' ) <matplotlib.legend.Legend at 0x7f3a93caf748> In the above graph, we see the equilibrium point. We can ask the following question. What will happen to the equilibrium price and quantity if demand or supply curve shits? Let make a table and summarize the changes in equilibrium due to changes in supply and demand curve. Change Change in \\(P&#94;{*}\\) Change in \\(Q&#94;{*}\\) Supply increases \\(P\\) \\(\\Downarrow\\) \\(Q\\) \\(\\Uparrow\\) Supply decreases \\(P\\) \\(\\Uparrow\\) \\(Q\\) \\(\\Downarrow\\) Demand increases \\(P\\) \\(\\Uparrow\\) \\(Q\\) \\(\\Uparrow\\) Demand decreases \\(P\\) \\(\\Downarrow\\) \\(Q\\) \\(\\Downarrow\\) Demand increases, Supply increases \\(P\\) \\(\\Updownarrow\\) , \\(\\\\(indeterminate)\\) \\(Q\\) \\(\\Uparrow\\) Demand increases, Supply decreases \\(P\\) \\(\\Uparrow\\) \\(Q\\) \\(\\Updownarrow\\) , \\(\\\\(indeterminate)\\) Demand decreases, Supply increases \\(P\\) \\(\\Downarrow\\) \\(Q\\) \\(\\Updownarrow\\) , \\(\\\\(indeterminate)\\) Demand decreases, Supply decreases \\(P\\) \\(\\Updownarrow\\) , \\(\\\\(indeterminate)\\) \\(Q\\) \\(\\Downarrow\\) From the above table, we see that in some cases due to the simultaneous changes in supply and demand we cannot determine equilibrium price or quantity movement. That's the case when other factors are in consideration, but these factors are out of the scope for this blog. To sum up, we saw how to model simple one good competitive market both mathematically and programmatically. This simple economy model is a building block of more complex models. I do hope you gained some understanding of building economic models. If you feel ambitious you can introduce government tax in this model and calculate supply with and without tax as well as a deadweight loss by imposing government tax and see graphically how the tax affects the supply curve. References Khan Academy Quantecon if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Economics","url":"https://dsfabric.org/modeling-single-good-market-in-python","loc":"https://dsfabric.org/modeling-single-good-market-in-python"},{"title":"Taylor Series Expansion with Python","text":"In this blog, I want to review famous Taylor Series Expansion and its special case Maclaurin Series Expansion. According to wikipedia , the aim of Taylor Series Expansion (TSE) is to represent a function as an infinite sum of terms that are derived from the values of that function's derivatives, which in turn are evaluated at some predefined single point. In other words, by using TSE, we try to represent some given function as an infinite sum of its derivatives and these derivates are evaluated at some single point which we can choose. Before diving into mechanics of TSE and its special case Maclaurin Series Expansion (MSE), it's worth to know some history behind these guys. Back in the 17th century the concept of expansion first was introduced by mathematician James Gregory , but in 1715 the notion of function expansion was formally introduced by Brook Taylor . A one-dimensional Taylor series is an expansion of a real function \\(\\mathbb F(x)\\) about a point \\(x=a\\) is given by: $$ \\mathbb F(x) \\approx \\mathbb F(a) + \\mathbb F&#94;{'}(a)\\cdot(x - a) + \\frac{1}{2!}\\cdot\\mathbb F&#94;{''}(a)\\cdot(x - a)&#94;{2} + \\frac{1}{3!}\\cdot\\mathbb F&#94;{3}(a)\\cdot(x - a)&#94;{3} + \\cdots + \\frac{1}{n!}\\cdot\\mathbb F&#94;{n}(a)\\cdot(x - a)&#94;{n} $$ where, \\(n!\\) denotes the factorial of \\(n\\) and \\(\\mathbb F&#94;{n}(a)\\) denotes nth derivative of \\(\\mathbb F\\) evaluated at point \\(a\\) . Every term on the right hand side denotes the order of Taylor expansion. For instance, \\(\\mathbb F(a)\\) is zeroth-order expansion and \\(\\mathbb F&#94;{'}(a)\\cdot(x - a)\\) is the first-order expansion. The above representation is called open-form representation of an expansion. We can write this expansion in more compact notation in the following way: $$ \\sum_{n = 0}&#94;{\\infty} = \\frac{\\mathbb F&#94;{n}(a)}{n!}\\cdot(x - a)&#94;n $$ This is the closed-form representation of an expansion. To see the intuition, let review some example. I'm interested what is Taylor expansion of order 3 of \\(cos(x)\\) at \\(x = a\\) . To follow the above definition we have: $$ \\mathbb F(x) = cos(x) $$ $$ cos(x) \\approx cos(a) - sin(a)\\cdot(x - a) - \\frac{1}{2}\\cdot cos(a)\\cdot (x - a)&#94;2 + \\frac{1}{6}\\cdot sin(a)\\cdot(x - a)&#94;3 $$ You ask, what is \\(a\\) and how can we choose it? \\(a\\) is the point where we want to have \\(cosine\\) approximation and it can be any number from \\(-\\infty\\) to \\(+\\infty\\) . Note that, this is not the case for other functions. We are restricted to choose \\(a\\) from domain of a given function. Now, let do Taylor approximation for \\(sin(x)\\) at \\(x = a\\) $$ \\mathbb F(x) = sin(x) $$ $$ sin(x) \\approx sin(a) + cos(a)\\cdot(x - a) + \\frac{1}{2}\\cdot sin(a)\\cdot (x - a)&#94;2 - \\frac{1}{6}\\cdot cos(a)\\cdot(x - a)&#94;3 $$ We can go further and do Taylor series expansion for exponent \\(e&#94;{x}\\) at \\(x = a\\) is $$ \\mathbb F(x) = e&#94;{x} $$ $$ e&#94;{x} \\approx e&#94;{a} + e&#94;{a}\\cdot(x - a) + \\frac{1}{2}\\cdot e&#94;{a}\\cdot (x - a)&#94;2 + \\frac{1}{6}\\cdot e&#94;{a}\\cdot(x - a)&#94;3 $$ As we have three functions approximations, let choose the value for \\(a\\) and set it to zero and see what we will have. For \\(cos(x)\\) where \\(x = a = 0\\) we have: $$ cos(x) = 1 - \\frac{1}{2}x&#94;2 $$ for \\(sin(x)\\) at \\(x = a = 0\\) we have: $$ sin(x) = x - \\frac{1}{6}x&#94;3 $$ For \\(e&#94;{x}\\) where \\(x = a = 0\\) we have: $$ e&#94;{x} = 1 + x + \\frac{1}{2}x&#94;2 + \\frac{1}{6}x&#94;3 $$ This kind of expansion is known as Maclaurin series expansion, in other words when approximation point is zero we call it Maclaurin expansion. Calculating third order approximation for these functions by hand does not seem too hard, but for higher order it's tedious. To solve this problem we can use Python, namely Sympy if we want to have a symbolic approximation, or Numpy / Scipy to have a numeric approximation. Not to be confused with numeric approximation and approximation point. At \\(a = 0\\) for function \\(\\mathbb F(x) = e&#94;{x}\\) we had Taylor approximation \\(e&#94;{x} = 1 + x + \\frac{1}{2}x&#94;2 + \\frac{1}{6}x&#94;3\\) . If we evaluate this expression at, say \\(x = 1\\) we have function output. In this setup, $$ \\mathbb F(x) = e&#94;{x} \\approx 1 + x + \\frac{1}{2}x&#94;2 + \\frac{1}{6}x&#94;3\\mid_{x=1} $$ and evaluated at \\(x = 1\\) we have \\(\\mathbb F(1) = e&#94;{1} = 2.71828182 \\approx 2.66666666\\) , which is close to the real output. Now let visualize these functions and their Taylor approximations at different points with a different order of expansion. Before visualizing results it's good to have a function which will do symbolic Taylor expansion for higher orders for one variable functions. For multivariable functions, it's up to you. from sympy import series , Symbol from sympy.functions import sin , cos , exp from sympy.plotting import plot import matplotlib.pyplot as plt % matplotlib inline plt . rcParams [ 'figure.figsize' ] = 13 , 10 plt . rcParams [ 'lines.linewidth' ] = 2 # Define symbol x = Symbol ( 'x' ) # Function for Taylor Series Expansion def taylor ( function , x0 , n ): \"\"\" Parameter \"function\" is our function which we want to approximate \"x0\" is the point where to approximate \"n\" is the order of approximation \"\"\" return function . series ( x , x0 , n ) . removeO () While defining \"taylor\" function, in return statement I used \".removeO()\" method. This method is used in series expansion to remove \\(\\mathit O(x&#94;{n})\\) term, which is Landau order term at \\(x = 0\\) and not to be confused with big \\(\\mathit O\\) notation used in computer science, which generally represents the Landau order term at \\(x = \\infty\\) . We can do \\(sin(x)\\) , \\(cos(x)\\) , and \\(e(x)\\) expansion by using Sympy . print ( 'sin(x) =' , taylor ( sin ( x ), 0 , 4 )) print ( 'cos(x) =' , taylor ( cos ( x ), 0 , 4 )) print ( 'e(x) =' , taylor ( exp ( x ), 0 , 4 )) sin(x) = -x**3/6 + x cos(x) = 1 - x**2/2 e(x) = x**3/6 + x**2/2 + x + 1 That's not all. We can evaluate these functions at any point. For instance as we did above for \\(x = 1\\) print ( 'sin(1) =' , taylor ( sin ( x ), 0 , 4 ) . subs ( x , 1 )) print ( 'cos(1) =' , taylor ( cos ( x ), 0 , 4 ) . subs ( x , 1 )) print ( 'e(1) =' , taylor ( exp ( x ), 0 , 4 ) . subs ( x , 1 )) sin(1) = 5/6 cos(1) = 1/2 e(1) = 8/3 As we have all the necessary tools to visualize the results, let do it. Taylor Expansion for Sine # This will plot sine and its Taylor approximations p = plot ( sin ( x ), taylor ( sin ( x ), 0 , 1 ), taylor ( sin ( x ), 0 , 3 ), taylor ( sin ( x ), 0 , 5 ), ( x , - 3.5 , 3.5 ), legend = True , show = False ) p [ 0 ] . line_color = 'blue' p [ 1 ] . line_color = 'green' p [ 2 ] . line_color = 'firebrick' p [ 3 ] . line_color = 'black' p . title = 'Taylor Series Expansion for Sine' p . show () Taylor Expansion for Cosine # This will plot cosine and its Taylor approximations p = plot ( cos ( x ), taylor ( cos ( x ), 0 , 2 ), taylor ( cos ( x ), 0 , 4 ), taylor ( cos ( x ), 0 , 6 ), ( x , - 4.5 , 4.5 ), legend = True , show = False ) p [ 0 ] . line_color = 'blue' p [ 1 ] . line_color = 'green' p [ 2 ] . line_color = 'firebrick' p [ 3 ] . line_color = 'black' p . title = 'Taylor Series Expansion for Cosine' p . show () Taylor Expansion for Exponent # This will plot exponent and its Taylor approximations p = plot ( exp ( x ), taylor ( exp ( x ), 0 , 1 ), taylor ( exp ( x ), 0 , 2 ), taylor ( exp ( x ), 0 , 3 ), ( x , - 2 , 2 ), legend = True , show = False ) p [ 0 ] . line_color = 'blue' p [ 1 ] . line_color = 'green' p [ 2 ] . line_color = 'firebrick' p [ 3 ] . line_color = 'black' p . title = 'Taylor Series Expansion for Exponent' p . show () To conclude, in this post we saw how the Taylor series expansion works and coded it in Python. Taylor series expansion while approximating a function introduces approximation error. The magnitude of error depends on the approximation order. If we increase the order of approximation, the error term will decrease, or we can set the tolerance level for error in advance. In other words, the error term in approximation can be regarded as \\(N&#94;{th}\\) order Remainder term. For \\(\\mathbb F(x)\\) at \\(x = x_{0} = a\\) the remainder term is defined as: $$ R_{n}(x) = \\mathbb F(x) - P_{n}(x) $$ Where, \\(P_{n}(x)\\) is the \\(N&#94;{th}\\) order Taylor polynomial for \\(\\mathbb F(x)\\) at \\(x = x_{0} = a\\) . So, $$ \\mathbb F(x) = P_{n}(x) + R_{n}(x) $$ You may wonder why you need Taylor expansion, but it's very important concept in mathematics and one of mathematical beauty , Euler's Identity is derived from Taylor series expansion of \\(cos(x)\\) , \\(sin(x)\\) , and \\(e(x)\\) . The derivation of Euler's Identity deserves separate post, but if you want to see the derivation, you can take a look at Khan Academy . Moreover, some training algorithms for neural networks, such as Steepest Descent , Newton's method , and Conjugate Gradient uses first or second order Taylor series expansion to minimize performance index. References Commonly Used Taylor Series Wikipedia Khan Academy if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/taylor-series-expansion-with-python","loc":"https://dsfabric.org/taylor-series-expansion-with-python"},{"title":"Simple Interactive Data Visualization with Plotly Express","text":"I fairly believe that, if someone is good at data visualization he/she at least has heard about Hans Rosling. One of the most influential data storyteller, and honestly because of the talk he gave at TED, it influenced me too much that I decided to learn data visualization. Since then, I had some progress as well some stagnation but the most noteworthy thing here is that I got my hands dirty with great data visualization tools such as matplotlib , seaborn , bokeh , altair , and plotly . All of these data visualization tools are great in its own niche and describing the pros and cons of each other is the matter of another separate blog. Having in mind that today's world is more interactive and most data viz wizards like \"one line\" solutions(almost true). Plotly made a huge improvement in this direction. Two days ago they introduced Plotly Express which is a new high-level Python visualization library. Long story short it is the wrapper for Plotly.py that makes building complex charts simple. Note that, at the moment of writing Plotly Express was seperately installable. However, now it's part of Plotly and can be imported by calling import plotly.express as px . Developers main aim for building plotly express: Our main goal with Plotly Express was to make it easier to use Plotly.py for exploration and rapid iteration. Let's go through some example and see how plotly express works. import plotly.express as px import plotly.offline as pyo # Plotly Express has build in datasets. You can load this datasets by uncommenting below. # For expositional purposes, I use well-known gapminder data gapminder = px . data . gapminder () # iris = px.data.iris() # tips = px.data.tips() # election = px.data.election() # wind = px.data.wind() # carshare = px.data.carshare() # Extract data for 2007 gapminder2007 = gapminder . query ( \"year == 2007\" ) gapminder2007 . head () country continent year lifeExp pop gdpPercap iso_alpha iso_num 59 Argentina Americas 2007 75.320 40301927 12779.379640 ARG 4 11 Afghanistan Asia 2007 43.828 31889923 974.580338 AFG 8 23 Albania Europe 2007 76.423 3600523 5937.029526 ALB 12 35 Algeria Africa 2007 72.301 33333216 6223.367465 DZA 24 47 Angola Africa 2007 42.731 12420476 4797.231267 AGO 32 # Simple scatter plot. # GDP Per Capita vs Life Expectancy px . scatter ( gapminder2007 , x = \"gdpPercap\" , y = \"lifeExp\" , labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' }) Pelican produces static html files. So, plotly interactive charts do not show up. Firstly, I converted plotly chart into standalone html file and then added by using IFrame tag. That's why it is here. Each point represents country with its own continent. To see continents we can color each dot by continent name. px . scatter ( gapminder2007 , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' }) And, what if we want to adjust each point size as it to be the country population size? That's should not be a problem. px . scatter ( gapminder2007 , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , size = 'pop' , size_max = 60 , labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' }) If we hover-over the mouse to each data point the point description will appear. It shows continent name, x and y coordinates, and population size. As we already know each point is a country but it's not properly shown in the description. We can achieve this easily. px . scatter ( gapminder2007 , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , size = 'pop' , size_max = 60 , hover_name = 'country' , labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' }) I wrote initial command and at each step added two extra arguments. That's the advantage of Plotly Express. Now, what if want to see how this chart evolves over time. Simply, we need to see what happened before 2007 and make it more interactive. px . scatter ( gapminder , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , size = 'pop' , size_max = 60 , hover_name = 'country' , animation_frame = 'year' , animation_group = 'country' , log_x = True , range_x = [ 100 , 100000 ], range_y = [ 25 , 90 ], labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' , 'pop' : 'Population' }) That's not all. Let plot same data on a map. px . choropleth ( gapminder , locations = 'iso_alpha' , color = 'lifeExp' , hover_name = 'country' , animation_frame = 'year' , color_continuous_scale = px . colors . sequential . Plasma , projection = 'natural earth' , labels = { 'lifeExp' : 'Life Expectancy' }) That was good, but wait. What if you don't like jupyter?, trust me there are such guys That's not a problem. Combine Plotly-Express with Plotly and plot offline to make standalone html file. fig = px . scatter ( gapminder , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , size = 'pop' , size_max = 60 , hover_name = 'country' , animation_frame = 'year' , animation_group = 'country' , log_x = True , range_x = [ 100 , 100000 ], range_y = [ 25 , 90 ], labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' , 'pop' : 'Population' }) # Save figure object as html file pyo . plot ( fig , filename = 'gapminder.html' ) After glorifying plotly express, it is natural to find some drawbacks. Has plotly express some limitations? Particularly, how much data can we put into it? According to them, there is no hard limit for data set size but it is preferable to use 1000 points for some chart types and make some input parameter adjustments, while other chart types easily handle more points. I'm still trying to figure out the mechanics of plotly express and dig deeper, however, it won't be a bad idea to check more, about plotly express here . References Medium Blog Plotly Express Example Gallery","tags":"Data Science","url":"https://dsfabric.org/simple-interactive-data-visualization-with-plotly-express","loc":"https://dsfabric.org/simple-interactive-data-visualization-with-plotly-express"},{"title":"Advances of Linear Algebra with Python - Part II","text":"This is the fourth and last post in blog series about linear algebra. Introduction Basics of linear algebra Intermediate linear algebra Advances in linear algebra : Part I and Part II This is the continuation of the part one and in this post I will introduce you the following topics: Matrix Decompositions Cholesky Decomposition QR Decomposition Eigendecomposition Singular Value Decomposition Inverse of a Square Full Rank Matrix Numerical Representation Cholesky Decomposition QR Decomposition Eigendecomposition Singular Value Decomposition Inverse of a Square Full Rank Matrix Conclusion References Matrix Decomposition Matrix Decompositions In linear algebra, matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. Factorizing a matrix means that we want to find a product of matrices that is equal to the initial matrix. These techniques have a wide variety of uses and consequently, there exist several types of decompositions. Below, I will consider some of them, mostly applicable to machine learning or deep learning. Cholesky Decomposition The Cholesky Decomposition is the factorization of a given symmetric square matrix \\(A\\) into the product of a lower triangular matrix, denoted by \\(L\\) and its transpose \\(L&#94;{T}\\) . This decomposition is named after French artillery officer Andre-Louis Cholesky . The formula is: $$ A = LL&#94;{T} $$ For rough sense, let \\(A\\) be $$ A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} $$ Then we can represent \\(A\\) as $$ A = LL&#94;{T} = \\begin{bmatrix} l_{11} & 0 & 0 \\\\ l_{21} & l_{22} & 0 \\\\ l_{31} & l_{32} & l_{33} \\end{bmatrix} \\cdot \\begin{bmatrix} l_{11} & l_{12} & l_{13} \\\\ 0 & l_{22} & l_{23} \\\\ 0 & 0 & a_{33} \\end{bmatrix} = \\begin{bmatrix} l_{11}&#94;{2} & l_{21}l_{11} & l_{31}l_{11} \\\\ l_{21}l_{11} & l_{21}&#94;{2} + l_{22}&#94;{2} & l_{31}l_{21} + l_{32}l_{22} \\\\ l_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}&#94;{2} + l_{32}&#94;{2} + l_{33}&#94;2 \\end{bmatrix} $$ The diagonal elements of matrix \\(L\\) can be calculated by the following formulas: $$ l_{11} = \\sqrt{a_{11}} \\quad \\quad l_{22} = \\sqrt{a_{22} - l_{21}&#94;{2}} \\quad \\quad l_{33} = \\sqrt{a_{33} - (l_{31}&#94;{2} + l_{32}{2})} $$ And in general, for diagonal elements of the matrix \\(L\\) we have: $$ l_{kk} = \\sqrt{a_{kk} - \\sum_{j = 1}&#94;{k - 1}l_{kj}&#94;{2}} $$ For the elements below the main diagonal, \\(l_{ik}\\) where \\(i > k\\) , the formulas are $$ l_{21} = \\frac{1}{l_{11}}a_{21} \\quad \\quad l_{31} = \\frac{1}{l_{11}}a_{31} \\quad \\quad l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) $$ And the general formula is $$ l_{ik} = \\frac{1}{l_{kk}}\\Big(a_{ik} - \\sum_{j = 1}&#94;{k - 1}l_{ij}l_{kj}\\Big) $$ Messy formulas! Consider a numerical example to see what happen under the hood. We have a matrix \\(A\\) $$ A = \\begin{bmatrix} 25 & 15 & -5 \\\\ 15 & 18 & 0 \\\\ -5 & 0 & 11 \\end{bmatrix} $$ According to the above formulas, let find a lower triangular matrix \\(L\\) . We have $$ l_{11} = \\sqrt{a_{11}} = \\sqrt{25} = 5 \\quad \\quad l_{22} = \\sqrt{a_{22} - l_{21}&#94;{2}} = \\sqrt{18 - 3&#94;{2}} = 3 \\quad \\quad l_{33} = \\sqrt{a_{33} - (l_{31}&#94;{2} + l_{32}&#94;{2})} = \\sqrt{11 - ((-1)&#94;{2} + 1&#94;{2})} = 3 $$ Seems, we have missing non-diagonal elements, which are $$ l_{21} = \\frac{1}{l_{11}}a_{21} = \\frac{1}{5}15 = 3 \\quad \\quad l_{31} = \\frac{1}{l_{11}}a_{31} = \\frac{1}{5}(-5) = -1 \\quad \\quad l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{3}(0 - (-1)\\cdot 3) = 1 $$ So, our matrix \\(L\\) is $$ L = \\begin{bmatrix} 5 & 0 & 0 \\\\ 3 & 3 & 0 \\\\ -1 & 1 & 3 \\end{bmatrix} \\quad \\quad L&#94;{T} = \\begin{bmatrix} 5 & 3 & -1 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 3 \\end{bmatrix} $$ Multiplication of this matrices is up to you. QR Decomposition QR decomposition is another type of matrix factorization, where a given \\(m \\times n\\) matrix \\(A\\) is decomposed into two matrices, \\(Q\\) which is orthogonal matrix, which in turn means that \\(QQ&#94;{T} = Q&#94;{T}Q = I\\) and the inverse of \\(Q\\) equal to its transpose, \\(Q&#94;{T} = Q&#94;{-1}\\) , and \\(R\\) which is upper triangular matrix. Hence, the formula is given by $$ A = QR $$ As \\(Q\\) is an orthogonal matrix, there are three methods to find \\(Q\\) , one is Gramm-Schmidt Process , second is Householder Transformation , and third is Givens Rotation . These methods are out of the scope of this blog post series and hence I'm going to explain all of them in separate blog posts. Consequently, there is no calculation besides python code in numerical representation section. Eigendecomposition Here is the question. What's the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform matrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula: $$ A = Q \\Lambda Q&#94;{-1} $$ \\(A\\) is \\(n\\times n\\) square matrix, \\(Q\\) is the matrix whose columns are the eigenvectors, which in turn are linearly independent and \\(\\Lambda\\) is diagonal matrix of eigenvalues of \\(A\\) and these eigenvalues are not necessarily distinct. To see the detailed steps of this decomposition, consider the aforementioned example of the matrix \\(A\\) for which we already found eigenvalues and eigenvectors. $$ A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} \\quad Q = \\begin{bmatrix} 0 & 1 & 0 \\\\ -2 & 0 & 1 \\\\ 1 & 0 & 2 \\end{bmatrix} \\quad \\Lambda = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 11 \\end{bmatrix} \\quad Q&#94;{-1} = \\begin{bmatrix} 0 & -0.4 & 0.2 \\\\ 1 & 0 & 0 \\\\ 0 & 0.2 & 0.4 \\end{bmatrix} $$ We have all the matrices and now take matrix multiplication according to the above formula. Particularly, multiply \\(Q\\) by \\(\\Lambda\\) and by \\(Q&#94;{-1}\\) . We have to get original matrix \\(A\\) Furthermore, if matrix \\(A\\) is a real symmetric matrix, then eigendecomposition can be performed by the following formula: $$ A = Q \\Lambda Q&#94;{T} $$ The only difference between this formula and above formula is that the matrix \\(A\\) is \\(n\\times n\\) real symmetric square matrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real symmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example: $$ A = \\begin{bmatrix} 6 & 2 \\\\ 2 & 3 \\end{bmatrix} $$ The matrix is symmetric because of the original matrix equal to its transpose, \\(A = A&#94;{T}\\) Its eigenvalues are \\(\\lambda_{1} = 7\\) and \\(\\lambda_{2} = 2\\) and corresponding eigenvectors are $$ v_{\\lambda_{1}} = \\begin{bmatrix} 0.89442719 \\\\ 0.4472136 \\end{bmatrix} \\quad v_{\\lambda_{2}} = \\begin{bmatrix} -0.4472136 \\\\ 0.89442719 \\end{bmatrix} $$ And in this set up, matrices \\(Q\\) , \\(\\Lambda\\) and \\(Q&#94;{T}\\) are the following: $$ Q = \\begin{bmatrix} 0.89442719 & -0.4472136 \\\\ 0.4472136 & 0.89442719 \\\\ \\end{bmatrix} \\quad \\Lambda = \\begin{bmatrix} 7 & 0 \\\\ 0 & 2 \\\\ \\end{bmatrix} \\quad Q&#94;{T} = \\begin{bmatrix} 0.89442719 & 0.4472136 \\\\ -0.4472136 & 0.89442719 \\\\ \\end{bmatrix} $$ Taking matrix product gives initial matrix \\(A\\) . To verify all of this calculation see Python code below. Eigendecomposition cannot be used for non-square matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices. Singular Value Decomposition Singular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition. In this context, generalization means that eigendecomposition is applicable only for square \\(n \\times n\\) matrices, while Singular Value Decomposition (SVD) is applicable for any \\(m \\times n\\) matrices. SVD for a \\(m \\times n\\) matrix \\(A\\) is computed by the following formula: $$ A = U \\ D \\ V&#94;{T} $$ Where, \\(U\\) 's columns are left singular vectors of \\(A\\) , \\(V\\) 's columns are right singular vectors of \\(A\\) and \\(D\\) is a diagonal matrix, not necessarily square matrix, containing singular values of \\(A\\) on main diagonal. Singular values of \\(m \\times n\\) matrix \\(A\\) are the square roots of the eigenvalues of \\(A&#94;{T}A\\) , which is a square matrix. If our initial matrix \\(A\\) is square or \\(n \\times n\\) then singular values coincide eigenvalues. Moreover, all of these defines the path towards eigendecomposition. Let see how this path is defined. Matrices, \\(U\\) , \\(D\\) , and \\(V\\) can be found by transforming \\(A\\) into a square matrix and computing eigenvalues and eigenvectors of this transformed matrix. This transformation is done by multiplying \\(A\\) by its transpose \\(A&#94;{T}\\) . After that, matrices \\(U\\) , \\(D\\) and \\(V\\) are the following: \\(U\\) corresponds to the eigenvectors of \\(AA&#94;{T}\\) \\(V\\) corresponds to eigenvectors of \\(A&#94;{T}A\\) \\(D\\) corresponds to eigenvalues, either \\(AA&#94;{T}\\) or \\(A&#94;{T}A\\) , which are the same Theory almost always seems confusing. Consider a numerical example and Python code below for clarification. Let our initial matrix \\(A\\) be: $$ A = \\begin{bmatrix} 0 & 1 & 0 \\\\ \\sqrt{2} & 2 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix} $$ Here, to use SVD first we need to find \\(AA&#94;{T}\\) and \\(A&#94;{T}A\\) . $$ AA&#94;{T} = \\begin{bmatrix} 2 & 2 & 2 \\\\ 2 & 6 & 2 \\\\ 2 & 2 & 2 \\end{bmatrix} \\quad A&#94;{T}A = \\begin{bmatrix} 2 & 2\\sqrt{2} & 0 \\\\ 2\\sqrt{2} & 6 & 2 \\\\ 0 & 2 & 2 \\end{bmatrix} $$ In the next step, we have to find eigenvalues and eigenvectors for \\(AA&#94;{T}\\) and \\(A&#94;{T}A\\) . The characteristic polynomial is $$ -\\lambda&#94;{3} + 10\\lambda&#94;2 - 16\\lambda $$ with roots equal to \\(\\lambda_{1} = 8\\) , \\(\\lambda_{2} = 2\\) , and \\(\\lambda_{3} = 0\\) . Note that these eigenvalues are the same for the \\(A&#94;{T}A\\) . We need singular values which are square root from eigenvalues. Let denote them by \\(\\sigma\\) such as \\(\\sigma_{1} = \\sqrt{8} = 2\\sqrt{2}\\) , \\(\\sigma_{2} = \\sqrt{2}\\) and \\(\\sigma_{3} = \\sqrt{0} = 0\\) . We now can construct diagonal matrix of singular values: $$ D = \\begin{bmatrix} 2\\sqrt{2} & 0 & 0 \\\\ 0 & \\sqrt{2} & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} $$ Now we have to find matrices \\(U\\) and \\(V\\) . We have everything what we need. First find eigenvectors of \\(AA&#94;{T}\\) for \\(\\lambda_{1} = 8\\) , \\(\\lambda_{2} = 2\\) , and \\(\\lambda_{3} = 0\\) , which are the following: $$ U_{1} = \\begin{bmatrix} \\frac{1}{\\sqrt{6}}\\\\ \\frac{2}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}} \\end{bmatrix} \\quad U_{2} = \\begin{bmatrix} -\\frac{1}{\\sqrt{3}}\\\\ \\frac{1}{\\sqrt{3}} \\\\ -\\frac{1}{\\sqrt{3}} \\end{bmatrix} \\quad U_{3} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}}\\\\ 0 \\\\ -\\frac{1}{\\sqrt{2}} \\end{bmatrix} $$ Note that eigenvectors are normalized. As we have eigenvectors, our \\(U\\) matrix is: $$ U = \\begin{bmatrix} \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\ \\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\ \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} \\end{bmatrix} $$ In the same fashion, we can find matrix \\(V\\) , which is: $$ V = \\begin{bmatrix} \\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\ \\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\ \\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2} \\end{bmatrix} $$ According to the formula we have $$ A = U \\ D \\ V&#94;{T} = \\begin{bmatrix} \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\ \\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\ \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} \\end{bmatrix} \\cdot \\begin{bmatrix} 2\\sqrt{2} & 0 & 0 \\\\ 0 & \\sqrt{2} & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\ \\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\ \\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2} \\end{bmatrix} &#94;{T} = A $$ Inverse of a Square Full Rank Matrix Here, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition. Let's get started. If a matrix \\(A\\) can be eigendecomposed and it has no any eigenvalue equal to zero, then this matrix has the inverse and this inverse is given by: $$ A&#94;{-1} = Q \\Lambda&#94;{-1} Q&#94;{-1} $$ Matrices, \\(Q\\) , and \\(\\Lambda\\) are already known for us. Consider an example: $$ A = \\begin{bmatrix} 1 & 2 \\\\ 4 & 3 \\end{bmatrix} $$ Its eigenvalues are \\(\\lambda_{1} = -1\\) and \\(\\lambda_{2} = 5\\) and eigenvectors are: $$ v_{\\lambda_{1}} = \\begin{bmatrix} -0.70710678 \\\\ 0.70710678 \\end{bmatrix} \\quad v_{\\lambda_{2}} = \\begin{bmatrix} 0.4472136 \\\\ -0.89442719 \\end{bmatrix} $$ Let calculate the inverse of \\(A\\) $$ A&#94;{-1} = Q \\Lambda&#94;{-1} Q&#94;{-1} = \\begin{bmatrix} -0.70710678 & -0.4472136 \\\\ 0.70710678 & -0.89442719 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 & -0 \\\\ 0 & 0.2 \\end{bmatrix} \\cdot \\begin{bmatrix} -0.94280904 & 0.47140452 \\\\ -0.74535599 & -0.74535599 \\end{bmatrix} = \\begin{bmatrix} -0.6 & 0.4 \\\\ 0.8 & -0.2 \\end{bmatrix} $$ Numerical Representation Cholesky Decomposition import numpy as np A = np . array ([[ 25 , 15 , - 5 ], [ 15 , 18 , 0 ], [ - 5 , 0 , 11 ]]) # Cholesky decomposition, find lower triangular matrix L L = np . linalg . cholesky ( A ) # Take transpose L_T = np . transpose ( L ) # Check if it's correct A == np . dot ( L , L_T ) array([[ True, True, True], [ True, True, True], [ True, True, True]]) QR Decomposition import numpy as np A = np . array ([[ 12 , - 51 , 4 ], [ 6 , 167 , - 68 ], [ - 4 , 24 , - 41 ]]) # QR decomposition Q , R = np . linalg . qr ( A ) print ( \"Q =\" , Q , sep = ' \\n ' ) print () print ( \"R =\" , R , sep = ' \\n ' ) print () print ( \"A = QR\" , np . dot ( Q , R ), sep = ' \\n ' ) Q = [[ - 0 . 85714286 0 . 39428571 0 . 33142857 ] [ - 0 . 42857143 - 0 . 90285714 - 0 . 03428571 ] [ 0 . 28571429 - 0 . 17142857 0 . 94285714 ]] R = [[ - 14 . - 21 . 14 .] [ 0 . - 175 . 70 .] [ 0 . 0 . - 35 .]] A = QR [[ 12 . - 51 . 4 .] [ 6 . 167 . - 68 .] [ - 4 . 24 . - 41 .]] Eigendecomposition import numpy as np # Eigendecomposition for non-symmetric matrix A = np . array ([[ 2 , 0 , 0 ], [ 0 , 3 , 4 ], [ 0 , 4 , 9 ]]) eigenvalues1 , eigenvectors1 = np . linalg . eig ( A ) # Form diagonal matrix from eigenvalues L1 = np . diag ( eigenvalues1 ) # Separate eigenvector matrix and take its inverse Q1 = eigenvectors1 inv_Q = np . linalg . inv ( Q1 ) B = np . dot ( np . dot ( Q1 , L1 ), inv_Q ) # Check if B equal to A print ( \"Decomposed matrix B:\" ) print ( B ) # Numpy produces normalized eigenvectors and don't be confused with my calculations above # Eigendecomposition for symmetric matrix C = np . array ([[ 6 , 2 ],[ 2 , 3 ]]) eigenvalues2 , eigenvectors2 = np . linalg . eig ( C ) # Eigenvalues L2 = np . diag ( eigenvalues2 ) # Eigenvectors Q2 = eigenvectors2 Q2_T = Q2 . T D = np . dot ( np . dot ( Q2 , L2 ), Q2 . T ) # Check if D equal to C print () print ( \"Decomposed matrix D:\" ) print ( D ) Decomposed matrix B : [[ 2 . 0 . 0 .] [ 0 . 3 . 4 .] [ 0 . 4 . 9 .]] Decomposed matrix D : [[ 6 . 2 .] [ 2 . 3 .]] Singular Value Decomposition import numpy as np np . set_printoptions ( suppress = True ) # Suppress scientific notation A = np . array ([[ 0 , 1 , 0 ], [ np . sqrt ( 2 ), 2 , 0 ], [ 0 , 1 , 1 ]]) U , D , V = np . linalg . svd ( A ) print ( \"U =\" , U ) print () print ( \"D =\" , D ) print () print ( \"V =\" , V ) B = np . dot ( U , np . dot ( np . diag ( D ), V )) print () print ( \"B =\" , B ) U = [[ - 0 . 32099833 0 . 14524317 - 0 . 93587632 ] [ - 0 . 87192053 - 0 . 43111301 0 . 23215547 ] [ - 0 . 36974946 0 . 8905313 0 . 26502706 ]] D = [ 2 . 75398408 1 . 09310654 0 . 46977627 ] V = [[ - 0 . 44774472 - 0 . 8840243 - 0 . 13425984 ] [ - 0 . 55775521 0 . 15876626 0 . 81467932 ] [ 0 . 69888038 - 0 . 43965249 0 . 56415592 ]] B = [[ 0 . 1 . - 0 . ] [ 1 . 41421356 2 . 0 . ] [ 0 . 1 . 1 . ]] Inverse of a Square Full Rank Matrix import numpy as np A = np . array ([[ 1 , 2 ], [ 4 , 3 ]]) # Eigenvalues and Eigenvectors L , Q = np . linalg . eig ( A ) # Diagonal eigenvalues L = np . diag ( L ) # Inverse inv_L = np . linalg . inv ( L ) # Inverse of igenvector matrix inv_Q = np . linalg . inv ( Q ) # Calculate the inverse of A inv_A = np . dot ( Q , np . dot ( inv_L , inv_Q )) # Print the inverse print ( \"The inverse of A is\" ) print ( inv_A ) The inverse of A is [[-0.6 0.4] [ 0.8 -0.2]] Conclusion In conclusion, my aim was to make linear algebra tutorials which are in absence, while learning machine learning or deep learning. Particularly, existing materials either are pure mathematics books which cover lots of unnecessary(actually they are necessary) things or machine learning books which assume that you already have some linear algebra knowledge. The series starts from very basic and at the end explains some advanced topics. I can say that I tried my best to filter the materials and only explained the most relevant linear algebra topics for machine learning and deep learning. Based on my experience, these tutorials are not enough to master the concepts and all intuitions but the journey should be continuous. Meaning, that you have to practice more and more. References Matrix Decomposition Cholesky Decomposition Matrix Decomposition Introduction To Linear Algebra if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/advances-of-linear-algebra-with-python-part-ii","loc":"https://dsfabric.org/advances-of-linear-algebra-with-python-part-ii"},{"title":"Advances of Linear Algebra with Python - Part I","text":"This is the first part of the fourth and last post in blog series about linear algebra. Introduction Basics of linear algebra Intermediate linear algebra Advances in linear algebra : Part I and Part II In this post I will introduce you to the advances of linear algebra, which in turn includes the following: Vector Basis Vectors Matrix Gaussian Elimination of a Matrix Gauss-Jordan Elimination of a Matrix The Inverse of a Matrix Using Gauss-Jordan Elimination Image of a Matrix Kernel of a Matrix Rank of a Matrix Find the Basis of a Matrix Transformations Linear Transformation Transformations of Magnitude and Amplitude Affine Transformation Eigenvalues Eigenvectors Spectrum and Spectral Radius Numerical Representation Kernel or Null Space of a Matrix Linear Transformations Eigenvalues and Eigenvectors Conclusion for part I References Vector Matrix Vector Basis Vectors In the basics , we saw what is a unit vector. To refresh, the unit vector is the vector with length 1 and the formula is $$ \\hat{X} = \\frac{X}{\\|X\\|} $$ For farther explanation, unit vectors can be used to represent the axes of a Cartesian coordinate system . For example in a three-dimensional Cartesian coordinate system such vectors are: $$ \\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\quad \\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\quad \\hat{k} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} $$ which represents, \\(x\\) , \\(y\\) , and \\(z\\) axes, respectively. For two dimensional space we have $$ \\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\quad \\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$ Let deal with two-dimensional space to catch the idea of basis easily and then generalize this idea for higher dimensions. Imagine, we have vector space or collection of vectors \\(\\vec{V}\\) over the Cartesian coordinate system. This space includes all two-dimensional vectors, or in other words, vectors with only two elements, \\(x\\) , and \\(y\\) . A basis , call it \\(B\\) , of vector space \\(V\\) over the Cartesian coordinate system is a linearly independent subset of \\(V\\) that spans whole vector space \\(V\\) . To be precise, basis \\(B\\) to be the basis it must satisfy two conditions: Linearly independence property - states that all vectors in \\(B\\) are linearly independent The spanning property - states that \\(B\\) spans whole \\(V\\) We can combine these two conditions in one sentence. \\(B\\) is the basis if its all elements are linearly independent and every element of \\(V\\) is a linear combination of elements of \\(B\\) . From these conditions, we can conclude that unit vectors \\(\\hat{i}\\) and \\(\\hat{j}\\) are the basis of \\(\\mathbb{R&#94;2}\\) . This kind of bases are also called standard basis or natural basis . The standard basis are denoted by \\(e_{1}\\) , \\(e_{2}\\) , \\(e_{3}\\) and so on. I will be consistent and use the later notation for standard basis and \\(\\hat{i}\\) , \\(\\hat{j}\\) and \\(\\hat{k}\\) for unit vectors. These standard basis vectors are the basis in the sense that any other vector in \\(V\\) can be expressed uniquely as a linear combination of these unit vectors. For example, every vector \\(v\\) in two-dimensional space can be written as $$ x\\ e_{1} + y\\ e_{2} $$ where \\(e_{1}\\) and \\(e_{2}\\) are unit vectors and \\(x\\) and \\(y\\) are scalar components or elements of the vector \\(v\\) . Now, to generalize the idea for higher dimensions we just have to apply the same logic as above, for \\(\\mathbb{R&#94;3}\\) and more. In \\(\\mathbb{R&#94;3}\\) we have standard basis vectors \\(e_{1}\\) , \\(e_{2}\\) , \\(e_{3}\\) , and generally for \\(\\mathbb{R&#94;n}\\) we have standard basis vector space $$ E = \\begin{bmatrix} e_{1} \\\\ e_{2} \\\\ \\cdots \\\\ e_{n} \\end{bmatrix} $$ To generalize the definition of basis further let consider the following: If elements \\(\\{v_{1}, v_{2},\\cdots,v_{n}\\}\\) of \\(V\\) generate \\(V\\) and in addition they are linearly independent, then \\(\\{v_{1}, v_{2},\\cdots,v_{n}\\}\\) is called a basis of \\(V\\) . We shall say that the elements \\(v_{1}, v_{2},\\cdots,v_{n}\\) constitute or form a basis of V. Vector space \\(V\\) can have several basis. At this stage, the notion of basis seems very abstract even for me, and believe me it was totally unclear for me until I solved some examples by hand. I'll show you how to compute basis after explaining row-echelon and reduced row-echelon forms and you'll understand it. However, it's not enough only to know how to row-reduce the given matrix. It's necessary to know which basis you want. Either column space or row space basis or the basis for nullspace. These notions are explained below and after that, we can find the basis for each of them. Matrix Gaussian Elimination of a Matrix In linear algebra, Gaussian Elimination is the method to solve the system of linear equations. This method is the sequence of operations performed on the coefficient matrix of the system. Except for solving the linear systems, the method can be used to find the rank of a matrix, the determinant as well as the inverse of a square invertible matrix. And what is the sequence of operations? Under this notion, elementary row operations are meant. We've covered it in the previous post but for the refresher, ERO's are: Interchange rows Multiply each element in a row by a non-zero number Multiply a row by a non-zero number and add the result to another row Performing Gaussian elimination results in the matrix in Row Echelon Form (ref). The matrix is said to be in row echelon form if it satisfies the following conditions: The first non-zero element in each row, called the leading entry, is a 1 Each leading entry is in a column, which is the right side of the leading entry in the previous row Below the leading entry in a column, all other entries are zero To catch the idea of this process, let consider the example. Actually, we have no matrix (not necessarily true), we have the system of linear equations in the following form: $$ \\begin{cases} x + 2y - z = 5\\\\ 3x + y - 2z = 9\\\\ -x + 4y + 2z = 0 \\end{cases} $$ Based on these equations we can form the following matrix $$ \\begin{bmatrix} 1 & 2 & -1 \\\\ 3 & 1 & -2 \\\\ -1 & 4 & 2 \\end{bmatrix} $$ This matrix is called coefficient matrix as it contains the coefficients of the linear equations. Having the coefficient matrix, we can rewrite our system in the following form: $$ Ax = b $$ Where \\(A\\) is the coefficient matrix, \\(x\\) is the vector of the unknowns, and \\(b\\) is the vector of the right-hand side components To solve this simultaneous system, the coefficients matrix is not enough. We need something more, on which we can perform ELO's. This matrix is: $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 3 & 1 & -2 & |& 9 \\\\ -1 & 4 & 2 & |& 0 \\end{bmatrix} = [A | b] $$ which is called augmented matrix , which in turn gives us the possibility to perform ELO's, in other words, we do Gaussian elimination and the resulted matrix will be in row echelon form. Using back substitution on the resulted matrix gives the solution to our system of equations. Let do it by hand. We have the initial system $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 3 & 1 & -2 & |& 9 \\\\ -1 & 4 & 2 & |& 0 \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ 3x + y - 2z = 9 \\\\ -x + 4y + 2z = 0 \\end{cases} $$ Then, using ERO's \\(R3 \\rightarrow R3 + R1\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 3 & 1 & -2 & |& 9 \\\\ 0 & 6 & 1 & |& 5 \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ 3x + y - 2z = 9 \\\\ 6y + z = 5 \\end{cases} $$ \\(R2 \\rightarrow R2 - 3R1\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 0 & -5 & 1 & |& -6 \\\\ 0 & 6 & 1 & |& 5 \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ -5y + z = -6 \\\\ 6y + z = 5 \\end{cases} $$ \\(R2 \\rightarrow R2 + R3\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 0 & 1 & 2 & |& -1 \\\\ 0 & 6 & 1 & |& 5 \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ y + 2z = -1 \\\\ 6y + z = 5 \\end{cases} $$ \\(R3 \\rightarrow R3 - 6R2\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 0 & 1 & 2 & |& -1 \\\\ 0 & 0 & -11 & |& 11 \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ y + 2z = -1 \\\\ -11z = 11 \\end{cases} $$ \\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 0 & 1 & 2 & |& -1 \\\\ 0 & 0 & 1 & |& -1 \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\quad (A)\\\\ y + 2z = -1 \\quad (B)\\\\ z = -1 \\quad (C) \\end{cases} $$ Back substitution $$ \\begin{cases} (C) \\quad z = -1 \\\\ (B) \\quad y = -1 - 2z \\quad \\Rightarrow \\quad y = -1 - 2(-1) = 1 \\\\ (A) \\quad x = 5 - 2y + z \\quad \\Rightarrow \\quad x = 5 - 2(1) + (-1) = 2 \\end{cases} $$ Solution $$ x = 2 \\\\ y = 1 \\\\ x = -1 $$ This is the solution of the initial system, as well as the last system and every intermediate system. The matrix obtained in step 5 above is in Row-Echelon form as it satisfied above-mentioned conditions. Note that, starting with a particular matrix, a different sequence of ERO's can lead to different row echelon form Gauss-Jordan Elimination of a Matrix Gaussian elimination performs row operations to produce zeros below the main diagonal of the coefficient matrix to reduce it to row echelon form. Once it's done we perform back substitution to find the solution. However, we can continue performing ERO's to reduce coefficient matrix farther, to produce Reduced Row Echelon Form (rref). The matrix is in reduced row echelon form if it satisfies the following conditions: It is in row echelon form The leading entry in each row is the only non-zero entry in its column Gauss-Jordan elimination starts when Gauss elimination left off. Loosely speaking, Gaussian elimination works from the top down and when it stops, we start Gauss-Jordan elimination from the bottom up. The Reduced Row Echelon Form matrix is the result of Gauss-Jordan Elimination process. We can continue our example from step 5 and see what is Gauss-Jordan elimination. At step 5 we had \\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 0 & 1 & 2 & |& -1 \\\\ 0 & 0 & 1 & |& -1 \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ y + 2z = -1 \\\\ z = -1 \\end{cases} $$ Now, from bottom to up we perform the following ERO's \\(R2 \\rightarrow R2 - 2R3\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 5 \\\\ 0 & 1 & 0 & |& 1 \\\\ 0 & 0 & 1 & |& -1 \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ y = 1 \\\\ z = -1 \\end{cases} $$ \\(R1 \\rightarrow R1 + R3\\) $$ \\begin{bmatrix} 1 & 2 & 0 & |& 4 \\\\ 0 & 1 & 0 & |& 1 \\\\ 0 & 0 & 1 & |& -1 \\end{bmatrix} \\equiv \\begin{cases} x + 2y = 4 \\\\ y = 1 \\\\ z = -1 \\end{cases} $$ \\(R1 \\rightarrow R1 - 2R2\\) $$ \\begin{bmatrix} 1 & 0 & 0 & |& 2 \\\\ 0 & 1 & 0 & |& 1 \\\\ 0 & 0 & 1 & |& -1 \\end{bmatrix} \\equiv \\begin{cases} x = 2 \\\\ y = 1 \\\\ z = -1 \\end{cases} $$ The solution is $$ x = 2 \\\\ y = 1 \\\\ x = -1 $$ and this is the same as the solution of the Gauss elimination. The matrix in step 8 is the Reduced Row Echelon Form of our initial coefficient matrix \\(A\\) . The Inverse of a Matrix Using Gauss-Jordan Elimination Suppose, we have given the matrix and want to find its inverse but do not want to use the technique mentioned in the intermediate post. We can use Gauss-Jordan elimination with little modification to find the inverse of a matrix. To be consistent, I use the above coefficient matrix but not the right-hand side of the system. So, our matrix is $$ A = \\begin{bmatrix} 1 & 2 & -1 \\\\ 3 & 1 & -2 \\\\ -1 & 4 & 2 \\end{bmatrix} $$ To find the inverse of \\(A\\) , we need to augment \\(A\\) by the identity matrix \\(I\\) which has the same dimensions as \\(A\\) . It is a must the identity to have the same dimensions. After augmentation we have $$ [A | I] = \\begin{bmatrix} 1 & 2 & -1 & |& 1 & 0 & 0 \\\\ 3 & 1 & -2 & |& 0 & 1 & 0 \\\\ -1 & 4 & 2 & |& 0 & 0 & 1 \\end{bmatrix} $$ We have to perform elementary row operations in the same way as we did in the above example. Particularly, \\(R3 \\rightarrow R3 + R1\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 1 & 0 & 0\\\\ 3 & 1 & -2 & |& 0 & 1 & 0\\\\ 0 & 6 & 1 & |& 1 & 0 & 1 \\end{bmatrix} $$ \\(R2 \\rightarrow R2 - 3R1\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 1 & 0 & 0\\\\ 0 & -5 & 1 & |& -3 & 1 & -3\\\\ 0 & 6 & 1 & |& 1 & 0 & 1 \\end{bmatrix} $$ \\(R2 \\rightarrow R2 + R3\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 1 & 0 & 0\\\\ 0 & 1 & 2 & |& -2 & 1 & -2\\\\ 0 & 6 & 1 & |& 1 & 0 & 1 \\end{bmatrix} $$ \\(R3 \\rightarrow R3 - 6R2\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 1 & 0 & 0\\\\ 0 & 1 & 2 & |& -2 & 1 & -2\\\\ 0 & 0 & -11 & |& 13 & -6 & 13 \\end{bmatrix} $$ \\(R3 \\rightarrow R3 \\cdot -\\frac{1}{11}\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 1 & 0 & 0\\\\ 0 & 1 & 2 & |& -2 & 1 & -2\\\\ 0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{bmatrix} $$ \\(R2 \\rightarrow R2 - 2R3\\) $$ \\begin{bmatrix} 1 & 2 & -1 & |& 1 & 0 & 0\\\\ 0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\ 0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{bmatrix} $$ \\(R1 \\rightarrow R1 + R3\\) $$ \\begin{bmatrix} 1 & 2 & 0 & |& -\\frac{2}{11} & \\frac{6}{11} & -\\frac{13}{11}\\\\ 0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\ 0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{bmatrix} $$ \\(R1 \\rightarrow R1 - 2R2\\) $$ \\begin{bmatrix} 1 & 0 & 0 & |& -\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\ 0 & 1 & 0 & |& \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\ 0 & 0 & 1 & |& -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{bmatrix} $$ Our inverse of \\(A\\) is $$ A&#94;{-1} = \\begin{bmatrix} -\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\ \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\ -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{bmatrix} $$ Image of a Matrix Let \\(A\\) be \\(m\\times n\\) matrix. Space spanned by its column vectors are called range, image, or column space of a matrix \\(A\\) . The row space is defined similarly. I only consider column space as all the logic is the same for row space. The precise definition is the following: Let \\(A\\) be an \\(m\\times n\\) matrix, with column vectors \\(v_{1}, v_{2}, \\cdots, v_{n}\\) . A linear combination of these vectors is any vector of the following form: \\(c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n}\\) , where \\(c_{1}, c_{2}, \\cdots , c_{n}\\) are scalars. The set of all possible linear combinations of \\(v_{1}, v_{2}, \\cdots , v_{n}\\) is called the column space of \\(A\\) . For example: $$ A = \\begin{bmatrix} 1 & 0\\\\ 0 & 1\\\\ 2 & 0 \\end{bmatrix} $$ Column vectors are: $$ v_{1} = \\begin{bmatrix} 1\\\\ 0\\\\ 2 \\end{bmatrix} \\quad v_{2} = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} $$ A linear combination of \\(v_{1}\\) and \\(v_{2}\\) is any vector of the form $$ c_{1} \\begin{bmatrix} 1\\\\ 0\\\\ 2 \\end{bmatrix} + c_{2} \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} = \\begin{bmatrix} c_{1}\\\\ c_{2}\\\\ 2c_{1} \\end{bmatrix} $$ The set of all such vectors is the column space of \\(A\\) . Kernel of a Matrix In linear algebra, the kernel or a.k.a null space is the solution of the following homogeneous system: $$A\\cdot X = 0$$ where \\(A\\) is a \\(m\\times n\\) matrix and \\(X\\) is a \\(m\\times 1\\) vector and is denoted by \\(Ker(A)\\) . For more clarity, let consider the numerical example. Lat our matrix \\(A\\) be the following: $$ A = \\begin{bmatrix} 2 & 7 & 1 & 3 \\\\ -4 & -2 & 2 & -2 \\\\ -1 & 7 & 3 & 2 \\\\ -2 & 2 & 2 & 0 \\\\ \\end{bmatrix} $$ and our \\(X\\) is $$ X = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\ \\end{bmatrix} $$ We have to form the following system: $$ A\\cdot X = \\begin{bmatrix} 2 & 7 & 1 & 3 \\\\ -4 & -2 & 2 & -2 \\\\ -1 & 7 & 3 & 2 \\\\ -2 & 2 & 2 & 0 \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix} $$ After that, we have to put this system into row-echelon or reduced row-echelon form. Let skip detailed calculation and present only results, which is the last matrix. $$ A = \\begin{bmatrix} 2 & 7 & 1 & 3\\\\ -4 & -2 & 2 & -2\\\\ -1 & 7 & 3 & 2\\\\ -2 & 2 & 2 & 0 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ -4 & -2 & 2 & -2\\\\ 2 & 7 & 1 & 3\\\\ -2 & 2 & 2 & 0 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & -30 & -10 & -10\\\\ 0 & 21 & 7 & 7\\\\ 0 & -12 & -4 & -4 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & 3 & 1 & 1\\\\ 0 & 3 & 1 & 1\\\\ 0 & 3 & 1 & 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & 3 & 1 & 1\\\\ 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ Now, to find the kernel of the original matrix \\(A\\) , we have to solve the following system of equations: $$ \\begin{cases} x_{1} - 7x_{2} - 3x_{3} - 2x_{4} = 0 \\\\ 3x_{2} + x_{3} + x_{4} = 0 \\end{cases} \\rightarrow \\begin{cases} x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4}\\\\ x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\end{cases} $$ From this solution we conclude that the kernel of \\(A\\) is $$ Ker(A) = \\begin{bmatrix} x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\ x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\ x_{3} \\\\ x_{4} \\end{bmatrix} $$ Where, \\(x_{3}\\) and \\(x_{4}\\) are free variables and can be any number in \\(R\\) Note, that both original matrix \\(A\\) and its row-echelon for has the same kernel. This means that row reduction preserves the kernel or null space. Rank of a Matrix In the intermediate tutorial, you saw how to calculate the determinant of a matrix and also saw that any non zero determinant of sub-matrix of the original matrix shows its non-degenerateness. In other words, nonzero determinant gives us information about the rank of the matrix. Also, I said that there was not only one way to find the rank of a matrix. After reviewing Gauss and Gauss-Jordan Elimination and Row-Echelon and Reduced Row-Echelon forms you know that indeed there are other ways to find the determinant of a matrix as well as the rank of the matrix. To keep DRY , here I only consider a numerical example. The code is provided in the intermediate tutorial. Suppose we have matrix \\(A\\) in the following form: $$ A = \\begin{bmatrix} 3 & 2 & -1\\\\ 2 & -3 & -5\\\\ -1 & -4 &- 3 \\end{bmatrix} $$ Perform Elementary Row Operations we get reduced-echelon form: $$ A = \\begin{bmatrix} 3 & 2 & -1\\\\ 2 & -3 & -5\\\\ -1 & -4 & -3 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 4 & 3\\\\ 3 & 2 & -1\\\\ 2 & -3 & -5 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 4 & 3\\\\ 0 & -10 & -10\\\\ 0 & -11 & -11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 4 & 3\\\\ 0 & 1 & 1\\\\ 0 & -11 & -11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 4 & 3\\\\ 0 & 1 & 1\\\\ 0 & 0 & 0 \\end{bmatrix} $$ From the last matrix we see that the nonzero determinant only exists in \\(2\\times2\\) sub-matrices, hence rank of the matrix \\(A\\) is 2. Find the Basis of a Matrix Now we are able to find the basis for column space and row space as well as the basis for the kernel. The columns of a matrix \\(A\\) span the column space but they may not form a basis if the column vectors are linearly dependent. If this is the case, only some subset of these vectors forms the basis. To find the basis for column space we reduce matrix \\(A\\) to reduced row-echelon form. For example: $$ A = \\begin{bmatrix} 2 & 7 & 1 & 3 \\\\ -4 & -2 & 2 & -2 \\\\ -1 & 7 & 3 & 2 \\\\ -2 & 2 & 2 & 0 \\\\ \\end{bmatrix} $$ Row reduced form of \\(A\\) is: $$ B = \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & 3 & 1 & 1\\\\ 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ We see that only column 1 and column 2 are linearly independent in reduced form and hence column 1 and column 2 of the original matrix \\(A\\) form the basis, which is: $$ \\begin{bmatrix} 2 \\\\-4\\\\-1\\\\-2 \\end{bmatrix} \\quad \\text{and} \\quad \\begin{bmatrix} 7\\\\-2 \\\\ 7 \\\\ 2 \\end{bmatrix} $$ To find the basis for row space, let consider different matrix and again let it be \\(A\\) . $$ A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 2 & 7 & 4 \\\\ 1 & 5 & 2 \\end{bmatrix} $$ To reduce \\(A\\) to reduced row-echelon form we have: $$ B = \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} $$ As in column space case, we see that linearly independent, nonzero row vectors are $$ \\begin{bmatrix} 1 \\\\0\\\\2 \\end{bmatrix} \\quad \\text{and} \\quad \\begin{bmatrix} 0\\\\1 \\\\ 0 \\end{bmatrix} $$ To find the basis for kernel let consider our old example. In this case our matrix is: $$ A = \\begin{bmatrix} 2 & 7 & 1 & 3 \\\\ -4 & -2 & 2 & -2 \\\\ -1 & 7 & 3 & 2 \\\\ -2 & 2 & 2 & 0 \\\\ \\end{bmatrix} $$ And its row reduced form is $$ B = \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & 3 & 1 & 1\\\\ 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ We solved this and got the following result: $$ Ker(A) = \\begin{bmatrix} x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\ x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\ x_{3} \\\\ x_{4} \\end{bmatrix} $$ Now to have basis for null space just plug values for \\(x_{3} = 1\\) and \\(x_{4} = 0\\) , resulted vector is $$ \\begin{bmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\\\ 1 \\\\ 0 \\end{bmatrix} $$ The resulted vector is one set of the basis for kernel space. The values for \\(x_{3}\\) and \\(x_{4}\\) are up to you as they are free variables. Transformations Matrices and vectors are used together to manipulate spatial dimensions. This has a lot of applications, including the mathematical generation of 3D computer graphics, geometric modeling, and the training and optimization of machine learning algorithms. Here, I present some types of transformation. However, the list will not be exhaustive. Firstly, define linear transformation: Linear transformation or linear map, is a mapping (function) between two vector spaces that preserves addition and scalar multiplication operations Linear Transformation You can manipulate a vector by multiplying it with a matrix. The matrix acts like a function that operates on an input vector to produce a vector output. Specifically, matrix multiplications of vectors are linear transformations that transform the input vector into the output vector. For example, consider a matrix \\(A\\) and vector \\(v\\) $$ A = \\begin{bmatrix} 2 & 3 \\\\ 5 & 2 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $$ Define transformation \\(T\\) to be: $$ T(\\vec{v}) = A \\vec{v} $$ This transformation is simply dot or inner product and give the following result: $$ T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 8 \\\\ 9 \\end{bmatrix} $$ In this case, both the input and output vector has 2 components. In other words, the transformation takes a 2-dimensional vector and produces a new 2-dimensional vector. Formally we can write this in the following way: $$ T: \\rm I\\!R&#94;{2} \\to \\rm I\\!R&#94;{2} $$ The transformation does not necessarily have to be \\(n \\times n\\) . The dimension of the output vector and the input vector may differ. Rewrite our matrix \\(A\\) and vector \\(v\\) . $$ A = \\begin{bmatrix} 2 & 3 \\\\ 5 & 2 \\\\ 1 & 1 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $$ Apply above transformation gives, $$ T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 8 \\\\ 9 \\\\ 3 \\end{bmatrix} $$ Now, our transformation transforms a vector from 2-dimensional space into 3-dimensional space. We can rite this transformation as $$ T: \\rm I\\!R&#94;{2} \\to \\rm I\\!R&#94;{3} $$ Transformations of Magnitude and Amplitude When we multiply a vector by a matrix we transform it in at least one of the following two ways Scale the length (Magnitude) Change the direction (Amplitude) Change in length (Magnitude), but not change in direction (Amplitude) $$ A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $$ transformation gives, $$ T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} $$ In this case, the resulted vector changed in length but not changed in direction. See code and visualization in Numerical Representation part. Change in direction (Amplitude), but not change in length (Magnitude) $$ A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $$ transformation gives, $$ T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$ This time, resulted vector changed in direction but has the same length. Change in direction (Amplitude) and in length (Magnitude) $$ A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $$ transformation gives, $$ T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} $$ This time the resulted vector changed in the direction as well as the length. Affine Transformation An Affine transformation multiplies a vector by a matrix and adds an offset vector, sometimes referred to as bias . $$ T(\\vec{v}) = A\\vec{v} + \\vec{b} $$ Consider following example $$ T(\\vec{v}) = A\\vec{v} + \\vec{b} = \\begin{bmatrix} 5 & 2\\\\ 3 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} + \\begin{bmatrix} -2\\\\ -6 \\end{bmatrix} = \\begin{bmatrix} 5\\\\ -2 \\end{bmatrix} $$ This kind of transformation is actually the basic block of linear regression, which is a core foundation for machine learning. However, these concepts are out of the scope of this tutorial. Python code is below for this transformation. Eigenvalues Let consider matrix \\(A\\) $$ A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} $$ Now, let multiply this matrix with vector $$ \\vec{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} $$ We have the following: $$ A \\cdot v = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\\\ 0 \\end{bmatrix} = 2 \\cdot v $$ That's the beautiful relationship yes? To prove this is not the only one vector, which can do this try this vector \\(\\vec{v} = [0\\quad 1\\quad 2]\\) instead of old \\(v\\) . You should get \\(11\\cdot \\vec{v}\\) This beautiful relationship comes from the notion of eigenvalues and eigenvectors. In this case \\(2\\) and \\(11\\) are eigenvalues of the matrix \\(A\\) . Let formalize the notion of eigenvalue and eigenvector: Let \\(A\\) be an \\(n\\times n\\) square matrix. If \\(\\lambda\\) is a scalar and \\(v\\) is non-zero vector in \\(\\mathbb{R&#94;n}\\) such that \\(Av = \\lambda v\\) then we say that \\(\\lambda\\) is an eigenvalue and \\(v\\) is eigenvector I believe you are interested in how to find eigenvalues. Consider again our matrix \\(A\\) and follow steps to find eigenvalues. Given that our matrix \\(A\\) is a square matrix, the condition that characterizes an eigenvalue \\(\\lambda\\) is the existence of a nonzero vector \\(v\\) such that \\(Av = \\lambda v\\) . We can rewrite this equation in the following way: $$ Av = \\lambda v \\\\ Av - \\lambda v = 0 \\\\ Av - \\lambda I v = 0 \\\\ (A - \\lambda I)v = 0 $$ The final form of this equation makes it clear that \\(v\\) is the solution of a square, homogeneous system. To have the nonzero solution(we required it in above definition), then the determinant of the coefficient matrix - \\((A - \\lambda I)\\) must be zero. This is achieved when the columns of the coefficient matrix are linearly dependent. In other words, to find eigenvalues we have to choose \\(\\lambda\\) such that to solve the following equation: $$ det(A - \\lambda I) = 0 $$ This equation is called characteristic equation For more clarity, let solve it with a particular example. We have square matrix \\(A\\) and follow the above equation gives us: $$ det(A - \\lambda I) = det\\Bigg( \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} - \\lambda \\cdot \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\Bigg) = det\\Bigg( \\begin{bmatrix} 2 - \\lambda & 0 & 0 \\\\ 0 & 3 - \\lambda & 4 \\\\ 0 & 4 & 9 - \\lambda \\end{bmatrix} \\Bigg) \\Rightarrow \\\\ \\quad \\\\ \\Rightarrow (2 - \\lambda)[(3 - \\lambda)(9 - \\lambda) - 16] = -\\lambda&#94;3 + 14\\lambda&#94;2 - 35\\lambda + 22 $$ The equation \\(-\\lambda&#94;3 + 14\\lambda&#94;2 - 35\\lambda + 22\\) is called characteristic polynomial of the matrix \\(A\\) and will be of degree \\(n\\) if \\(A\\) is \\(n\\times n\\) The zeros or roots of this characteristic polynomial are the eigenvalues of the original matrix \\(A\\) . In this case the roots are \\(2\\) , \\(1\\) , and \\(11\\) . Surprise! Our matrix \\(A\\) have three eigenvalues and two of them are already known for us from above example. Eigenvalues of a square matrix \\(A\\) have some nice features: The determinant of \\(A\\) equals to the product of the eigenvalues The trace of \\(A\\) (The sum of the elements on the principal diagonal) equal the sum of the eigenvalues If \\(A\\) is symmetric matrix, then all of its eigenvalues are real If \\(A\\) is invertible (The determinant of \\(A\\) is not zero) and \\(\\lambda_{1}, \\cdots, \\lambda_{n}\\) are its eigenvalues, then the eigenvalues of \\(A&#94;{-1}\\) are \\(1 / \\lambda_{1}, \\cdots, 1 / \\lambda_{n}\\) From first feature we have that the matrix is invertible if and only if all its eigenvalues are nonzero. Eigenvectors It's time to calculate eigenvectors, but let firstly define what is eigenvector and how it relates to the eigenvalue. Any nonzero vector \\(v\\) which satisfies characteristic equation is said to be an eigenvector of \\(A\\) corresponding to \\(\\lambda\\) Continue above example and see what are eigenvectors corresponding to eigenvalues \\(\\lambda = 2\\) , \\(\\lambda = 1\\) , and \\(\\lambda = 11\\) , respectively. Eigenvector for \\(\\lambda = 1\\) $$ (A - 1I)\\cdot \\begin{bmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\end{bmatrix} =\\Bigg( \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\Bigg)\\cdot \\begin{bmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 4 \\\\ 0 & 4 & 8 \\end{bmatrix}\\cdot \\begin{bmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\end{bmatrix}= \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} $$ Rewrite this as a system of equations, we'll get $$ \\begin{cases} v_{1} = 0\\\\ 2v_{2} + 4v_{3} = 0\\\\ 4v_{2} + 8v{3} = 0 \\end{cases}\\rightarrow \\begin{cases} v_{1} = 0 \\\\ v_{2} = -2v_{3}\\\\ v_{3} = 1 \\end{cases} \\rightarrow \\begin{cases} v_{1} = 0 \\\\ v_{2} = -2\\\\ v_{3} = 1 \\end{cases} $$ So, our eigenvector corresponding to eigenvalue \\(\\lambda = 1\\) is $$ v_{\\lambda = 1} = \\begin{bmatrix} 0 \\\\ -2 \\\\ 1 \\end{bmatrix} $$ Finding eigenvectors for \\(\\lambda = 2\\) and \\(\\lambda = 11\\) is up to you. Spectrum and Spectral Radius The Spectral Radius of a square matrix \\(A\\) is the largest absolute values of its eigenvalues and is denoted by \\(\\rho(A)\\) . More formally, Spectral radius of a \\(n \\times n\\) matrix \\(A\\) is: $$ \\rho(A) = max \\Big\\{ \\mid \\lambda \\mid \\ : \\lambda \\ is \\ an \\ eigenvalue \\ of \\ A \\Big\\} $$ Stated otherwise, we have $$ \\rho(A) = max \\Big\\{ \\mid \\lambda_{1} \\mid, \\cdots, \\mid \\lambda_{n} \\mid \\Big\\} $$ It's noteworthy that the set of all eigenvalues $$ \\Big\\{ \\lambda : \\lambda \\in \\lambda(A) \\Big\\} $$ is called the Spectrum From above example we had three eigenvalues, \\(\\lambda = 2\\) , \\(\\lambda = 1\\) and \\(\\lambda = 11\\) which are spectrum of \\(A\\) and spectral radius for our matrix \\(A\\) is \\(\\lambda = 11\\) Numerical Representation Kernel or Null Space of a Matrix import numpy as np from scipy.linalg import null_space A = np . array ([[ 2 , 7 , 1 , 3 ], [ - 4 , - 2 , 2 , - 2 ], [ - 1 , 7 , 3 , 2 ],[ - 2 , 2 , 2 , 0 ]]) kernel_A = null_space ( A ) print ( \"Normalized Kernel\" , kernel_A , sep = ' \\n ' ) # This matrix is normalized, meaning that it has unit length # To find unnormalized kernel we have to do the following: # Import sympy from sympy import Matrix B = [[ 2 , 7 , 1 , 3 ], [ - 4 , - 2 , 2 , - 2 ], [ - 1 , 7 , 3 , 2 ],[ - 2 , 2 , 2 , 0 ]] B = Matrix ( B ) kernel_B = B . nullspace () print () print ( \"Unnormalized Kernel\" , kernel_B , sep = ' \\n ' ) # In unnormilized case, we clearly see that sympy automatically choose values for our free variables. # In first case x_3 = 1; x_4 = 0 and in the second case x_3 = 0; x_4 = 1 # Resulted vector(s) are basis for the null space for our matrix A Normalized Kernel [[ 0 . 59408621 0 . 00166 ] [ - 0 . 09787364 - 0 . 40852336 ] [ 0 . 69195985 0 . 41018336 ] [ - 0 . 39833892 0 . 81538673 ]] Unnormalized Kernel [ Matrix ([ [ 2 / 3 ], [ - 1 / 3 ], [ 1 ], [ 0 ]]), Matrix ([ [ - 1 / 3 ], [ - 1 / 3 ], [ 0 ], [ 1 ]])] Linear Transformations import numpy as np import matplotlib.pyplot as plt % matplotlib inline v = np . array ([ 1 , 0 ]) A = np . array ([[ 2 , 0 ], [ 0 , 2 ]]) t = A @v # dot product print ( \"Resulted vector is: t =\" , t ) # Plot v and t vecs = np . array ([ t , v ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . grid () plt . quiver ( * origin , vecs [:, 0 ], vecs [:, 1 ], color = [ 'blue' , 'green' ], scale = 10 ) plt . show () # Original vector v is green and transformed vector t is blue. # Vector t has same direction as v but greater magnitude Resulted vector is: t = [2 0] import numpy as np import matplotlib.pyplot as plt % matplotlib inline v = np . array ([ 1 , 0 ]) A = np . array ([[ 0 , - 1 ], [ 1 , 0 ]]) t = A @v print ( \"Resulted vector is: t =\" , t ) # Plot v and t vecs = np . array ([ v , t ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . grid () plt . quiver ( * origin , vecs [:, 0 ], vecs [:, 1 ], color = [ 'green' , 'blue' ], scale = 10 ) plt . show () # Resulted vector change the direction but has the same length Resulted vector is: t = [0 1] import numpy as np import matplotlib.pyplot as plt % matplotlib inline v = np . array ([ 1 , 0 ]) A = np . array ([[ 2 , 1 ], [ 1 , 2 ]]) t = A @v print ( \"Resulted vector is: t =\" , t ) # Plot v and t vecs = np . array ([ v , t ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . grid () plt . quiver ( * origin , vecs [:, 0 ], vecs [:, 1 ], color = [ 'green' , 'blue' ], scale = 10 ) plt . show () # Resulted vector changed the direction, as well as the length Resulted vector is: t = [2 1] import numpy as np import matplotlib.pyplot as plt % matplotlib inline v = np . array ([ 1 , 1 ]) A = np . array ([[ 5 , 2 ], [ 3 , 1 ]]) b = np . array ([ - 2 , - 6 ]) t = A @v + b print ( \"Resulted vector is: t =\" , t ) # Plot v and t vecs = np . array ([ v , t ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . grid () plt . quiver ( * origin , vecs [:, 0 ], vecs [:, 1 ], color = [ 'green' , 'blue' ], scale = 15 ) plt . show () # The resulted vector t is blue Resulted vector is: t = [ 5 -2] Eigenvalues and Eigenvectors import numpy as np A = np . array ([[ 2 , 0 , 0 ], [ 0 , 3 , 4 ], [ 0 , 4 , 9 ]]) eigenvalues , eigenvectors = np . linalg . eig ( A ) print ( \"Eigenvalues are: \" , eigenvalues ) print () print ( \"Eigenvectors are: \" , eigenvectors , sep = ' \\n ' ) # Note that this eigenvectors seems different from my calculation. However they are not different. # They are normalized to have unit length Eigenvalues are : [ 11 . 1 . 2 .] Eigenvectors are : [[ 0 . 0 . 1 . ] [ 0 . 4472136 0 . 89442719 0 . ] [ 0 . 89442719 - 0 . 4472136 0 . ]] Conclusion for part I In conclusion, part one is relatively heavy but as it contains lots of calculations. That's why we use computers to solve this kind of problems. Despite Numpy's build in functions there is big avenue to write algorithm to compute eigenvalues for instance. That would be very helpful for practicing linear algebra and python simultaneously. The second part is devoted solely for matrix decompositions. References Vector Linear Algebra Done Right Matrix Linear Algebra Topics if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/advances-of-linear-algebra-with-python-part-i","loc":"https://dsfabric.org/advances-of-linear-algebra-with-python-part-i"},{"title":"Intermediates of Linear Algebra with Python - Part II","text":"This is the second part of third post in blog series about linear algebra. Introduction Basics of linear algebra Intermediate linear algebra : Part I and Part II Advances in linear algebra This is the continuation of the part one and in this post I will introduce you the following topics: Matrix Types of Matrices Trace of a Matrix Determinant of a Matrix Minor of a Matrix Cofactor of a Matrix Determinant of a Matrix - continuation Matrix Division Inverse of a Matrix Matrix Division - continuation Solving Systems of Equations with Matrices Elemenraty Row Operations Rank of a Matrix Power of a Matrix Norm of a Matrix Numerical Representation Conclusion Matrix Types of Matrices During years of linear algebra evolution, there appeared different types of matrices. Some of them were fundamentals, some of them appeared lately. In this part, I will introduce some basic types of matrices and give you reference to find some other useful ones. Previously, I talked about the identity matrix, which operates as number 1 in matrix multiplication and is denoted by capital letter \\(I\\) . A square matrix is a matrix with the same number of rows and columns. $$ A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} $$ A diagonal matrix is a matrix in which the entries on principal diagonal are non-zero and all the others are zeros. $$ A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix} $$ Scalar multiple of the identity matrix is called scalar matrix that is also diagonal. This means on the main diagonal all elements are equal. $$ A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix} $$ A square matrix is called triangular matrix if all of its elements above the main diagonal are zero ( lower triangular matrix ) or all of its elements below the main diagonal are zero ( upper triangular matrix ). $$ A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 4 & 5 & 0 \\\\ 7 & 8 & 9 \\end{bmatrix} \\quad A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 5 & 6 \\\\ 0 & 0 & 9 \\end{bmatrix} $$ These matrices are lower and upper triangular matrices, respectively. A null or zero matrix is a matrix with all elements equal to zero. $$ A = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} $$ A matrix of ones is where all elements equal to 1. $$ A = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix} $$ Symmetric matrix is a square matrix that is equal to its own transpose or \\(A = A&#94;T\\) . For example, $$ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix} $$ is a symmetric matrix. Furthermore, matrix elements are symmetric with respect to main diagonal or are equal. A skew-symmetric matrix is a square matrix whose transpose equals its negative or \\(A&#94;T = -A\\) . For example, $$ A = \\begin{bmatrix} 0 & 3 & 4 \\\\ -3 & 0 & 7 \\\\ -4 & -7 & 0 \\end{bmatrix} \\quad A&#94;T = \\begin{bmatrix} 0 & -3 & -4 \\\\ 3 & 0 & -7 \\\\ 4 & 7 & 0 \\end{bmatrix} \\quad -A = \\begin{bmatrix} 0 & -3 & -4 \\\\ 3 & 0 & -7 \\\\ 4 & 7 & 0 \\end{bmatrix} $$ Involutory matrix is a square matrix that is equal to its own inverse. More precisely, it is the matrix whose square is the identity matrix. $$ A = \\begin{bmatrix} -5 & -8 & 0 \\\\ 3 & 5 & 0 \\\\ 1 & 2 & -1 \\end{bmatrix} $$ then $$ A&#94;2 = \\begin{bmatrix} -5 & -8 & 0 \\\\ 3 & 5 & 0 \\\\ 1 & 2 & -1 \\end{bmatrix} \\cdot \\begin{bmatrix} -5 & -8 & 0 \\\\ 3 & 5 & 0 \\\\ 1 & 2 & -1 \\end{bmatrix} \\ = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\ = I $$ A square matrix is called idempotent matrix, if multiplied by itself yields itself. Equivalently, \\(A \\cdot A = A\\) $$ A = \\begin{bmatrix} 2 & -2 & -4 \\\\ -1 & 3 & 4 \\\\ 1 & -2 & -3 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 & -2 & -4 \\\\ -1 & 3 & 4 \\\\ 1 & -2 & -3 \\end{bmatrix} \\ = \\begin{bmatrix} 2 & -2 & -4 \\\\ -1 & 3 & 4 \\\\ 1 & -2 & -3 \\end{bmatrix} $$ A nildepotent matirix is such that \\(A&#94;k = 0\\) for some positive integer \\(k\\) . This means, for some positive \\(k\\) , multipling matrix \\(A\\) by \\(k\\) times gives zero matrix. For matrix \\(A\\) and for \\(k=2\\) we have: $$ A = \\begin{bmatrix} 5 & -3 & 2 \\\\ 15 & -9 & 6 \\\\ 10 & -6 & 4 \\end{bmatrix} $$ $$ \\quad $$ $$ A&#94;2 = \\begin{bmatrix} 5 & -3 & 2 \\\\ 15 & -9 & 6 \\\\ 10 & -6 & 4 \\end{bmatrix} \\cdot \\begin{bmatrix} 5 & -3 & 2 \\\\ 15 & -9 & 6 \\\\ 10 & -6 & 4 \\end{bmatrix} \\ = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} $$ So, as I said there are much much more matrices, but I restricted here due to limited space. If you think you need more, definitely check this wikipedia page . Trace of a Matrix The trace of \\(n\\times n\\) square matrix \\(A\\) is the sum of all elements on the main diagonal. It is defined only for square matrices and the formula is: $$ tr(A)=\\sum_{i=1}&#94;{n}a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn} $$ Where \\(a_{ii}\\) denotes the entry on \\(i\\) -th row and \\(j\\) -th column of matrix A. For example, Let \\(A\\) be a matrix, $$ A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} $$ Then the trace is: $$ tr(A)=\\sum_{i=1}&#94;{3}a_{ii} = a_{11} + a_{22} + a_{33} = 1 + 5 + 9 = 15 $$ Determinant of a Matrix There is not plain English definition of determinant but I'll try to explain it by examples to catch the main idea behind of that special number. However, we can consider determinant as a function which as an input accepts \\(n \\times n\\) matrix and output real or a complex number, that is called determinant of input matrix and is denoted by \\(det(A)\\) or \\(|A|\\) . For any \\(2 \\times 2\\) square matrix \\(A\\) determinant is calculated by: $$ A = \\begin{bmatrix} a & b \\\\ c & d \\\\ \\end{bmatrix} $$ $$ det(A) = ad - bc $$ It seems easy to calculate the determinant of any \\(2 \\times 2\\) matrix right? Now think about how to calculate determinant for higher dimensional matrices...did you find a way? If no, let me explain it step by step. If we have, say \\(3 \\times 3\\) matrix \\(A\\) and want to calculate determinant we need some other notions such as minors and co-factors of that matrix. Minor of a Matrix A minor of matrix \\(A\\) is the determinant of some smaller square matrix. Precisely, the minor \\(M_{i,j}\\) is the determinant of matrix \\(A\\) with row \\(i\\) and column \\(j\\) omitted. Minor of matrix \\(A\\) is denoted by \\(M_{ij}\\) , where \\(i\\) and \\(j\\) denotes element of \\(i\\) -th row and \\(j\\) -th column. Let have general matrix \\(A\\) : $$ A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} $$ We can take rows or columns to find all minors. It's up to you which one you take, rows or columns. Let take the columns. We take the first element of our matrix \\(a_{11}\\) and delete row and column along it. As the first element is \\(a_{11}\\) , we have to delete first row and first column. After that, we take the second element of the first column which is \\(a_{21}\\) and do same or delete second row and first column. After that, we take the third element of the first column \\(a_{31}\\) and delete third row and first column. We have to do these for three columns. After all of that, we have: $$ M_{11} = \\begin{bmatrix} \\square & \\square & \\square \\\\ \\square & a_{22} & a_{23} \\\\ \\square & a_{32} & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{bmatrix} \\ = a_{22}a_{33} - a_{23}a_{32} $$ $$ \\quad $$ $$ M_{21} = \\begin{bmatrix} \\square & a_{12} & a_{13} \\\\ \\square & \\square & \\square \\\\ \\square & a_{32} & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} a_{12} & a_{13} \\\\ a_{32} & a_{33} \\end{bmatrix} \\ = a_{12}a_{33} - a_{13}a_{32} $$ $$ \\quad $$ $$ M_{31} = \\begin{bmatrix} \\square & a_{12} & a_{13} \\\\ \\square & a_{22} & a_{23} \\\\ \\square & \\square & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{12} & a_{13} \\\\ a_{22} & a_{23} \\end{bmatrix} \\ = a_{12}a_{23} - a_{13}a_{22} $$ $$ \\quad $$ $$ M_{12} = \\begin{bmatrix} \\square & \\square & \\square \\\\ a_{21} & \\square & a_{23} \\\\ a_{31} & \\square & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} a_{21} & a_{23} \\\\ a_{31} & a_{33} \\end{bmatrix} \\ = a_{21}a_{33} - a_{23}a_{31} $$ $$ \\quad $$ $$ M_{22} = \\begin{bmatrix} a_{11} & \\square & a_{13} \\\\ \\square & \\square & \\square \\\\ a_{31} & \\square & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} a_{11} & a_{13} \\\\ a_{31} & a_{33} \\end{bmatrix} \\ = a_{11}a_{33} - a_{13}a_{31} $$ $$ \\quad $$ $$ M_{32} = \\begin{bmatrix} a_{11} & \\square & a_{13} \\\\ a_{21} & \\square & a_{23} \\\\ \\square & \\square & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{11} & a_{13} \\\\ a_{21} & a_{13} \\end{bmatrix} \\ = a_{11}a_{13} - a_{13}a_{21} $$ $$ \\quad $$ $$ M_{13} = \\begin{bmatrix} \\square & \\square & \\square \\\\ a_{21} & a_{22} & \\square \\\\ a_{31} & a_{32} & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{bmatrix} \\ = a_{21}a_{32} - a_{22}a_{31} $$ $$ \\quad $$ $$ M_{23} = \\begin{bmatrix} a_{11} & a_{12} & \\square \\\\ \\square & \\square & \\square \\\\ a_{31} & a_{32} & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{31} & a_{32} \\end{bmatrix} \\ = a_{11}a_{32} - a_{12}a_{31} $$ $$ \\quad $$ $$ M_{33} = \\begin{bmatrix} a_{11} & a_{12} & \\square \\\\ a_{21} & a_{22} & \\square \\\\ \\square & \\square & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\ = a_{11}a_{22} - a_{12}a_{21} $$ These nine scalars are minors of matrix \\(A\\) . Once again, minor is not smaller matrix, it is determinant of a smaller matrix. Cofactor of a Matrix We left one more step to compute determinant of \\(3 \\times 3\\) matrix \\(A\\) . This step is cofactor of matrix \\(A\\) . The cofactor of matrix \\(A\\) is the minor, multiplied by \\((-1)&#94;{i+j}\\) and is denoted by \\(C_{ij}\\) $$ C_{ij} = (-1)&#94;{i+j} \\cdot M_{ij} $$ where \\(i\\) is the number of row and \\(j\\) is the number of column of matrix \\(A\\) . In the above case our co-factors are: $$ C_{11} = (-1)&#94;{1+1} \\cdot M_{11} $$ $$ C_{21} = (-1)&#94;{2+1} \\cdot M_{21} $$ $$ C_{31} = (-1)&#94;{3+1} \\cdot M_{31} $$ $$ C_{12} = (-1)&#94;{1+2} \\cdot M_{12} $$ $$ C_{22} = (-1)&#94;{2+2} \\cdot M_{22} $$ $$ C_{32} = (-1)&#94;{3+2} \\cdot M_{32} $$ $$ C_{13} = (-1)&#94;{1+3} \\cdot M_{13} $$ $$ C_{23} = (-1)&#94;{2+3} \\cdot M_{23} $$ $$ C_{33} = (-1)&#94;{3+3} \\cdot M_{33} $$ So, the sum of \\(i\\) and \\(j\\) in the power of \\((-1)\\) switch the sign of every minor. Determinant of a Matrix - continuation We are ready to compute the determinant of our \\(3 \\times 3\\) matrix \\(A\\) . We need to expand this matrix along one of the row or one of the column to compute the determinant. It's up to you which one you take, row or column. Let take the first column. Now, what does expansion means? We have to fix either \\(i\\) if we choose a row, or \\(j\\) if we choose column. At first glance it seems confusing but an example will make sense. This expansion is called Laplace Expansion and is used to compute the determinant of any \\(n \\times n\\) matrix. $$ det(A) = \\sum_{j\\prime=1}&#94;{n}a_{ij\\prime}C_{ij\\prime} \\ = \\sum_{i\\prime=1}&#94;{n}a_{i\\prime j}C_{i\\prime j} $$ where \\(i\\prime\\) means we fixed index \\(i\\) or row and we change only column index. In case of \\(j\\prime\\) we fixed index \\(j\\) or columns and change the only row. So, when \\(i\\) is fixed it is called row expansion and when \\(j\\) is fixed it's called column expansion. \\(C_{ij}\\) is our co-factor. To continue the above example, let expand our initial matrix \\(A\\) by the first column, meaning I fix \\(j\\) to be 1 and only change row index \\(i\\) from 1 to 3. In this particular case above formula is: $$ det(A) = \\sum_{j\\prime=1}&#94;{3}a_{ij\\prime}C_{ij\\prime} \\ = a_{11}C_{11} + a_{21}C_{21} + a_{31}C_{31} $$ Instead, if I choose first row, I have to fix row index \\(i\\) and change column index \\(j\\) from 1 to 3 and determinant formula is: $$ det(A) = \\sum_{i\\prime=1}&#94;{3}a_{i\\prime j}C_{i\\prime j} \\ = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} $$ Below, you will see numerical example. Matrix Division We can't actually divide matrix by a matrix; but when we want to divide matrices, we can take advantage of the fact that division by a given number is the same as multiplication by the reciprocal of that number. For matrix division, we use a related idea; we multiply matrix by the inverse of a matrix. If we have two matrices \\(A\\) and \\(B\\) , we can do: $$ A \\div B = A \\cdot B&#94;{-1} $$ Here, \\(B&#94;{-1}\\) is the inverse of matrix \\(B\\) . As taking inverse of a matrix requires computations and is not easy, let explain it below and then return here. Inverse of a Matrix The matrix is said to be invertible if: $$ A \\cdot A&#94;{-1} = I $$ where \\(I\\) is the identity matrix and \\(A&#94;{-1}\\) is the inverse of \\(A\\) . Generally, matrix inverse is only defined for square matrices, but there still exist ways to take the inverse of non-square matrices but this is out of the scope of this blog series and I will not consider. For \\(2 \\times 2\\) matrix \\(A\\) $$ A = \\begin{bmatrix} a & b \\\\ c & d \\\\ \\end{bmatrix} $$ Inverse of \\(A\\) is: $$ A&#94;{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\\\ \\end{bmatrix} $$ What happened there? - I swapped the positions of a and d - I changed the signs of b and c - I multiplied the resulting matrix by 1 over the determinant of the matrix \\(A\\) For example, $$ A = \\begin{bmatrix} 6 & 2 \\\\ 1 & 2 \\\\ \\end{bmatrix} \\quad A&#94;{-1} = \\frac{1}{(6 \\times 2) - (2 \\times 1)} \\begin{bmatrix} 2 & -2 \\\\ -1 & 6 \\\\ \\end{bmatrix} \\ = \\begin{bmatrix} 0.2 & -0.2 \\\\ -0.1 & 0.6 \\\\ \\end{bmatrix} $$ to check if this is really the inverse of \\(A\\) , multiply \\(A\\) by its inverse in order to get an identity matrix. Now let take the inverse of \\(3 \\times 3\\) matrix. This process is long and involves taking minors, co-factors and determinant. After that, above-mentioned operations should be understandable. It has to be mentioned that there are several ways to take matrix inverse but as I started here explaining minors, co-factors and determinant I use this technique to find inverse. We can calculate the inverse by: step 1: Calculate the matrix of minors step 2: Turn the matrix of minors into the matrix of cofactors step 3: Transpose the matrix of cofactors step 4: Multiply transpose of cofactor by 1/determinant Let have matrix: $$ A = \\begin{bmatrix} 4 & 2 & 2 \\\\ 6 & 2 & 4 \\\\ 2 & 2 & 8 \\end{bmatrix} $$ Step 1: Calculate the matrix of minors $$ M_{11} = \\begin{bmatrix}\\color{blue}4 & \\color{lightgray}2 & \\color{lightgray}2\\\\\\color{lightgray}6 & \\color{red}2 & \\color{red}4\\\\\\color{lightgray}2 & \\color{red}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(2\\times8) - (4\\times2) = 8\\;\\;\\;\\;\\begin{bmatrix}8 & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}? \\end{bmatrix} $$ $$ M_{12} = \\begin{bmatrix}\\color{lightgray}4 & \\color{blue}2 & \\color{lightgray}2\\\\\\color{red}6 & \\color{lightgray}2 & \\color{red}4\\\\\\color{red}2 & \\color{lightgray}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(6\\times8) - (4\\times2) = 40\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} $$ $$ M_{13} = \\begin{bmatrix}\\color{lightgray}4 & \\color{lightgray}2 & \\color{blue}2\\\\\\color{red}6 & \\color{red}2 & \\color{lightgray}4\\\\\\color{red}2 & \\color{red}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(6\\times2) - (2\\times2) = 8\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} $$ $$ M_{21} = \\begin{bmatrix}\\color{lightgray}4 & \\color{red}2 & \\color{red}2\\\\\\color{blue}6 & \\color{lightgray}2 & \\color{lightgray}4\\\\\\color{lightgray}2 & \\color{red}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(2\\times8) - (2\\times2) = 12\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} $$ $$ M_{22} = \\begin{bmatrix}\\color{red}4 & \\color{lightgray}2 & \\color{red}2\\\\\\color{lightgray}6 & \\color{blue}2 & \\color{lightgray}4\\\\\\color{red}2 & \\color{lightgray}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(4\\times8) - (2\\times2) = 28\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} $$ $$ M_{23} = \\begin{bmatrix}\\color{red}4 & \\color{red}2 & \\color{lightgray}2\\\\\\color{lightgray}6 & \\color{lightgray}2 & \\color{blue}4\\\\\\color{red}2 & \\color{red}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(4\\times2) - (2\\times2) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} $$ $$ M_{31} = \\begin{bmatrix}\\color{lightgray}4 & \\color{red}2 & \\color{red}2\\\\\\color{lightgray}6 & \\color{red}2 & \\color{red}4\\\\\\color{blue}2 & \\color{lightgray}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(2\\times4) - (2\\times2) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} $$ $$ M_{32} = \\begin{bmatrix}\\color{red}4 & \\color{lightgray}2 & \\color{red}2\\\\\\color{red}6 & \\color{lightgray}2 & \\color{red}4\\\\\\color{lightgray}2 & \\color{blue}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(4\\times4) - (2\\times6) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & 4 & \\color{lightgray}?\\end{bmatrix} $$ $$ M_{33} = \\begin{bmatrix}\\color{red}4 & \\color{red}2 & \\color{lightgray}2\\\\\\color{red}6 & \\color{red}2 & \\color{lightgray}4\\\\\\color{lightgray}2 & \\color{lightgray}2 & \\color{blue}8\\end{bmatrix}\\;\\;\\;\\;(4\\times2) - (2\\times6) = -4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & 4 & -4\\end{bmatrix} $$ Our matrix of minors is: $$ M = \\begin{bmatrix} 8 & 40 & 8 \\\\ 12 & 28 & 4 \\\\ 4 & 4 & -4 \\end{bmatrix} $$ Note that I used rows to find minors, in contrast to columns in the previous example. Step 2: Turn the matrix of minors into the matrix of cofactors To turn minors matrix into cofactor matrix, we just need to change the sign of elements in minors matrix according to the rule proposed above section. Cofactor matrix is: $$ C = \\begin{bmatrix} 8 & -40 & 8 \\\\ -12 & 28 & -4 \\\\ 4 & -4 & -4 \\end{bmatrix} $$ Step 3: Transpose the matrix of cofactors We need to take the transpose of the cofactor matrix. In other words, swap their positions over the main diagonal (the main diagonal stays the same). $$ C&#94;{T}= \\begin{bmatrix}8 & \\color{green}-\\color{green}1\\color{green}2 & \\color{orange}4\\\\\\color{green}-\\color{green}4\\color{green}0 & 28 & \\color{purple}-\\color{purple}4\\\\\\color{orange}8 & \\color{purple}-\\color{purple}4 & -4\\end{bmatrix} $$ This matrix is called Adjugate or Adjoint , which is simple the transpose of the cofactor matrix. Step 4: Multiply transpose of cofactor by \\(\\frac{1}{determinant}\\) As we did all the necessary operations to have determinant, let compute it firstly and then multiply the adjoint matrix by \\(\\frac{1}{determinant}\\) . Using formula: $$ det(A) = \\sum_{i\\prime=1}&#94;{3}a_{i\\prime j}C_{i\\prime j} \\ = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} $$ We have: $$ det(A) = (4 \\times 8) + (2 \\times (-40)) + (2 \\times 8) = -32 $$ Now the inverse is: $$ A&#94;{-1} = \\frac{1}{-32} \\cdot \\begin{bmatrix} 8 & -40 & 8 \\\\ -12 & 28 & -4 \\\\ 4 & -4 & -4 \\end{bmatrix} \\ = \\begin{bmatrix} -0.25 & 0.375 & -0.125 \\\\ 1.25 & 0.875 & 0.125 \\\\ -0.25 & 0.125 & 0.125 \\end{bmatrix} $$ Let's verify that the original matrix multiplied by the inverse results in an identity matrix: $$ A \\cdot A&#94;{-1} = \\begin{bmatrix}4 & 2 & 2\\\\6 & 2 & 4\\\\2 & 2 & 8\\end{bmatrix} \\cdot \\begin{bmatrix}-0.25 & 0.375 & -0.125\\\\1.25 & -0.875 & 0.125\\\\-0.25 & 0.125 & 0.125\\end{bmatrix} $$ $$ \\begin{bmatrix}(4\\times-0.25)+(2\\times1.25)+(2\\times-0.25) & (4\\times0.375)+(2\\times-0.875)+(2\\times0.125) & (4 \\times-0.125)+(2 \\times-0.125)+(2 \\times0.125)\\\\ (6 \\times-0.25)+(2\\times1.25)+(4\\times-0.25) & (6\\times0.375)+(2 \\times-0.875)+(4\\times0.125) & (6 \\times-0.125)+(2 \\times-0.125)+(4 \\times0.125)\\\\ (2\\times-0.25)+(2\\times1.25)+(8\\times-0.25) & (2 \\times0.375)+(2 \\times-0.875)+(8 \\times0.125) & (2 \\times-0.125)+(2 \\times-0.125)+(8 \\times0.125)\\end{bmatrix} $$ $$ \\begin{bmatrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1\\end{bmatrix} = I $$ Do you see how challenging can be finding the inverse of \\(4 \\times 4\\) matrix? That's why we use calculators or computer program to compute it. Matrix Division - continuation As we already know how to compute the inverse of a matrix, the division is easy now. If we have two matrices: $$ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} $$ and $$ B = \\begin{bmatrix} 4 & 2 & 2 \\\\ 6 & 2 & 4 \\\\ 2 & 2 & 8 \\end{bmatrix} $$ then \\(A\\) divided by \\(B\\) is $$ A \\cdot B&#94;{-1} \\ = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\cdot \\begin{bmatrix} -0.25 & 0.375 & -0.125 \\\\ 1.25 & 0.875 & 0.125 \\\\ -0.25 & 0.125 & 0.125 \\end{bmatrix} \\ = \\begin{bmatrix} 1.5 & -1 & 0.5 \\\\ 3.75 & -2.125 & 0.875 \\\\ 6 & -3.25 & 1.25 \\end{bmatrix} \\ \\equiv A \\div B $$ Solving System of Equations with Matrices In the previous blog, I talked about the system of linear equations and we solved this system graphically and algebraically. One of the great things about matrices, is that they can help us solve systems of equations. For example, consider the following system of equations: $$ \\begin{cases} 2x + 4y = 18 \\\\ 6x + 2y = 34 \\end{cases} $$ We can write this in matrix form, like this: $$ \\begin{bmatrix} 2 & 4 \\\\ 6 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\ = \\begin{bmatrix} 18 \\\\ 34 \\end{bmatrix} $$ If we calculate the dot product between the matrix and vector on the left side, we can see clearly that this represents the original equations. Now let rename our matrices: $$ A = \\begin{bmatrix} 2 & 4 \\\\ 6 & 2 \\end{bmatrix} \\quad X = \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\quad B = \\begin{bmatrix} 18 \\\\ 34 \\end{bmatrix} $$ This can be represented as \\(AX = B\\) and we know that to find \\(X\\) we have to solve this: \\(B \\div A\\) . Since we cannot divide matrices in this way, we have to use the previous technique. Find the inverse of \\(A\\) and multiply by \\(B\\) . The inverse of \\(A\\) : $$ A&#94;{-1} = \\begin{bmatrix} -0.1 & 0.2 \\\\ 0.3 & -0.1 \\end{bmatrix} $$ $$ X = \\begin{bmatrix} -0.1 & 0.2 \\\\ 0.3 & -0.1 \\end{bmatrix} \\cdot \\begin{bmatrix} 18 \\\\ 34 \\end{bmatrix} \\ = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} $$ Now, instead of \\(x\\) and \\(y\\) in the original equation put \\(5\\) and \\(2\\) and this will make equality true. $$ 10 + 8 = 18 $$ $$ 30 + 4 = 34 $$ Elementary Row Operations Elementary row operations (ERO) play an important role in many matrix algebra applications, such as finding the inverse of a matrix and solving simultaneous linear equations. These topics are covered in advance part of the series. An ERO transforms a given matrix \\(A\\) into a new matrix \\(A&#94;{'}\\) via one of the following operations: Interchange two rows (or columns) Multiply each element in a row (or column) by a non-zero number Multiply a row (or column) by a non-zero number and add the result to another row (or column) To catch the idea behind this operations let do the example. We have a matrix \\(A\\) such that $$ A = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 1 & 3 & 5 & 6 \\\\ 0 & 1 & 2 & 3 \\end{bmatrix} $$ Type 1 ERO that interchange rows 1 and 3 of \\(A\\) would yield $$ A&#94;{'} = \\begin{bmatrix} 0 & 1 & 2 & 3 \\\\ 1 & 3 & 5 & 6 \\\\ 1 & 2 & 3 & 4 \\end{bmatrix} $$ Type 2 ERO that multiplies row 2 of \\(A\\) by 3 would yield $$ A&#94;{'} = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 3 & 9 & 15 & 18 \\\\ 0 & 1 & 2 & 3 \\end{bmatrix} $$ Type 3 ERO that multiplies row 2 of \\(A\\) by 4 and replace row 3 of \\(A\\) by \\(4 \\times (\\text{row 2 of A}) + \\text{row 3 of A}\\) would yield row 3 of \\(A&#94;{'}\\) to be \\( 4 \\times [1 \\ 3 \\ 5 \\ 6] + [0 \\ 1 \\ 2 \\ 3] = [4 \\ 13 \\ 22 \\ 27]\\) and $$ A&#94;{'} = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 1 & 3 & 5 & 6 \\\\ 4 & 13 & 22 & 27 \\end{bmatrix} $$ Except this, to perform an elementary row operation on the matrix \\(A\\) , first we can perform the operation on the corresponding identity matrix to obtain an elementary matrix, then multiply \\(A\\) on the left by this elementary matrix. More precisely this means that we take one ERO, whichever we want and perform this operation on corresponding identity matrix of \\(A\\) . If \\(A\\) has \\(m \\times n\\) dimension we have identity matrix \\(I_{m \\times n}\\) . After that, we multiply \\(A\\) by this identity matrix. If we denote the elementary matrix by \\(E\\) then, we multiply \\(A\\) by \\(E\\) in the following way: $$ \\begin{equation}E_{1} \\cdot A \\end{equation} $$ where \\(E_{1}\\) is ERO one performed on identity matrix. Rank of a Matrix The maximum number of linearly independent rows in a matrix \\(A\\) is called the row rank of \\(A\\) and the maximum number of linearly independent columns in \\(A\\) is called the column rank of \\(A\\) . If \\(A\\) is \\(n \\times m\\) matrix, that is if matrix \\(A\\) has \\(m\\) rows and \\(n\\) columns then the following inequality holds: $$ \\text{row rank of} \\ A \\leq m \\\\ \\text{column rank of} \\ A \\leq n $$ Furthermore, for any matrix \\(A\\) $$ \\text{row rank of} \\ A = \\text{column rank of} \\ A $$ From the above inequality it follows that $$ Rank(A) \\leq min(m, n) $$ This means that if a matrix has, for example, 3 rows and 5 columns, its rank cannot be more than 3. The rank of a matrix would be zero if and only if the matrix had no elements. If a matrix had even one element, its minimum rank would be one. When all of the vectors in a matrix are linearly independent, the matrix is said to be full rank . To calculate the rank of a matrix, we have to compute the determinant. It turns out that the rank of a matrix \\(A\\) , denoted by \\(Rank(A)\\) is the size of the largest non-zero \\(m \\times m\\) sub-matrix with non-zero determinant. To simplify further, if the determinant of \\(4 \\times 4\\) matrix \\(A\\) is zero and any \\(3 \\times3\\) sub-matrix of original matrix \\(A\\) has non-zero determinant then the rank of the original matrix \\(A\\) is \\(3\\) . So we can say that rank shows the \"non-degenerateness\" of the matrix \\(A\\) . Actually, there is no only one way to compute the rank. I provided one more way in the advanced tutorial. Power of a Matrix We can rise a square matrix \\(A\\) in any nonnegative power just like any number. This is defined as the product of \\(A\\) by itself \\(n\\) times. If matrix \\(A\\) has inverse, then \\(A&#94;{-n} = (A&#94;{-1})&#94;{n}\\) or take inverse of \\(A\\) and multiply by itself \\(n\\) times. For example, if $$ A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} $$ then $$ A&#94;{2} = A A \\ = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\ = \\begin{bmatrix} 7 & 10 \\\\ 15 & 22 \\end{bmatrix} $$ Norm of a Matrix In the previous post I talked about vector norms but did not mentioned matrix norm. There are three types of matrix norms: Matrix norms induced by vector norms Entrywise matrix norms Schatten norms Here, I will introduce only the first two types of matrix norm and depict one example of each to give the general idea of matrix norms. Induced norms usually are denoted by: $$\\|A\\|_p$$ In the special cases of \\(p = 1, 2, \\infty\\) , the induced matrix norms can be computed by: $$ \\|A\\|_1 = max_{1\\leq j \\leq m} \\sum_{i = 1}&#94;{m}|a_{ij}| $$ Which is the maximum absolute column sum of the matrix \\(A\\) $$ \\|A\\|_2 = \\|A\\|_F = \\sigma_{max}(A) $$ Where \\(\\|A\\|_F\\) is Frobenius Norm, which will be discussed below and \\(\\sigma_{max}(A)\\) is the spectral norm. The later will be discussed in the next post. $$ \\|A\\|_{\\infty} = max_{1\\leq i \\leq m} \\sum_{j = 1}&#94;{n}|a_{ij}| $$ which is the maximum absolute row sum of the matrix \\(A\\) . To clarify this farther, let consider the following example: $$ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} $$ $$ \\|A\\|_1 = max(|1| + |4| + |7|; |2| + |5| + |8|; |3| + |6| + |9|) = max(12; 15; 18) = 18 $$ $$ \\quad $$ $$ \\|A\\|_{\\infty} = max(|1| + |2| + |3|; |4| + |5| + |6|; |7| + |8| + |9|) = max(6; 15; 24) = 24 $$ Imagine, we have a vector whose elements are matrices instead of scalars. Then norm defined here is entrywise matrix norm. The general formula for entrywise matrix norm is: $$ \\|A\\|_{p,q} = \\left(\\sum_{j=1}&#94;{n}\\left(\\sum_{i=1}&#94;{m}|a_{ij}&#94;{p}\\right)&#94;\\frac{q}{p}\\right)&#94;\\frac{1}{q} $$ where \\(p,q \\geq 1\\) When \\(p=q=2\\) we have Frobenius Norm or Frobenius Inner Product: $$ \\|A\\|_{F} = \\sqrt{\\sum_{i=1}&#94;m \\sum_{j=1}&#94;{n}|a_{ij}|&#94;2} $$ and when \\(p=q=\\infty\\) , we have Max Norm: $$ \\|A\\|_{max} = max_{ij}|a_{ij}| $$ If we have matrix \\(A\\) such that: $$ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} $$ Then Frobenius Norm is: $$ \\|A\\|_{F} = \\sqrt{(|1|&#94;2 + |2|&#94;2 + |3|&#94;2 + |4|&#94;2 + |5|&#94;2 + |6|&#94;2 + |7|&#94;2 + |8|&#94;2 + |9|&#94;2)} = \\sqrt{1 + 4 + 9 + 16 + 25 + 36 + 49 + 64 + 81} = \\sqrt{285} \\approx 16.89 $$ For two matrices \\(A\\) and \\(B\\) we have Frobenius Inner Product: $$ \\langle A,B \\rangle_{F} = \\sum_{i,j}\\overline{A_{i,j}}B_{i,j} = tr \\left(\\overline{A&#94;T}B \\right) $$ Where overline denotes the complex conjugate of a matrix. If $$ A = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\\\ \\end{bmatrix} $$ And $$ B = \\begin{bmatrix} 8 & -3 \\\\ 4 & 1 \\\\ \\end{bmatrix} $$ Then $$ \\langle A,B \\rangle_{F} = 2 \\cdot 8 + 0 \\cdot (-3) + 1 \\cdot 4 + 1 \\cdot 1 = 21 $$ Numerical Representation Types of Matrices import numpy as np # Diagonal Matrix diagonal = np . diag ([ 1 , 2 , 3 ]) print ( 'Diagonal Matrix' ) print ( diagonal ) print () # Lower Triangular Matrix low_triang = np . tril ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( 'Lower Triangular Matrix' ) print ( low_triang ) print () # Upper Triangular Matrix upper_triang = np . triu ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( 'Upper Triangular Matrix' ) print ( upper_triang ) print () # Matrix of Zeros zeros = np . zeros (( 3 , 3 ), dtype = int ) # \"dtype=int\" is necessary not to get float values print ( 'Matrix of Zeros' ) print ( zeros ) print () # Matrix of Ones ones = np . ones (( 3 , 3 ), dtype = int ) print ( 'Matrix of Ones' ) print ( ones ) print () # Identity Matrix identity = np . eye ( 3 , dtype = int ) print ( 'Identity Matrix' ) print ( identity ) Diagonal Matrix [[ 1 0 0 ] [ 0 2 0 ] [ 0 0 3 ]] Lower Triangular Matrix [[ 1 0 0 ] [ 4 5 0 ] [ 7 8 9 ]] Upper Triangular Matrix [[ 1 2 3 ] [ 0 5 6 ] [ 0 0 9 ]] Matrix of Zeros [[ 0 0 0 ] [ 0 0 0 ] [ 0 0 0 ]] Matrix of Ones [[ 1 1 1 ] [ 1 1 1 ] [ 1 1 1 ]] Identity Matrix [[ 1 0 0 ] [ 0 1 0 ] [ 0 0 1 ]] Trace of a Matrix import numpy as np A = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) trace = np . trace ( A ) print ( trace ) 15 Determinant of a Matrix import numpy as np A = np . array ([[ 4 , 2 , 2 ], [ 6 , 2 , 4 ], [ 2 , 2 , 8 ]]) determinant = np . linalg . det ( A ) print ( determinant ) -32.000000000000014 Inverse of a Matrix import numpy as np A = np . array ([[ 4 , 2 , 2 ], [ 6 , 2 , 4 ], [ 2 , 2 , 8 ]]) inverse = np . linalg . inv ( A ) print ( inverse ) [[-0.25 0.375 -0.125] [ 1.25 -0.875 0.125] [-0.25 0.125 0.125]] Matrix Division import numpy as np A = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) B = np . array ([[ 4 , 2 , 2 ], [ 6 , 2 , 4 ], [ 2 , 2 , 8 ]]) # A divided by B is dot product btw A and inverse of B inv_B = np . linalg . inv ( B ) X = np . dot ( A , inv_B ) print ( X ) [[ 1.5 -1. 0.5 ] [ 3.75 -2.125 0.875] [ 6. -3.25 1.25 ]] Solving System of Equations with Matrices import numpy as np A = np . array ([[ 2 , 4 ], [ 6 , 2 ]]) B = np . array ([[ 18 ], [ 34 ]]) A_inverse = np . linalg . inv ( A ) print ( 'The inverse of A is' , A_inverse , sep = ' \\n ' ) print () X = np . dot ( A_inverse , B ) print ( 'X =' , X , sep = ' \\n ' ) The inverse of A is [[ - 0 . 1 0 . 2 ] [ 0 . 3 - 0 . 1 ]] X = [[ 5 .] [ 2 .]] Rank of a Matrix import numpy as np A = np . array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ,], [ 1 , - 3 , 5 ]]) rank_A = np . linalg . matrix_rank ( A ) print ( 'Rank(A) = ' , rank_A ) print () # matrix B has full rank B = np . array ([[ 2 , 2 , - 1 ], [ 4 , 0 , 2 ], [ 0 , 6 , - 3 ]]) rank_B = np . linalg . matrix_rank ( B ) print ( 'Rank(B) = ' , rank_B ) Rank ( A ) = 2 Rank ( B ) = 3 Power of a Matrix import numpy as np A = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) A_square = np . linalg . matrix_power ( A , 2 ) print ( A_square ) [[ 7 10] [15 22]] Norm of a Matrix import numpy as np A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) frobenius_norm = np . linalg . norm ( A , ord = 'fro' ) column_max_norm = np . linalg . norm ( A , ord = 1 ) row_max_norm = np . linalg . norm ( A , ord = np . inf ) # same as infinity norm # Frobenius Inner Product A = np . array ([[ 2 , 0 ], [ 1 , 1 ]]) B = np . array ([[ 8 , - 3 ], [ 4 , 1 ]]) frobenius_inner_product = np . vdot ( A , B ) # numpy.vdot function flattens high dimensional arrays and take dot product print ( 'Frobenius Norm is:' , frobenius_norm ) print ( 'Column Max Norm is:' , column_max_norm ) print ( 'Row Max Norm is:' , row_max_norm ) print ( 'Frobenius Inner Product is:' , frobenius_inner_product ) Frobenius Norm is: 16.881943016134134 Column Max Norm is: 18.0 Row Max Norm is: 24.0 Frobenius Inner Product is: 21 Conclusion To sum up both part of intermediate linear algebra, we've reviewed a lot of materials. Some of them seemed easy, while some of them at the first glance seemed complex, but I do hope a little more practice and reading this tutorial 2 times will help you to master all of these intuitions further. For any questions, suggestions, corrections and/or short speeches comment below. References Matrix Introduction To Linear Algebra Linear Algebra Topics Khan Academy - Linear Algebra if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/intermediates-of-linear-algebra-with-python-part-ii","loc":"https://dsfabric.org/intermediates-of-linear-algebra-with-python-part-ii"},{"title":"Intermediates of Linear Algebra with Python - Part I","text":"This is the first part of the third post in blog series about linear algebra. Introduction Basics of linear algebra Intermediate linear algebra : Part I and Part II Advances in linear algebra In this post I will introduce you to the intermediates of linear algebra, which in turn includes the following: Vector Cross Product Span Linear Independence and Dependence Numerical Representation Cross Product Linear Independence and Dependence Conclusion for part I References Vector Vector Cross Product In the case of the dot product between two vectors, we saw that the result is a scalar. In the case of a cross product the result is a vector, so the cross product is also called the vector product. The resulted vector is a vector that is at right angles to both the other vectors in 3D Euclidean space. This means that the cross-product only really makes sense when working with vectors that contain three components. There are two formulas to calculate the cross product. One is algebraic and second is geometric. More precisely, the first formula catches the algebraic intuition of cross product and second catches the geometric intuition of the cross product. If we have two vectors \\(A\\) and \\(B\\) in such a way: $$ A = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ a_{3} \\end{bmatrix} \\quad B = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ b_{3} \\end{bmatrix} $$ The algebraic formula is: $$ \\vec{C} = \\vec{A} \\times \\vec{B} = \\begin{bmatrix} (a_{2} \\cdot b_{3}) - (a_{3} \\cdot b_{2}) \\\\ (a_{3} \\cdot b_{1}) - (a_{1} \\cdot b_{3}) \\\\ (a_{1} \\cdot b_{2}) - (a_{2} \\cdot b_{1}) \\end{bmatrix} $$ At the first glance, the formula is not easy to remember. Let try another formula which uses some advanced components. $$ \\vec{A} \\times \\vec{B} = \\begin{bmatrix} \\vec{i} & \\vec{j} & \\vec{k} \\\\ a_{1} & a_{2} & a_{3} \\\\ b_{1} & b_{2} & b_{3} \\end{bmatrix} $$ where \\(\\vec{i}\\) , \\(\\vec{j}\\) and \\(\\vec{k}\\) are bases vectors and is explained advance part. To find the cross product we have to find the determinant of above matrix in the following way: $$ \\vec{A} \\times \\vec{B} = \\begin{bmatrix} a_{2} & a_{3} \\\\ b_{2} & b_{3} \\end{bmatrix} \\vec{i} \\quad - \\quad \\begin{bmatrix} a_{1} & a_{3} \\\\ b_{1} & b_{3} \\end{bmatrix} \\vec{j} \\quad + \\quad \\begin{bmatrix} a_{1} & a_{2} \\\\ b_{1} & b_{2} \\end{bmatrix} \\vec{k} $$ Suppose we have two vectors: $$ A = \\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\end{bmatrix} \\quad B = \\begin{bmatrix} 1 \\\\ 2 \\\\ -2 \\end{bmatrix} $$ Then cross product is: $$ \\vec{C} = \\vec{A} \\times \\vec{B} = \\begin{bmatrix} (3 \\cdot (-2)) - (1 \\cdot 2) \\\\ (1 \\cdot 1) - (2 \\cdot (-2)) \\\\ (2 \\cdot 2) - (3 \\cdot 1) \\end{bmatrix} = \\begin{bmatrix} (-6) - 2 \\\\ 1 - (-4) \\\\ 4 - 3 \\end{bmatrix} = \\begin{bmatrix} -8 \\\\ 5 \\\\ 1 \\end{bmatrix} $$ The geometric formula is: $$ \\vec{C} = \\vec{A} \\times \\vec{B} = \\|\\vec{A}\\|\\cdot\\|\\vec{B}\\|\\cdot\\sin{\\theta} \\cdot \\hat{n} $$ where, \\(\\theta\\) is the angle between \\(\\vec{A}\\) and \\(\\vec{B}\\) . Also, \\(\\hat{n}\\) is a unit vector perpendicular to both \\(\\vec{A}\\) and \\(\\vec{B}\\) , such that \\(\\vec{A}\\) , \\(\\vec{B}\\) and \\(\\hat{n}\\) form a right-handed system There is also geometric application of the cross product. If we three vectors \\(\\vec{a}\\) , \\(\\vec{b}\\) and \\(\\vec{c}\\) which forms the three dimensional figure such as: Then the area of the parallelogram ( two dimensional front of this object ) is given by: $$ {\\rm{Area}} = \\left\\| {\\vec{a} \\times \\vec{b}} \\right\\| $$ This is the dot product of the result of cross product between vectors \\(\\vec{a}\\) and \\(\\vec{b}\\) The volume of the parallelepiped ( the whole three dimensional object ) is given by: $$ {\\rm{Volume}} = \\left| {\\vec{a} \\centerdot \\left( {\\vec{b} \\times \\vec{c}} \\right)} \\right| $$ here, absolute value bars are necessary since the result could be negative and volume must be positive. Span suppose we have set of vectors: $$ {\\alpha _1},...,{\\alpha _n} \\in A $$ then we can define the space \\(S\\) spanned by: $$ {\\alpha _1},...,{\\alpha _n} $$ as $$ S\\left( {{\\alpha_{1}},...,{\\alpha_{n}}} \\right) = \\left\\{ {\\sum_{i = 1}&#94;{n} {{c_{i}}{\\alpha_{i}}\\;|\\;{c_{i}} \\in \\mathbb{R}} } \\right\\} $$ which is the set of all linear combinations of the vectors in this subspace. The set is a subspace of \\(A\\) : $$ S\\left( {{\\alpha _1},...,{\\alpha _n}} \\right) \\subset A $$ In other words, if we have set of vectors $$ A := \\{a_1, \\ldots, a_k\\} \\in \\mathbb R&#94;n $$ it's natural to think about the new vectors we can create from these vectors by performing linear operations. New vectors created in this way are called linear combinations of \\(A\\) . Particularly, \\(X \\in \\mathbb{R}&#94;n\\) is a linear combination of \\(A\\) if $$ X = \\beta_1 a_1 + \\cdots + \\beta_k a_k \\text{ for some scalars } \\beta_1, \\ldots, \\beta_k $$ here, \\(\\beta_1, \\ldots, \\beta_k\\) are called coefficients of the linear combination. The set of linear combinations of \\(A\\) is the span of \\(A\\) and is written as \\(Span(A)\\) For example, if $$ \\vec{A} = \\{e_1, e_2, e_3\\} \\quad \\text{such that} \\quad e_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} , \\quad e_2 := \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} , \\quad e_3 := \\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} $$ then the span of \\(A\\) is all of \\(\\mathbb{R}&#94;3\\) , because, for any \\(X=(x_1, x_2, x_3)\\in \\mathbb{R}&#94;3\\) , we can write $$ X = x_1 e_1 + x_2 e_2 + x_3 e_3 $$ This means that by using \\(A\\) or vectors \\(e_1\\) , \\(e_2\\) and \\(e_3\\) we can generate any vector in \\(\\mathbb{R}&#94;3\\) by performing linear operations. Linear Independence and Dependence Above, I mentioned a linear combination. In order to define linear dependence and independence let farther clarify what is a linear combination. If we have a set of vectors $$ \\vec{A} = \\{a_1, \\ldots, a_k\\} $$ which all have the same dimension, then A linear combination of the vectors in \\(A\\) is any vector of the form \\(c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k}\\) , where \\(c_{1}, c_{2}, \\ldots, c_{k}\\) are arbitrary scalars For example, if \\(A = \\{[1, 2], [2, 1]\\}\\) , then $$ 2 a_{1} - a_{2} = 2 ([1, 2]) - [2, 1] = [0, 3] $$ is linear combination of the vectors in \\(A\\) . A set \\(A\\) of m-dimensional vectors is linearly independent if the only linear combination of vectors in \\(A\\) that equals \\(0\\) is the trivial linear combination This formal definition seems a little bit confusing. Let consider the example to catch the idea. The set of vectors $$ A = \\{[1, 0], [0, 1]\\} $$ is linearly independent. Let prove this claim. We need to find constants \\(c_{1}\\) and \\(c_{2}\\) satisfying $$ c_{1} ([1, 0]) + c_{2} ([0, 1]) = [0, 0] $$ solving this system of equations gives that \\([c_{1}, c_{2}] = [0, 0] \\rightarrow c_{1} = c_{2} = 0\\) , in turn this implies that \\(A\\) is linearly independent. The following statements are equivalent for a linear independence of \\(A\\) : No vector in \\(A\\) can be formed as a linear combination of the other vectors. This means that if we have three vectors in \\(A\\) , any of them cannot be expressed as the linear combination of the other two. If \\(c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k} = 0\\) , then \\(c_{1} = c_{2} = \\cdots = c_{k} = 0\\) A set \\(A\\) of m-dimensional vectors is linearly dependent if there is a nontrivial linear combination of the vectors in \\(A\\) that adds up to \\(0\\) The set of vectors $$ A = \\{[1, 2], [2, 4]\\} $$ is linearly dependent set of vectors. Let see how. $$ c_{1} ([1, 2]) + c_{2} ([2, 4]) = [0, 0] $$ There is a nontrivial linear combination with \\(c_{1} = 2\\) and \\(c_{2} = 1\\) that yields \\(0\\) . This implies \\(A\\) is the linearly dependent set of vectors. It's easy in this case to spot linear dependence by first glance, as the second vector is 2 times the first vector, which indicates linear dependence. Numerical Representation Cross Product import numpy as np A = np . array ([ 2 , 3 , 1 ]) B = np . array ([ 1 , 2 , - 2 ]) print ( 'A =' , A ) print ( 'B = ' , B ) print () print ( 'Cross Product is:' ) C = np . cross ( A , B ) print ( C ) A = [ 2 3 1 ] B = [ 1 2 - 2 ] Cross Product is : [ - 8 5 1 ] This figure below shows the span of \\(A = \\{a_1, a_2\\}\\) in \\(\\mathbb{R}&#94;3\\) . The span is a 2 dimensional plane passing through these two points and the origin import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D from scipy.interpolate import interp2d % matplotlib inline fig = plt . figure ( figsize = ( 10 , 8 )) ax = fig . gca ( projection = '3d' ) x_min , x_max = - 5 , 5 y_min , y_max = - 5 , 5 a , b = 0.2 , 0.1 ax . set ( xlim = ( x_min , x_max ), ylim = ( x_min , x_max ), zlim = ( x_min , x_max ), xticks = ( 0 , ), yticks = ( 0 , ), zticks = ( 0 , )) gs = 3 z = np . linspace ( x_min , x_max , gs ) y = np . zeros ( gs ) x = np . zeros ( gs ) ax . plot ( x , y , z , 'k-' , lw = 2 , alpha = 0.5 ) ax . plot ( z , x , y , 'k-' , lw = 2 , alpha = 0.5 ) ax . plot ( y , z , x , 'k-' , lw = 2 , alpha = 0.5 ) # Linear function to generate plane def f ( x , y ): return a * x + b * y # Set vector coordinates x_coords = np . array (( 3 , 3 )) y_coords = np . array (( 4 , - 4 )) z = f ( x_coords , y_coords ) for i in ( 0 , 1 ): ax . text ( x_coords [ i ], y_coords [ i ], z [ i ], f '$a_{i+1}$' , fontsize = 14 ) # We need to draw lines from origin to the vectors for i in ( 0 , 1 ): x = ( 0 , x_coords [ i ]) y = ( 0 , y_coords [ i ]) z = ( 0 , f ( x_coords [ i ], y_coords [ i ])) ax . plot ( x , y , z , 'b-' , lw = 1.5 , alpha = 0.6 ) # As we already draw axes and vectors, it's time to plot the plane grid_size = 50 xr2 = np . linspace ( x_min , x_max , grid_size ) yr2 = np . linspace ( y_min , y_max , grid_size ) x2 , y2 = np . meshgrid ( xr2 , yr2 ) z2 = f ( x2 , y2 ) ax . plot_surface ( x2 , y2 , z2 , rstride = 1 , cstride = 1 , cmap = cm . jet , linewidth = 0 , antialiased = True , alpha = 0.2 ) plt . show () Linear Independence and Dependence # Numpy has not a straightforward rule to find linearly independent vectors in some set, so we can use another Python module Sympy # Here, I'll show you how to find linearly independent vectors using Sympy and other implementations are up to you import sympy import numpy as np matrix = np . array ([[ 1 , 2 ],[ 2 , 1 ]]) # this is our set of vectors _ , inds = sympy . Matrix ( matrix ) . T . rref () print ( inds ) (0, 1) This says that the vectors at index 0 and 1 are linearly independent. Let consider a linearly dependent set of vectors to see the result of the above code clearly. matrix = np . array ([[ 0 , 1 , 0 , 0 ],[ 0 , 0 , 1 , 0 ],[ 0 , 1 , 1 , 0 ],[ 1 , 0 , 0 , 1 ]]) # this is our set of vectors _ , inds = sympy . Matrix ( matrix ) . T . rref () print ( inds ) (0, 1, 3) This says that vectors at index 0, 1, and 3 are linearly independent, while vector at index 2 is linearly dependent. Conclusion for part I Here, I only covered half of the materials that I initially had intended to cover. I split those materials into two parts, mainly because to improve readability and maintain consistency. In the second part I review matrices and operations on matrices. References Vector Linear Algebra Done Right Linear Algebra Topics if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/intermediates-of-linear-algebra-with-python-part-i","loc":"https://dsfabric.org/intermediates-of-linear-algebra-with-python-part-i"},{"title":"Basics of Linear Algebra with Python","text":"This is the second post in blog series about linear algebra. Introduction Basics of linear algebra Intermediate linear algebra: Part I and Part II Advances in linear algebra: Part I and Part II In this post I will introduce you to the basics of linear algebra, which in turn includes the following: One Variable Equation Two Variable Equation The Systems of Equations Vectors Vectors and Vector Notation Dimensions of Vector Operations on Vectors Vector Length Unit Vector Scalar Product of Two Vectors Numerical Representation Vector Addition Scalar Multiplication Vector Length Unit Vector Scalar Product Matrix Matrices and Matrix Notation Dimension of Matrices Matrix Operations Matrix Transpose Identity Matrix Numerical Representation Dimension of a Matrix Matrix Addition and Subtraction Nergative Matrix Matrix - Scalar Multiplication Matrix - Matrix Multiplication Matrix Transpose Matrix - Identity Matrix Multiplication Conclusion References Vector Matrix One Variable Equation Generally, equations state that two things are equal. They contain one or more variables and solving them means to find the value of those variable to make equality true. This value is known as a solution. Consider the following equation: $$ 2x + 5 = 15 $$ In this case, our variable is \\(x\\) and the solution is \\(10\\) . x = 5 2 * x + 5 == 15 True Two Variable Equation Equations with two variables are known as linear equations. Consider the following equation: $$ 2y + 3 = 2x - 1 $$ This equation includes two different variables, \\(x\\) , and \\(y\\) . These variables depend on one another. The value of \\(x\\) is determined in part by the value of \\(y\\) and vice-versa. So we can't solve this equation as in the case of one variable equation. However, we can express \\(y\\) in terms of \\(x\\) and obtain a result that describes a relative relationship between the variables. For example, let's solve this equation for \\(y\\) . First, rearrage equation in a way to have following: $$ 2y = 2x - 4 \\Rightarrow \\\\ \\Rightarrow y = x - 2 $$ Note that this is not linear function, this is an affine function . Below we will see the solution of the above equation for various values of \\(y\\) . It's also good practice to plot the graph to visualize the solutions. import pandas as pd # Create a dataframe with a column x, containing values from -10 to 10 df = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) # Add column y, by applying the solved equation to x df [ 'y' ] = df [ 'x' ] - 2 # Display the dataframe #df Above table shows valid solutions for values of \\(x\\) in range \\((-10, 10)\\) . Besides numerical solution, let see the graphical solution. import matplotlib.pyplot as plt % matplotlib inline plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df [ 'x' ], df [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . show () The solution of the above equation lies on the blue line, for any value pairs \\((x,y)~\\in~\\mathbb{R}\\) When we use a linear equation to plot a line, we can easily see where the line intersects the X and Y axes of the plot. These points are known as intercepts . The x-intercept is where the line intersects the X (horizontal) axis, and the y-intercept is where the line intersects the Y (horizontal) axis. The x-intercept is the point where the line crosses the \\(X\\) axis, and at this point, the value for \\(y\\) is always 0. Similarly, the y-intercept is where the line crosses the \\(Y\\) axis, at which \\(x\\) value is 0. So to find the intercepts, we need to solve the equation for \\(x\\) when \\(y\\) is 0 and for \\(y\\) when \\(x\\) is 0. For the x-intercept, we have: $$ y = x - 2 = 0 \\Rightarrow x = 2 $$ For y-intercept, we have: $$ y = x - 2 \\Rightarrow y = 0 - 2 \\Rightarrow y = -2 $$ import matplotlib.pyplot as plt % matplotlib inline plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df [ 'x' ], df [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . annotate ( 'x-intercept' ,( 2 , 0 ), color = 'red' , fontsize = 12 ) plt . annotate ( 'y-intercept' ,( 0 , - 2 ), color = 'red' , fontsize = 12 ) plt . show () It is natural to ask, what if we move one unit along the \\(x\\) axis, how the value for the \\(y\\) change? The answer to this question is the notion of slope . Slope is defined as $$ m = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}} $$ This means that for any given two ordered pairs of \\(x\\) and \\(y\\) , how a change in \\(x\\) affect \\(y\\) . For example: (5, 3) (6, 4) So, according to our formula, slope equal to: $$ m = \\frac{4 - 3}{6 - 5} = 1 $$ So what does that actually mean? Well, if we start from any point on the blue line and move one unit to the right (along with the \\(X\\) axis), we'll need to move 1 unit up (along with the \\(Y\\) axis) to get back to the blue line. The Systems of Equations To have the system of equations means that we have two or more linear equations together and we have to solve them simultaneously to make the equality true. There are three possible solutions of the linear system. One solution, No solution or system is inconsistent and infinitely many solutions. Generally, the linear system can have two or more variables and two or more equations. There, I will consider two variable and two-equation system with three solutions, in order to depict the intuition. It's up to you to delve deeper. The system with one solution, meaning two lines intersect $$ \\begin{cases} 2x-y = 2 \\\\ x+y = -2 \\end{cases} $$ If we divide these equations we'll get \\(x=-2\\) and \\(y=0\\) . This is the solution. Now let see it graphically. import pandas as pd import matplotlib.pyplot as plt % matplotlib inline df_1 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) # Add column y, by applying the solved equation to x df_1 [ 'y' ] = 2 * df_1 [ 'x' ] - 2 df_2 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) df_2 [ 'y' ] = - 1 * df_2 [ 'x' ] - 2 plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df_1 [ 'x' ], df_1 [ 'y' ]) plt . plot ( df_2 [ 'x' ], df_2 [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . annotate ( '2x - y = 2' , ( 7.5 , 18 ), weight = 'bold' ) plt . annotate ( 'x + y = -2' , ( - 10 , 10 ), weight = 'bold' ) plt . annotate ( 'Solution (x = -2 ; y = 0)' ,( 0 , - 3 )) # I put coordinates(0,-3) intentionally to make annotation look clear plt . show () The system with no solution or inconsistent system, meaning two lines are parallel $$ 3x+2y = 3 \\\\ 3x+2y = -4 $$ The system is inconsistent. There is no solution. import pandas as pd import matplotlib.pyplot as plt % matplotlib inline # Dataframe for first equation df_1 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) df_1 [ 'y' ] = ( - 3 / 2 ) * df_1 [ 'x' ] + 3 / 2 # Dataframe for second equation df_2 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) df_2 [ 'y' ] = ( - 3 / 2 ) * df_2 [ 'x' ] - 2 plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df_1 [ 'x' ], df_1 [ 'y' ]) plt . plot ( df_2 [ 'x' ], df_2 [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . annotate ( '3x + 2y = 3' , ( - 5 , 10 ), weight = 'bold' ) plt . annotate ( '3x + 2y = -4' , ( - 10 , 7.5 ), weight = 'bold' ) plt . show () The system with infinitely many solutions, meaning two lines coincide $$ x-y = -3 \\\\ 2x-2y = -6 $$ The system has infinitely many solutions, as one of them is a linear combination of another. In this case, the second equation is scaled by 2 version of the first equation. import pandas as pd import matplotlib.pyplot as plt % matplotlib inline # Dataframe for first equation df_1 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) df_1 [ 'y' ] = df_1 [ 'x' ] - 3 # Dataframe for second equation df_2 = pd . DataFrame ({ 'x' : range ( - 5 , 6 )}) df_2 [ 'y' ] = df_2 [ 'x' ] - 3 plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df_1 [ 'x' ], df_1 [ 'y' ]) plt . plot ( df_2 [ 'x' ], df_2 [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . annotate ( '2x - 2y = -6' , ( 5 , 5 ), weight = 'bold' ) plt . annotate ( 'x - y = -3' , ( - 5 , - 5 ), weight = 'bold' ) plt . show () Vectors Vectors and Vector Notation In plain English, the vector is a directed arrow. Mathematically, the vector is an object that has magnitude and direction. Magnitude is the length of the vector and direction is from its tail to its end. In other words, imagine vector as the line which connects two points in the Cartesian Coordinate System. A vector of length \\(n\\) is a sequence or array of \\(n\\) numbers, which we can write as: $$ \\vec{X} = (x_1, x_2, x_3...x_n) $$ or $$ \\vec{X} = [x_1, x_2, x_3, ... x_n] $$ Horizontally represented vector is a row vector, while vertically represented vector is a column vector. Let see how they look graphically. import matplotlib.pyplot as plt fig , ax = plt . subplots ( figsize = ( 10 , 8 )) # Set the axes through the origin for spine in [ 'left' , 'bottom' ]: ax . spines [ spine ] . set_position ( 'zero' ) for spine in [ 'right' , 'top' ]: ax . spines [ spine ] . set_color ( 'none' ) ax . set ( xlim = ( - 6 , 6 ), ylim = ( - 6 , 6 )) ax . grid () vecs = (( 2 , 4 ), ( - 3 , 3 )) # These are vectors for v in vecs : ax . annotate ( '' , xy = v , xytext = ( 0 , 0 ), arrowprops = dict ( facecolor = 'blue' , shrink = 0 , alpha = 0.7 , width = 0.5 )) ax . text ( 1.1 * v [ 0 ], 1.1 * v [ 1 ], str ( v )) plt . show () Dimensions of Vector The dimension of vector is the number of elements in it. For example, the above vector is row vector with dimension \\(1\\times n\\) , but if we take it as a column vector its dimension will be \\(n\\times 1\\) . $$ \\vec{X} = [x_1,x_2,...x_n]_{1\\times~n} $$ and $$ \\vec{X} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}_{n\\times1} $$ Operations on Vectors The most common operations on vectors are vector addition/subtraction and scalar multiplication. If we have two vectors, \\(\\vec{X}\\) and \\(\\vec{Y}\\) , we can add them up in the following way: $$ \\vec{X} + \\vec{Y} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}_{n\\times1} + \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}_{n\\times1} = \\begin{bmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{bmatrix}_{n\\times1} $$ Multiplying vector by a scalar \\(\\alpha\\) , gives $$ \\alpha \\vec{X} = \\begin{bmatrix} \\alpha x_1 \\\\ \\alpha x_2 \\\\ \\vdots \\\\ \\alpha x_n \\end{bmatrix}_{n\\times1} $$ For example: $$ \\vec{X} + \\vec{Y} = \\begin{bmatrix} 1 \\\\ 2 \\\\ \\vdots \\\\ 100 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 4 \\\\ \\vdots \\\\ 400 \\end{bmatrix} = \\begin{bmatrix} 1 + 3 \\\\ 2 + 4 \\\\ \\vdots \\\\ 100 + 400 \\end{bmatrix} $$ For scalar multiplication, if we have vector \\(X\\) and scalar \\(\\alpha = 5\\) then alpha times \\(X\\) is: $$ 5 * \\vec{X} = \\begin{bmatrix} 5 * 1 \\\\ 5 * 2 \\\\ \\vdots \\\\ 5 * 100 \\end{bmatrix} $$ Vector Length The vector length or magnitude is calculated by the following formula: $$ \\|\\vec{X}\\| = \\sqrt{x_1&#94;2 + x_2&#94;2 + x_3&#94;2 + ... + x_n&#94;2} = \\sqrt{\\sum_{i=1}&#94;n x_i&#94;2} $$ We can link notion of vector length to the Euclidean distance. If our vector \\(\\vec{X}\\) has tail at origin, \\(\\vec{0} = [0_1, 0_2, 0_3, ... , 0_n]\\) and point at \\(\\vec{X} = [x_1,x_2,...x_n]\\) , then Euclidean distance between tail and point is the length of \\(\\vec{X}\\) by the formula: $$ d(\\vec{0},\\vec{X}) = \\sqrt{(0_1 - x_1)&#94;2 + (0_2 - x_2)&#94;2 + ... + (0_n - x_n)&#94;2} = \\sqrt{\\sum_{i=1}&#94;n (0_i - x_i)&#94;2} $$ For example, we have vector \\(X\\) $$ \\vec{X} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} $$ then its length is $$ \\|\\vec{X}\\| = \\sqrt{1&#94;2 + 2&#94;2 + 3&#94;2} = \\sqrt{14} $$ Unit Vector What if the length of a vector equal to 1? This kind of vector is known as unit vector and it plays a very important role in different calculations and formulae. We'll see it in later posts, but here unit vector is defined as $$ \\hat{X} = \\frac{X}{\\|X\\|} $$ where \\(\\hat{X}\\) is a unit vector, the numerator is vector \\(\\vec{X}\\) and denominator is the norm of vector \\(\\vec{X}\\) . We can use vector \\(X\\) from above example. We already calculated it length which is \\(\\|\\vec{X}\\| = \\sqrt{14}\\) . So, we can construct unit vector \\(\\hat{X}\\) in the following way: $$ \\hat{X} = \\frac{X}{\\|X\\|} = \\frac{1}{\\sqrt{14}}; \\frac{2}{\\sqrt{14}}; \\frac{3}{\\sqrt{14}} \\Rightarrow [0.26726124; \\ 0.53452248; \\ 0.80178373] $$ Scalar Product of Two Vectors Multiplication of two vectors is known as dot product, scalar product, or inner product and is defined by: $$ \\langle\\, \\vec{X},\\vec{Y}\\rangle~=~\\vec{X}\\cdot\\vec{Y}~=~x_1\\times y_1 + x_2\\times y_2 + ... + x_n\\times y_n~=~\\sum_{i=1}&#94;n x_i\\cdot y_i $$ The inner product is defined only when the dimensions of two vectors coincide. Another formula of inner product is: $$ \\vec{X}\\cdot\\vec{Y}~=~\\|\\vec{X}\\|\\cdot\\|\\vec{Y}\\|\\cdot\\cos{\\theta} $$ where \\(\\cos{\\theta}\\) is an angle between the vectors \\(\\vec{X}\\) and \\(\\vec{Y}\\) . Numerical Representation import numpy as np # It's more convenient to represent the vector as a Numpy ndarray, rather than Python tuple. X = np . array ([ 1 , 2 , 3 ]) Y = np . array (( 2 , 4 , 6 )) print ( 'X is:' , type ( X )) print ( 'The dimension of X is:' , X . shape ) X is: <class 'numpy.ndarray'> The dimension of X is: (3,) Vector Addition print ( X + Y ) [3 6 9] # I use two dimensional vectors to show graphically the vector addition v = np . array (( 2 , 1 )) w = np . array (( - 3 , 2 )) V = np . array ([ w , v , v + w ]) origin = [ 0 ], [ 0 ] #plt.axis('equal') plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . grid () plt . ticklabel_format ( style = 'sci' , axis = 'both' , scilimits = ( 0 , 0 )) plt . quiver ( * origin , V [:, 0 ], V [:, 1 ], color = [ 'r' , 'b' , 'g' ], scale = 10 ) plt . show () Red is vector \\(\\vec{V}\\) , blue is \\(\\vec{W}\\) and green is the sum of these two vectors Scalar Multiplication import numpy as np X = np . array ([ 1 , 2 , 3 ]) print ( 5 * X ) [ 5 10 15] Vector Length import numpy as np X = np . array ([ 1 , 2 , 3 ]) # In numpy, there are two ways to compute vector length print ( np . sqrt ( np . sum ( X ** 2 ))) # More verbose method print () print ( np . linalg . norm ( X )) 3 . 7416573867739413 3 . 7416573867739413 Unit Vector import numpy as np X = np . array ([ 1 , 2 , 3 ]) n = X / np . linalg . norm ( X ) print ( 'Vector n =' , n ) Vector n = [0.26726124 0.53452248 0.80178373] # If we take length of vector n, we'll get 1 print ( 'The length of vector n is:' , np . linalg . norm ( n )) The length of vector n is: 1.0 Scalar Product import numpy as np # It's more convenient to represent the vector as a Numpy ndarray, rather than Python tuple. X = np . array ([ 1 , 2 , 3 ]) Y = np . array (( 2 , 4 , 6 )) # Numpy have two possible ways to compute vector inner product scalar_prod = np . sum ( X * Y ) dot_prod = np . dot ( X , Y ) print ( 'Scalar Product is:' , dot_prod ) # Compute angle between vector X and vector Y cosine_theta = np . dot ( X , Y ) / ( np . linalg . norm ( X ) * np . linalg . norm ( Y )) # Angle theta theta = np . arccos ( cosine_theta ) print ( 'Cosine theta is:' , cosine_theta ) print ( 'Angle theta is:' , theta ) Scalar Product is: 28 Cosine theta is: 1.0 Angle theta is: 0.0 Matrix Matrices and Matrix Notation Matrix is a rectangular array of numbers and/or expressions that are arranged into rows and columns. These rows and columns can be considered as row and column vectors. So, the matrix is the rectangular array which contains either row or column vectors. Generally, capital letters are used to denote matrix and lower case letters to denote each element of that matrix and I will follow this convention. A matrix arranges numbers into rows and columns, like this: $$ A = \\begin{bmatrix} a_{1,1} & a_{1,2}\\\\ a_{2,1} & a_{2,2} \\end{bmatrix} $$ Here, matrix \\(A\\) has four elements, denoted by lower letter \\(a\\) , where subscripts denote row and column number. For example, \\(a_{2,1}\\) denotes element at the cross of the second row and first column. Dimension of Matrices If a matrix \\(A\\) has \\(n\\) rows and \\(m\\) columns, we call \\(A\\) an \\(n\\times m\\) matrix and it is read as \"n by m matrix\" . A typical \\(n\\times m\\) matrix \\(A\\) can be written as: $$ A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1m} \\\\ a_{21} & a_{22} & \\cdots & a_{2m} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nm} \\end{bmatrix}_{n \\times m} $$ When \\(n=m\\) we have square matrix. To link this matrix to the vector we can rewrite it by the following way: $$ A = \\begin{bmatrix} [a_{11} & a_{12} & \\cdots & a_{1m}] \\\\ [a_{21} & a_{22} & \\cdots & a_{2m}] \\\\ [\\vdots & \\vdots & & \\vdots] \\\\ [a_{n1} & a_{n2} & \\cdots & a_{nm}] \\end{bmatrix}_{n \\times m} $$ Matrix Operations We add two matrices elementwise. To exist this addition we require that the dimensions of the two matrices coincide. If we have two matrices, \\(A\\) and \\(B\\) , addition is defined by: $$ A + B = \\begin{bmatrix} a_{11} & \\cdots & a_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{n1} & \\cdots & a_{nm} \\\\ \\end{bmatrix} + \\begin{bmatrix} b_{11} & \\cdots & b_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ b_{n1} & \\cdots & b_{nm} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} + b_{11} & \\cdots & a_{1m} + b_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{n1} + b_{n1} & \\cdots & a_{nm} + b_{nm} \\\\ \\end{bmatrix} $$ Matrix subtraction is defined in the same fashion as the addition. The nagative of a matrix, is just a matrix with the sign of each element reversed: $$ A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1m} \\\\ a_{21} & a_{22} & \\cdots & a_{2m} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nm} \\end{bmatrix} $$ $$ -A = \\begin{bmatrix} -a_{11} & -a_{12} & \\cdots & -a_{1m} \\\\ -a_{21} & -a_{22} & \\cdots & -a_{2m} \\\\ \\vdots & \\vdots & & \\vdots \\\\ -a_{n1} & -a_{n2} & \\cdots & -a_{nm} \\end{bmatrix} $$ Multiplying matrices is a little more complex than the operations we've seen so far. There are two cases to consider. One is scalar multiplication (multiplying a matrix by a single number), and second is matrix multiplication (multiplying a matrix by another matrix). If we have some scalar or number \\(\\gamma\\) and matrix \\(A\\) , scalar multiplication is: $$ \\gamma A = \\gamma \\begin{bmatrix} a_{11} & \\cdots & a_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{n1} & \\cdots & a_{nm} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\gamma a_{11} & \\cdots & \\gamma a_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ \\gamma a_{n1} & \\cdots & \\gamma a_{nm} \\\\ \\end{bmatrix} $$ To multiply two matrices, we take inner product of \\(i\\) -th row of the matrix \\(A\\) and \\(j\\) -th columns of matrix \\(B\\) . If we have two matrices \\(A\\) is \\(n \\times k\\) and \\(B\\) is \\(j \\times m\\) , then to multiply \\(A\\) and \\(B\\) , we require \\(k=j\\) , and resulting matrix \\(AB\\) is \\(n \\times m\\) . $$ A \\cdot B = \\begin{bmatrix} a_{11} & \\cdots & a_{1k} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{n1} & \\cdots & a_{nk} \\\\ \\end{bmatrix}_{n\\times k} \\cdot \\begin{bmatrix} b_{11} & \\cdots & b_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ b_{j1} & \\cdots & b_{jm} \\\\ \\end{bmatrix}_{j \\times m} = \\begin{bmatrix} (a_{11} \\times b_{11} & +~\\cdots~+ & a_{1k} \\times b_{j1}),~\\cdots~,(a_{11} \\times b_{1m} & +~\\cdots~+ & a_{1k} \\times b_{jm}) \\\\ \\vdots & \\vdots & \\vdots \\\\ (a_{n1} \\times b_{11} & +~\\cdots~+ & a_{nk} \\times b_{j1}),~\\cdots~,(a_{n1} \\times b_{1m} & +~\\cdots~+ & a_{nk} \\times b_{jm})\\\\ \\end{bmatrix}_{n \\times m} $$ If you did not catch the idea of matrix multiplication don't worry. In numerical representation section I will provide simpler example and some extra source to take a look. Note that, in matrix multiplication, \\(A \\cdot B\\) is not same as \\(B \\cdot A\\) . Matrix Transpose In the above example, we saw that the matrix is the collection of vectors. We also know that vectors can be horizontal as well as vertical, or row and column vectors. Now, what if we change in any matrix row vectors into column vectors? This operation is known as transposition. The idea of this operation is to change matrix rows into matrix columns or vice versa, and is denoted by the superscript \\(T\\) . $$ A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1m} \\\\ a_{21} & a_{22} & \\cdots & a_{2m} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nm} \\end{bmatrix}_{n \\times m} $$ Then $$ A&#94;{T} = \\begin{bmatrix} a_{11} & a_{21} & \\cdots & a_{n1} \\\\ a_{12} & a_{22} & \\cdots & a_{n2} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{1m} & a_{2m} & \\cdots & a_{nm} \\end{bmatrix}_{m \\times n} $$ Identity Matrix There are several different types of matrices. In this post, we will introduce only identity matix . Future post will introduce other types of matrices. An identity matrix (usually indicated by a capital \\(I\\) ) is the equivalent in matrix terms of the number 1. It always has the same number of rows as columns, and it has the value 1 in the diagonal element positions I 1,1 , I 2,2 , etc; and 0 in all other element positions. Here's an example of a \\(3 \\times 3\\) identity matrix: $$ I = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}_{3 \\times 3} $$ Multiplying any matrix by an identity matrix is the same as multiplying a number by 1; the result is the same as the original value: Numerical Representation Dimension of a Matrix import numpy as np # 3x3 matrix in Numpy A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) print ( 'Matrix A is' , A , sep = ' \\n ' ) print () print ( 'Dimensions of A is:' , A . shape ) Matrix A is [[ 1 2 3 ] [ 4 5 6 ] [ 7 8 9 ]] Dimensions of A is : ( 3 , 3 ) Matrix Addition and Subtraction import numpy as np A = np . array ([[ 1 , 5 , 3 ],[ 4 , 2 , 8 ],[ 3 , 6 , 9 ]]) B = np . array ([[ 1 , 1 , 3 ],[ 1 , 2 , 8 ],[ 0 , 5 , 3 ]]) A + B A - B print ( 'Matrix A is:' , A , sep = ' \\n ' ) print () print ( 'Matrix B is:' , B , sep = ' \\n ' ) print () print ( 'The sum of them is:' , A + B , sep = ' \\n ' ) print () print ( 'The difference of them is:' , A - B , sep = ' \\n ' ) Matrix A is : [[ 1 5 3 ] [ 4 2 8 ] [ 3 6 9 ]] Matrix B is : [[ 1 1 3 ] [ 1 2 8 ] [ 0 5 3 ]] The sum of them is : [[ 2 6 6 ] [ 5 4 16 ] [ 3 11 12 ]] The difference of them is : [[ 0 4 0 ] [ 3 0 0 ] [ 3 1 6 ]] Nergative Matrix import numpy as np C = np . array ([[ - 5 , - 3 , - 1 ],[ 1 , 3 , 5 ]]) - C print ( 'Matrix C is:' , C , sep = ' \\n ' ) print () print ( 'The negative of C is:' , - C , sep = ' \\n ' ) Matrix C is : [[ - 5 - 3 - 1 ] [ 1 3 5 ]] The negative of C is : [[ 5 3 1 ] [ - 1 - 3 - 5 ]] Matrix - Scalar Multiplication import numpy as np A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) scalar = 2 scalar * A print ( 'Matrix A is:' , A , sep = ' \\n ' ) print () print ( 'Scalar is:' , scalar , sep = ' \\n ' ) print () print ( 'Matrix multiplied by the scalar is:' , scalar * A , sep = ' \\n ' ) Matrix A is : [[ 1 2 3 ] [ 4 5 6 ] [ 7 8 9 ]] Scalar is : 2 Matrix multiplied by the scalar is : [[ 2 4 6 ] [ 8 10 12 ] [ 14 16 18 ]] As I mentioned earlier, matrix multiplication seems to be tricky at a first glance. Let's look at an example: $$ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix} 9 & 8 \\\\ 7 & 6 \\\\ 5 & 4 \\end{bmatrix} $$ Note that the first matrix is \\(2\\times 3\\) , and the second matrix is \\(3\\times 2\\) . The important thing here is that the first matrix has two rows, and the second matrix has two columns. To perform the multiplication, we first take the dot product of the first row of the first matrix (1,2,3) and the first column of the second matrix (9,7,5): $$ (1,2,3) \\cdot (9,7,5) = (1 \\times 9) + (2 \\times 7) + (3 \\times 5) = 38 $$ In our resulting matrix (which will always have the same number of rows as the first matrix, and the same number of columns as the second matrix), we can enter this into the first row and first column element: $$ \\begin{bmatrix} 38 & ?\\\\? & ? \\end{bmatrix} $$ Now we can take the dot product of the first row of the first matrix and the second column of the second matrix: $$ (1,2,3) \\cdot (8,6,4) = (1 \\times 8) + (2 \\times 6) + (3 \\times 4) = 32 $$ Let's add that to our resulting matrix in the first row and second column element: $$ \\begin{bmatrix} 38 & 32\\\\? & ? \\end{bmatrix} $$ Now we can repeat this process for the second row of the first matrix and the first column of the second matrix: $$ (4,5,6) \\cdot (9,7,5) = (4 \\times 9) + (5 \\times 7) + (6 \\times 5) = 101 $$ Which fills in the next element in the result: $$ \\begin{bmatrix} 38 & 32\\\\101 & ? \\end{bmatrix} $$ Finally, we get the dot product for the second row of the first matrix and the second column of the second matrix: $$ (4,5,6) \\cdot (8,6,4) = (4 \\times 8) + (5 \\times 6) + (6 \\times 4) = 86 $$ Giving us: $$ \\begin{bmatrix} 38 & 32\\\\101 & 86 \\end{bmatrix} $$ If this is not enough to catch the idea, take a look this explanation Matrix - Matrix Multiplication # In Numpy matrix multiplication can be done with no effort import numpy as np A = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) B = np . array ([[ 9 , 8 ], [ 7 , 6 ], [ 5 , 4 ]]) print ( 'A * B is:' , np . dot ( A , B ), sep = ' \\n ' ) print () print ( 'B * A is:' , np . dot ( B , A ), sep = ' \\n ' ) A * B is : [[ 38 32 ] [ 101 86 ]] B * A is : [[ 41 58 75 ] [ 31 44 57 ] [ 21 30 39 ]] Matrix Transpose import numpy as np A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) A . T print ( 'Matix A is:' , A , sep = ' \\n ' ) print () print ( 'Transpose of A is:' , A . T , sep = ' \\n ' ) Matix A is : [[ 1 2 3 ] [ 4 5 6 ] [ 7 8 9 ]] Transpose of A is : [[ 1 4 7 ] [ 2 5 8 ] [ 3 6 9 ]] Matrix - Identity Matrix Multiplication # We have two ways to define identity matrix in Numpy. # First is to define by hand, like above examples, and second is to use Numpy's buildin function import numpy as np I_1 = np . array ([[ 1. , 0. , 0. ],[ 0. , 1. , 0. ],[ 0. , 0. , 1. ]]) I_2 = np . identity ( 3 ) print ( I_1 ) print () print ( I_2 ) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]] [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]] # Matrix identity multiplication A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) I = np . identity ( 3 ) print ( 'A = ' , A , sep = ' \\n ' ) print ( 'A * I = ' , np . dot ( A , I ), sep = ' \\n ' ) A = [[1 2 3] [4 5 6] [7 8 9]] A * I = [[1. 2. 3.] [4. 5. 6.] [7. 8. 9.]] Conclusion In this post, I tried to cover the basics of linear algebra. I depicted some theory with examples solved by hand as well as with Numpy. I do hope, this blog will help you to grab the necessary knowledge in linear algebra basics and further gives you the direction where to dig deeper. I did not provide here further resources due to not to confuse the reader and give freedom to look for some other materials. If some part of the blog is someway unclear to you, please comment below. References Vector Vector Matrix Matrix if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/basics-of-linear-algebra-with-python","loc":"https://dsfabric.org/basics-of-linear-algebra-with-python"},{"title":"Introduction to Linear Algebra with Python","text":"These are the series of linear algebra mainly based on Numpy and Scipy . The series will follow the sequence: Introduction Basics of linear algebra Intermediate linear algebra: Part I and Part II Advances in linear algebra: Part I and Part II In these series I will attempt to demystify linear algebra concepts to beginners and combine it with Python for practical use. Linear algebra is one of the building block of data science among others. Its importance is huge, as all supervised, unsupervised and semi-supervised algorithms use it with some degree. One great example is Google's famous Page Rank algorithm, which heavily relies on it. And that is not all. We can find as many usage cases of linear algebra as many individuals exist in the data science field. The purpose of this blog series is to introduce you the ways how to use linear algebra in data science. More precisely, by this series, I intend to help aspiring data scientist to refresh their linear algebra knowledge with Python and gain some hands-on experience. Moreover, this may serve you as a starting point to dig deeper into an amazing world of linear algebra with Python. Let start by defining what is linear algebra Linear algebra is a branch of mathematics that is concerned with mathematical structures, closed under addition and scalar multiplication operations and that includes the theory of systems of linear equations, matrices, determinants, vector spaces, and linear transformations. Let's start explaining word by word the definition of linear algebra. Loosely speaking, mathematical structure is a set, together with a family of operations and relations defined on that set. Now divide \"Closed under addition and scalar multiplication\" into two parts. First is \"closed under addition\" which means that a set is \"closed under addition\" if the sum of any two members of this set belongs to this set again. For example, imagine the set of even integers. Then, take any to integer and add them up. The result is an even integer belonging to the initial set. Here is a mathematical definition. $$ A=\\{x \\in \\mathbb{Z}~\\vert~mod~2 =0\\} $$ Second is \"closed under scalar multiplication\". This means that the product of any member of the set and any scalar \\(\\alpha\\) such that \\(\\alpha~\\in \\mathbb{R}\\) is also in the set. The above-mentioned set is also closed under scalar multiplication. Generally, the sets \\(\\mathbb{N}, ~ \\mathbb{Z}, ~ \\mathbb{Q}\\) and \\(\\mathbb{R}\\) are closed under both addition and multiplication. $$ A = (0,1) $$ is closed under multiplication, but not addition. \\((0.6 + 0.7 = 1.3 > 1)\\) The set of all half integers $$ \\frac{\\mathbb{Z}}{2} = \\{x : \\exists~{y} \\in \\mathbb{Z}~(x = \\frac{y}{2})\\} $$ is closed under addition, but not under multiplication. \\((0.5 * 0.5 = 0.25~\\notin~\\frac{\\mathbb{Z}}{2})\\) The system of linear equations is a collection of two or more linear equations involving the same set of variables. The example is the following: $$ \\begin{cases} 3x + 2y - z = 1 \\\\ 2x - 2y + 4z = -2 \\\\ -x + \\frac{1}{2}y - z = 0 \\end{cases} $$ Matrix is just a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. There is no plain English definition of the determinant, but forthcoming blogs in this series will cover it in detail. Vector space is a set of objects (vectors) closed under finite vector addition and scalar multiplication. Linear transformation or linear map is a mapping between two vector spaces that preserves the addition and scalar multiplication rule. More mathematically, a linear transformation between two vector space \\(V\\) and \\(W\\) is a map \\(T : V~\\rightarrow~W\\) such that the following hold: \\(T(v_1 + v_2) = T(v_1) + T(v_2)\\) for any vectors \\(v_1\\) and \\(v_2\\) in \\(V\\) \\(T(\\alpha v_1) = \\alpha T(v_1)\\) for any scalar \\(\\alpha\\) Conclusion To sum up, this post is an introduction towards linear algebra series, where I will introduce you linear algebra concepts intuitively and programmatically in Python. The main idea of this series is to feel comfortable in the field and to give you the direction where to dig deeper. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/introduction-to-linear-algebra-with-python","loc":"https://dsfabric.org/introduction-to-linear-algebra-with-python"}]};