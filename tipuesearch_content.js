var tipuesearch = {"pages":[{"title":"Search · Data Science Fabric\n","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch · Data Science Fabric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n    ga('create', 'UA-136307659-1', 'auto');\n    ga('send', 'pageview');\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Fabric\n\n\nHome\nCategories\nTags\nArchives\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Fabric - Torture the data, and it will confess to anything. Ronald Coase\nPowered by Pelican. Theme: Elegant\n\n\n\n\n\n\n\n$(document).ready(function() {\n     $('#tipue_search_input').tipuesearch({\n             'mode' : 'json',\n         'show': 10,\n         'newWindow': false,\n     });\n});\n\n\n\n","tags":"","url":"https://dsfabric.org/search.html"},{"title":"Plotly Express","text":"I fairly believe that, if someone is good at data visualization he/she at least has heard about Hans Rosling. One of the most influential data storyteller, and honestly because of the talk he gave at TED, it influenced me too much that I decided to learn data visualization. Since then, I had some progress as well some stagnation but the most noteworthy thing here is that I got my hands dirty with great data visualization tools such as matplotlib , seaborn , Bokeh , altair , and plotly . All of these data visualization tools is great in its own niche and describing the pros and cons of each other is the matter of another separate blog. Having in mind that today's world is more interactive and most data viz wizards like \"one line\" solutions(almost true), Plotly made a huge improvement in this direction. Two days ago they introduced Plotly Express which is a new high-level Python visualization library. Long story short it is the wrapper for Plotly.py that makes building complex charts simple. Developers main aim for building plotly express: Our main goal with Plotly Express was to make it easier to use Plotly.py for exploration and rapid iteration. Let's go through some example and see how plotly express works. In [1]: import plotly_express as px import plotly.offline as pyo from IPython.display import IFrame In [2]: # Plotly Express has build in datasets. You can load this datasets by uncommenting below. # For expositional purposes, I use well-known gapminder data gapminder = px . data . gapminder () # iris = px.data.iris() # tips = px.data.tips() # election = px.data.election() # wind = px.data.wind() # carshare = px.data.carshare() In [3]: # Extract data for 2007 gapminder2007 = gapminder . query ( \"year == 2007\" ) In [ ]: # Simple scatter plot. # GDP Per Capita vs Life Expectancy px . scatter ( gapminder2007 , x = \"gdpPercap\" , y = \"lifeExp\" , labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' }) Pelican produces static html files. So, plotly interactive charts do not show up. Firstly, I converted plotly chart into standalone html file and then added by using IFrame tag. That's why it is here. In [4]: IFrame ( src = '../../images/first_plot.html' , width = 900 , height = 700 ) Out[4]: Each point represents country with its own continent. To see continents we can color each dot by continent name. In [ ]: px . scatter ( gapminder2007 , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' }) In [5]: IFrame ( src = '../../images/second_plot.html' , width = 900 , height = 700 ) Out[5]: And, what if we want to adjust each point size as it to be the country population size? That's should not be a problem. In [ ]: px . scatter ( gapminder2007 , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , size = 'pop' , size_max = 60 , labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' }) In [6]: IFrame ( src = '../../images/third_plot.html' , width = 900 , height = 700 ) Out[6]: If we put mouse to each data point the point description will appear. It shows continent name, x and y coordinates, and population size. As we already know each point is a country but it's not properly shown in the description. We can achieve this easily. In [ ]: fig = px . scatter ( gapminder2007 , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , size = 'pop' , size_max = 60 , hover_name = 'country' , labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' }) In [7]: IFrame ( src = '../../images/fourth_plot.html' , width = 900 , height = 700 ) Out[7]: I wrote initial command and at each step added two extra arguments. That's the advantage of Plotly Express. Now, what if want to see how this chart evolves over time. Simply, we need to see what happened before 2007 and make it more interactive. In [ ]: px . scatter ( gapminder , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , size = 'pop' , size_max = 60 , hover_name = 'country' , animation_frame = 'year' , animation_group = 'country' , log_x = True , range_x = [ 100 , 100000 ], range_y = [ 25 , 90 ], labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' , 'pop' : 'Population' }) In [8]: IFrame ( src = '../../images/fifth_plot.html' , width = 900 , height = 700 ) Out[8]: That's not all. Let plot same data on a map. In [ ]: px . choropleth ( gapminder , locations = 'iso_alpha' , color = 'lifeExp' , hover_name = 'country' , animation_frame = 'year' , color_continuous_scale = px . colors . sequential . Plasma , projection = 'natural earth' , labels = { 'lifeExp' : 'Life Expectancy' }) In [9]: IFrame ( src = '../../images/sixth_plot.html' , width = 900 , height = 700 ) Out[9]: That was good, but wait. What if you don't like jupyter?, trust me there are such guys That's not a problem. Combine Plotly-Express with Plotly and plot offline to make standalone html file. In [ ]: import plotly.offline as pyo import plotly_express as px In [ ]: fig = px . scatter ( gapminder , x = \"gdpPercap\" , y = \"lifeExp\" , color = 'continent' , size = 'pop' , size_max = 60 , hover_name = 'country' , animation_frame = 'year' , animation_group = 'country' , log_x = True , range_x = [ 100 , 100000 ], range_y = [ 25 , 90 ], labels = { 'gdpPercap' : 'GDP Per Capita' , 'lifeExp' : 'Life Expectancy' , 'pop' : 'Population' }) In [ ]: # Save figure object as html file pyo . plot ( fig , filename = 'gapminder.html' ) After glorifying plotly express, it is natural to find some drawbacks. Has plotly express some limitations? Particularly, how much data can we put into it? According to them, there is no hard limit for data set size but it is preferable to use 1000 points for some chart types and make some input parameter adjustments, while other chart types easily handle more points. I'm still trying to figure out the mechanics of plotly express and dig deeper, however, it won't be a bad idea to check more, about plotly express here . References Medium Blog Plotly Express Example Gallery if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Data Science","url":"https://dsfabric.org/articles/data-science/plotly-express.html","loc":"https://dsfabric.org/articles/data-science/plotly-express.html"},{"title":"Economics - Test","text":"Written with StackEdit . To do this project as I want I need to plan it carefully.","tags":"Economics","url":"https://dsfabric.org/articles/economics/economics-test.html","loc":"https://dsfabric.org/articles/economics/economics-test.html"},{"title":"Python - Test","text":"Written with StackEdit . To do this project as I want I need to plan it carefully.","tags":"Python","url":"https://dsfabric.org/articles/python/python-test.html","loc":"https://dsfabric.org/articles/python/python-test.html"},{"title":"Advances of Linear Algebra with Python","text":"This is the fourth and last post in blog series about linear algebra. Introduction Basics of linear algebra Intermediate linear algebra Advances in linear algebra First, second and third posts are here , here and here In this post I will introduce you to the advances of linear algebra, which in turn includes the following: Vector Basis Vectors Matrix Gaussian Elimination of a Matrix Gauss-Jordan Elimination of a Matrix The Inverse of a Matrix Using Gauss-Jordan Elimination Image of a Matrix Kernel of a Matrix Rank of a Matrix Find the Basis of a Matrix Transformations Eigenvalues Eigenvectors Spectrum and Spectral Radius Numerical Representation Matrix Decompositions Cholesky Decomposition QR Decomposition Eigendecomposition Singular Value Decomposition Inverse of a Square Full Rank Matrix Numerical Representation Conclusion References Vector Basis Vectors In the basics , we saw what is a unit vector. To refresh, the unit vector is the vector with length 1 and the formula is \\begin{align*} \\hat{X} = \\frac{X}{\\|X\\|} \\end{align*} For farther explanation, unit vectors can be used to represent the axes of a Cartesian coordinate system . For example in a three-dimensional Cartesian coordinate system such vectors are: \\begin{align*} \\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\quad \\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\quad \\hat{k} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{align*} which represents, $x$, $y$, and $z$ axes, respectively. For two dimensional space we have \\begin{align*} \\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\quad \\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\end{align*} Let deal with two-dimensional space to catch the idea of basis easily and then generalize this idea for higher dimensions. Imagine, we have vector space or collection of vectors $\\vec{V}$ over the Cartesian coordinate system. This space includes all two-dimensional vectors, or in other words, vectors with only two elements, $x$, and $y$. A basis , call it $B$, of vector space $V$ over the Cartesian coordinate system is a linearly independent subset of $V$ that spans whole vector space $V$. To be precise, basis $B$ to be the basis it must satisfie two conditions: Linearly independence property - states that all vectors in $B$ are linearly independent The spanning property - states that $B$ spans whole $V$ We can combine these two conditions in one sentence. $B$ is the basis if its all elements are linearly independent and every element of $V$ is a linear combination of elements of $B$. From these conditions, we can conclude that unit vectors $\\hat{i}$ and $\\hat{j}$ are the basis of $\\mathbb{R&#94;2}$. This kind of bases are also called standard basis or natural basis . The standard basis are denoted by $e_{1}$, $e_{2}$, $e_{3}$ and so on. I will be consistent and use the later notation for standard basis and $\\hat{i}$, $\\hat{j}$ and $\\hat{k}$ for unit vectors. These standard basis vectors are the basis in the sense that any other vector in $V$ can be expressed uniquely as a linear combination of these unit vectors. For example, every vector $v$ in two-dimensional space can be written as \\begin{align*} x\\ e_{1} + y\\ e_{2} \\end{align*} where $e_{1}$ and $e_{2}$ are unit vectors and $x$ and $y$ are scalar components or elements of the vector $v$. Now, to generalize the idea for higher dimensions we just have to apply the same logic as above, for $\\mathbb{R&#94;3}$ and more. In $\\mathbb{R&#94;3}$ we have standard basis vectors $e_{1}$, $e_{2}$, $e_{3}$, and generally for $\\mathbb{R&#94;n}$ we have standard basis vector space \\begin{align*} E = \\begin{bmatrix} e_{1} \\\\ e_{2} \\\\ \\cdots \\\\ e_{n} \\end{bmatrix} \\end{align*} To generalize the definition of basis further let consider the following: If elements $\\{v_{1}, v_{2},\\cdots,v_{n}\\}$ of $V$ generate $V$ and in addition they are linearly independent, then $\\{v_{1}, v_{2},\\cdots,v_{n}\\}$ is called a basis of $V$. We shall say that the elements $v_{1}, v_{2},\\cdots,v_{n}$ constitute or form a basis of V. Vector space $V$ can have several basis. At this stage, the notion of basis seems very abstract even for me, and believe me it was totally unclear for me until I solved some examples by hand. I'll show you how to compute basis after explaining row-echelon and reduced row-echelon forms and you'll understand it. However, it's not enough only to know how to row-reduce the given matrix. It's necessary to know which basis you want. Either column space or row space basis or the basis for nullspace. These notions are explained below and after that, we can find the basis for each of them. Matrix Gaussian Elimination of a Matrix In linear algebra, Gaussian Elimination is the method to solve the system of linear equations. This method is the sequence of operations performed on the coefficient matrix of the system. Except for solving the linear systems, the method can be used to find the rank of a matrix, the determinant as well as the inverse of a square invertible matrix. And what is the sequence of operations? Under this notion, elementary row operations are meant. We've covered it in the previous post but for the refresher, ERO's are: Interchange rows Multiplay each element in a row by a non-zero number Multiply a row by a non-zero number and add the result to another row Performing Gaussian elimination results in the matrix in Row Echelon Form (ref). The matrix is said to be in row echelon form if it satisfies the following conditions: The first non-zero element in each row, called the leading entry, is a 1 Each leading entry is in a column, which is the right side of the leading entry in the previous row Below the leading entry in a column, all other entries are zero To catch the idea of this process, let consider the example. Actually, we have no matrix (not necessarily true), we have the system of linear equations in the following form: \\begin{align*} \\begin{cases} x + 2y - z = 5\\\\ 3x + y - 2z = 9\\\\ -x + 4y + 2z = 0 \\end{cases} \\end{align*} Based on these equations we can form the following matrix \\begin{align*} \\begin{bmatrix} 1 & 2 & -1 \\\\ 3 & 1 & -2 \\\\ -1 & 4 & 2 \\end{bmatrix} \\end{align*} This matrix is called coefficient matrix as it contains the coefficients of the linear equations. Having the coefficient matrix, we can rewrite our system in the following form: \\begin{align*} Ax = b \\end{align*} Where $A$ is the coefficient matrix, $x$ is the vector of the unknowns, and $b$ is the vector of the right-hand side components To solve this simultaneous system, the coefficients matrix is not enough. We need something more, on which we can perform ELO's. This matrix is: \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 3 & 1 & -2 & 9 \\\\ -1 & 4 & 2 & 0 \\end{array} \\end{bmatrix} = [A | b] \\end{align*} which is called augmented matrix , which in turn gives us the possibility to perform ELO's, in other words, we do Gaussian elimination and the resulted matrix will be in row echelon form. Using back substitution on the resulted matrix gives the solution to our system of equations. Let do it by hand. We have the initial system \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 3 & 1 & -2 & 9 \\\\ -1 & 4 & 2 & 0 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ 3x + y - 2z = 9 \\\\ -x + 4y + 2z = 0 \\end{cases} \\end{align*} Then, using ERO's $R3 \\rightarrow R3 + R1$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 3 & 1 & -2 & 9 \\\\ 0 & 6 & 1 & 5 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ 3x + y - 2z = 9 \\\\ 6y + z = 5 \\end{cases} \\end{align*} $R2 \\rightarrow R2 - 3R1$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 0 & -5 & 1 & -6 \\\\ 0 & 6 & 1 & 5 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ -5y + z = -6 \\\\ 6y + z = 5 \\end{cases} \\end{align*} $R2 \\rightarrow R2 + R3$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 0 & 1 & 2 & -1 \\\\ 0 & 6 & 1 & 5 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ y + 2z = -1 \\\\ 6y + z = 5 \\end{cases} \\end{align*} $R3 \\rightarrow R3 - 6R2$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 0 & 1 & 2 & -1 \\\\ 0 & 0 & -11 & 11 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ y + 2z = -1 \\\\ -11z = 11 \\end{cases} \\end{align*} $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 0 & 1 & 2 & -1 \\\\ 0 & 0 & 1 & -1 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\quad (A)\\\\ y + 2z = -1 \\quad (B)\\\\ z = -1 \\quad (C) \\end{cases} \\end{align*} Backsubstitution \\begin{align*} \\begin{cases} (C) \\quad z = -1 \\\\ (B) \\quad y = -1 - 2z \\quad \\Rightarrow \\quad y = -1 - 2(-1) = 1 \\\\ (A) \\quad x = 5 - 2y + z \\quad \\Rightarrow \\quad x = 5 - 2(1) + (-1) = 2 \\end{cases} \\end{align*} Solution \\begin{align*} x = 2 \\\\ y = 1 \\\\ x = -1 \\end{align*} This is the solution of the initial system, as well as the last system and every intermediate system. The matrix obtained in step 5 above is in Row-Echelon form as it satisfied above-mentioned conditions. Note that, starting with a particular matrix, a different sequence of ERO's can lead to different row echelon form Gauss-Jordan Elimination of a Matrix Gaussian elimination performs row operations to produce zeros below the main diagonal of the coefficient matrix to reduce it to row echelon form. Once it's done we perform back substitution to find the solution. However, we can continue performing ERO's to reduce coefficient matrix farther, to produce Reduced Row Echelon Form (rref). The matrix is in reduced row echelon form if it satisfies the following conditions: It is in row echelon form The leading entry in each row is the only non-zero entry in its column Gauss-Jordan elimination starts when Gauss elimination left off. Loosely speaking, Gaussian elimination works from the top down and when it stops, we start Gauss-Jordan elimination from the bottom up. The Reduced Row Echelon Form matrix is the result of Gauss-Jordan Elimination process. We can continue our example from step 5 and see what is Gauss-Jordan elimination. At step 5 we had $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 0 & 1 & 2 & -1 \\\\ 0 & 0 & 1 & -1 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ y + 2z = -1 \\\\ z = -1 \\end{cases} \\end{align*} Now, from bottom to up we perform the following ERO's $R2 \\rightarrow R2 - 2R3$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & -1 & 5 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y - z = 5 \\\\ y = 1 \\\\ z = -1 \\end{cases} \\end{align*} $R1 \\rightarrow R1 + R3$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 2 & 0 & 4 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x + 2y = 4 \\\\ y = 1 \\\\ z = -1 \\end{cases} \\end{align*} $R1 \\rightarrow R1 - 2R2$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|c@{}} 1 & 0 & 0 & 2 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 \\end{array} \\end{bmatrix} \\equiv \\begin{cases} x = 2 \\\\ y = 1 \\\\ z = -1 \\end{cases} \\end{align*} The solution is \\begin{align*} x = 2 \\\\ y = 1 \\\\ x = -1 \\end{align*} and this is the same as the solution of the Gauss elimination. The matrix in step 8 is the Reduced Row Echelon Form of our initial coefficient matrix $A$. The Inverse of a Matrix Using Gauss-Jordan Elimination Suppose, we have given the matrix and want to find its inverse but do not want to use the technique mentioned in the intermediate post. We can use Gauss-Jordan elimination with little modification to find the inverse of a matrix. To be consistent, I use the above coefficient matrix but not the right-hand side of the system. So, our matrix is \\begin{align*} A = \\begin{bmatrix} 1 & 2 & -1 \\\\ 3 & 1 & -2 \\\\ -1 & 4 & 2 \\end{bmatrix} \\end{align*} To find the inverse of $A$, we need to augment $A$ by the identity matrix $I$ which has the same dimensions as $A$. It is a must the identity to have the same dimensions. After augmentation we have \\begin{align*} [A | I] = \\begin{bmatrix} \\begin{array}{@{}ccc|ccc@{}} 1 & 2 & -1 & 1 & 0 & 0 \\\\ 3 & 1 & -2 & 0 & 1 & 0 \\\\ -1 & 4 & 2 & 0 & 0 & 1 \\end{array} \\end{bmatrix} \\end{align*} We have to perform elementary row operations in the same way as we did in the above example. Particularly, $R3 \\rightarrow R3 + R1$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}} 1 & 2 & -1 & 1 & 0 & 0\\\\ 3 & 1 & -2 & 0 & 1 & 0\\\\ 0 & 6 & 1 & 1 & 0 & 1 \\end{array} \\end{bmatrix} \\end{align*} $R2 \\rightarrow R2 - 3R1$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}} 1 & 2 & -1 & 1 & 0 & 0\\\\ 0 & -5 & 1 & -3 & 1 & -3\\\\ 0 & 6 & 1 & 1 & 0 & 1 \\end{array} \\end{bmatrix} \\end{align*} $R2 \\rightarrow R2 + R3$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}} 1 & 2 & -1 & 1 & 0 & 0\\\\ 0 & 1 & 2 & -2 & 1 & -2\\\\ 0 & 6 & 1 & 1 & 0 & 1 \\end{array} \\end{bmatrix} \\end{align*} $R3 \\rightarrow R3 - 6R2$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}} 1 & 2 & -1 & 1 & 0 & 0\\\\ 0 & 1 & 2 & -2 & 1 & -2\\\\ 0 & 0 & -11 & 13 & -6 & 13 \\end{array} \\end{bmatrix} \\end{align*} $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}} 1 & 2 & -1 & 1 & 0 & 0\\\\ 0 & 1 & 2 & -2 & 1 & -2\\\\ 0 & 0 & 1 & -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{array} \\end{bmatrix} \\end{align*} $R2 \\rightarrow R2 - 2R3$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}} 1 & 2 & -1 & 1 & 0 & 0\\\\ 0 & 1 & 0 & \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\ 0 & 0 & 1 & -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{array} \\end{bmatrix} \\end{align*} $R1 \\rightarrow R1 + R3$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}} 1 & 2 & 0 & -\\frac{2}{11} & \\frac{6}{11} & -\\frac{13}{11}\\\\ 0 & 1 & 0 & \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\ 0 & 0 & 1 & -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{array} \\end{bmatrix} \\end{align*} $R1 \\rightarrow R1 - 2R2$ \\begin{align*} \\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}} 1 & 0 & 0 & -\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\ 0 & 1 & 0 & \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\ 0 & 0 & 1 & -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{array} \\end{bmatrix} \\end{align*} Our inverse of $A$ is \\begin{align*}A&#94;{-1} = \\begin{bmatrix} -\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\ \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\ -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11} \\end{bmatrix} \\end{align*} Image of a Matrix Let $A$ be $m\\times n$ matrix. Space spanned by its column vectors are called range, image, or column space of a matrix $A$. The row space is defined similarly. I only consider column space as all the logic is the same for row space. The precise definition is the following: Let $A$ be an $m\\times n$ matrix, with column vectors $v_{1}, v_{2}, \\cdots, v_{n}$. A linear combination of these vectors is any vector of the following form: $c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n}$, where $c_{1}, c_{2}, \\cdots , c_{n}$ are scalars. The set of all possible linear combinations of $v_{1}, v_{2}, \\cdots , v_{n}$ is called the column space of $A$. For example: \\begin{align*}A = \\begin{bmatrix} 1 & 0\\\\ 0 & 1\\\\ 2 & 0 \\end{bmatrix} \\end{align*} Column vectors are: \\begin{align*}v_{1} = \\begin{bmatrix} 1\\\\ 0\\\\ 2 \\end{bmatrix} \\quad v_{2} = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\end{align*} A linear combination of $v_{1}$ and $v_{2}$ is any vector of the form \\begin{align*}c_{1} \\begin{bmatrix} 1\\\\ 0\\\\ 2 \\end{bmatrix} + c_{2} \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} = \\begin{bmatrix} c_{1}\\\\ c_{2}\\\\ 2c_{1} \\end{bmatrix} \\end{align*} The set of all such vectors is the column space of $A$. Kernel of a Matrix In linear algebra, the kernel or a.k.a null space is the solution of the following homogeneous system: $A\\cdot X = 0$ where $A$ is a $m\\times n$ matrix and $X$ is a $m\\times 1$ vector and is denoted by $Ker(A)$. For more clarity, let consider the numerical example. Lat our matrix $A$ be the following: \\begin{align*}A = \\begin{bmatrix} 2 & 7 & 1 & 3 \\\\ -4 & -2 & 2 & -2 \\\\ -1 & 7 & 3 & 2 \\\\ -2 & 2 & 2 & 0 \\\\ \\end{bmatrix} \\end{align*} and our $X$ is \\begin{align*}X = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\ \\end{bmatrix} \\end{align*} We have to form the following system: \\begin{align*}A\\cdot X = \\begin{bmatrix} 2 & 7 & 1 & 3 \\\\ -4 & -2 & 2 & -2 \\\\ -1 & 7 & 3 & 2 \\\\ -2 & 2 & 2 & 0 \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix} \\end{align*} After that, we have to put this system into row-echelon or reduced row-echelon form. Let skip detailed calculation and present only results, which is the last matrix. \\begin{align*}A = \\begin{bmatrix} 2 & 7 & 1 & 3\\\\ -4 & -2 & 2 & -2\\\\ -1 & 7 & 3 & 2\\\\ -2 & 2 & 2 & 0 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ -4 & -2 & 2 & -2\\\\ 2 & 7 & 1 & 3\\\\ -2 & 2 & 2 & 0 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & -30 & -10 & -10\\\\ 0 & 21 & 7 & 7\\\\ 0 & -12 & -4 & -4 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & 3 & 1 & 1\\\\ 0 & 3 & 1 & 1\\\\ 0 & 3 & 1 & 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & 3 & 1 & 1\\\\ 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\end{align*} Now, to find the kernel of the original matrix $A$, we have to solve the following system of equations: \\begin{align*} \\begin{cases} x_{1} - 7x_{2} - 3x_{3} - 2x_{4} = 0 \\\\ 3x_{2} + x_{3} + x_{4} = 0 \\end{cases} \\rightarrow \\begin{cases} x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4}\\\\ x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\end{cases} \\end{align*} From this solution we conclude that the kernel of $A$ is \\begin{align*}Ker(A) = \\begin{bmatrix} x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\ x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\ x_{3} \\\\ x_{4} \\end{bmatrix} \\end{align*} Where, $x_{3}$ and $x_{4}$ are free variables and can be any number in $R$ Note, that both original matrix $A$ and its row-echelon for has the same kernel. This means that row reduction preserves the kernel or null space. Rank of a Matrix In the intermediate tutorial, you saw how to calculate the determinant of a matrix and also saw that any non zero determinant of sub-matrix of the original matrix shows its nondegenerateness. In other words, nonzero determinant gives us information about the rank of the matrix. Also, I said that there was not only one way to find the rank of a matrix. After reviewing Gauss and Gauss-Jordan Elimination and Row-Echelon and Reduced Row-Echelon forms you know that indeed there are other ways to find the determinant of a matrix as well as the rank of the matrix. To keep DRY , here I only consider a numerical example. The code is provided in the intermediate tutorial. Suppose we have matrix $A$ in the following form: \\begin{align*}A = \\begin{bmatrix} 3 & 2 & -1\\\\ 2 & -3 & -5\\\\ -1 & -4 &- 3 \\end{bmatrix} \\end{align*} Perform Elementary Row Operations we get reduced-echelon form: \\begin{align*}A = \\begin{bmatrix} 3 & 2 & -1\\\\ 2 & -3 & -5\\\\ -1 & -4 & -3 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 4 & 3\\\\ 3 & 2 & -1\\\\ 2 & -3 & -5 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 4 & 3\\\\ 0 & -10 & -10\\\\ 0 & -11 & -11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 4 & 3\\\\ 0 & 1 & 1\\\\ 0 & -11 & -11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 4 & 3\\\\ 0 & 1 & 1\\\\ 0 & 0 & 0 \\end{bmatrix} \\end{align*} From the last matrix we see that the nonzero determinant only exists in $2\\times2$ sub-matrices, hence rank of the matrix $A$ is 2. Find the Basis of a Matrix Now we are able to find the basis for column space and row space as well as the basis for the kernel. The columns of a matrix $A$ span the column space but they may not form a basis if the column vectors are linearly dependent. If this is the case, only some subset of these vectors forms the basis. To find the basis for column space we reduce matrix $A$ to reduced row-echelon form. For example: \\begin{align*}A = \\begin{bmatrix} 2 & 7 & 1 & 3 \\\\ -4 & -2 & 2 & -2 \\\\ -1 & 7 & 3 & 2 \\\\ -2 & 2 & 2 & 0 \\\\ \\end{bmatrix} \\end{align*} Row reduced form of $A$ is: \\begin{align*}B = \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & 3 & 1 & 1\\\\ 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\end{align*} We see that only column 1 and column 2 are linearly independent in reduced form and hence column 1 and column 2 of the original matrix $A$ form the basis, which is: \\begin{align*} \\begin{bmatrix} 2 \\\\-4\\\\-1\\\\-2 \\end{bmatrix} \\quad \\text{and} \\quad \\begin{bmatrix} 7\\\\-2 \\\\ 7 \\\\ 2 \\end{bmatrix} \\end{align*} To find the basis for row space, let consider different matrix and again let it be $A$. \\begin{align*}A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 2 & 7 & 4 \\\\ 1 & 5 & 2 \\end{bmatrix} \\end{align*} To reduce $A$ to reduced row-echelon form we have: \\begin{align*}B = \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\end{align*} As in column space case, we see that linearly independent, nonzero row vectors are \\begin{align*} \\begin{bmatrix} 1 \\\\0\\\\2 \\end{bmatrix} \\quad \\text{and} \\quad \\begin{bmatrix} 0\\\\1 \\\\ 0 \\end{bmatrix} \\end{align*} To find the basis for kernel let consider our old example. In this case our matrix is: \\begin{align*}A = \\begin{bmatrix} 2 & 7 & 1 & 3 \\\\ -4 & -2 & 2 & -2 \\\\ -1 & 7 & 3 & 2 \\\\ -2 & 2 & 2 & 0 \\\\ \\end{bmatrix} \\end{align*} And its row reduced form is \\begin{align*}B = \\begin{bmatrix} -1 & 7 & 3 & 2\\\\ 0 & 3 & 1 & 1\\\\ 0 & 0 & 0 & 0\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\end{align*} We solved this and got the following result: \\begin{align*}Ker(A) = \\begin{bmatrix} x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\ x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\ x_{3} \\\\ x_{4} \\end{bmatrix} \\end{align*} Now to have basis for null space just plug values for $x_{3} = 1$ and $x_{4} = 0$, resulted vector is \\begin{align*} \\begin{bmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\\\ 1 \\\\ 0 \\end{bmatrix} \\end{align*} The resulted vector is one set of the basis for kernel space. The values for $x_{3}$ and $x_{4}$ are up to you as they are free variables. Transformations Matrices and vectors are used together to manipulate spatial dimensions. This has a lot of applications, including the mathematical generation of 3D computer graphics, geometric modeling, and the training and optimization of machine learning algorithms. Here, I present some types of transformation. However, the list will not be exhaustive. Firstly, define linear transformation: Linear transformation or linear map, is a mapping (function) between two vector spaces that preserves addition and scalar multiplication operations Linear Transformation You can manipulate a vector by multiplying it with a matrix. The matrix acts like a function that operates on an input vector to produce a vector output. Specifically, matrix multiplications of vectors are linear transformations that transform the input vector into the output vector. For example, consider a matrix $A$ and vector $v$ \\begin{align*}A = \\begin{bmatrix} 2 & 3 \\\\ 5 & 2 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\end{align*} Define transformation $T$ to be: \\begin{align*} T(\\vec{v}) = A \\vec{v} \\end{align*} This transformation is simply dot or inner product and give the following result: \\begin{align*} T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 8 \\\\ 9 \\end{bmatrix} \\end{align*} In this case, both the input and output vector has 2 components. In other words, the transformation takes a 2-dimensional vector and produces a new 2-dimensional vector. Formally we can write this in the following way: \\begin{align*} T: \\rm I\\!R&#94;{2} \\to \\rm I\\!R&#94;{2} \\end{align*} The transformation does not necessarily have to be $n \\times n$. The dimension of the output vector and the input vector may differ. Rewrite our matrix $A$ and vector $v$. \\begin{align*}A = \\begin{bmatrix} 2 & 3 \\\\ 5 & 2 \\\\ 1 & 1 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\end{align*} Apply above transformation gives, \\begin{align*} T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 8 \\\\ 9 \\\\ 3 \\end{bmatrix} \\end{align*} Now, our transformation transforms a vector from 2-dimensional space into 3-dimensional space. We can rite this transformation as \\begin{align*} T: \\rm I\\!R&#94;{2} \\to \\rm I\\!R&#94;{3} \\end{align*} Transformations of Magnitude and Aplitude When we multiply a vector by a matrix we transform it in at least one of the following two ways Scale the length (Magnitude) Change the direction (Aplitude) Change in length (Magnitude), but not change in direction (Amplitude) \\begin{align*}A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{align*} transformation gives, \\begin{align*} T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} \\end{align*} In this case, the resulted vector changed in length but not changed in direction. See code and visualization in Numerical Representation part. Change in direction (Amplitude), but not change in length (Magnitude) \\begin{align*}A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{align*} transformation gives, \\begin{align*} T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\end{align*} This time, resulted vector changed in direction but has the same length. Change in direction (Amplitude) and in length (Magnitude) \\begin{align*}A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\quad \\vec{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{align*} transformation gives, \\begin{align*} T(\\vec{v}) = A \\vec{v} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\end{align*} This time the resulted vector changed in the direction as well as the length. Affine Transformation An Affine transformation multiplies a vector by a matrix and adds an offset vector, sometimes referred to as bias . \\begin{align*} T(\\vec{v}) = A\\vec{v} + \\vec{b} \\end{align*} Consider following example \\begin{align*}T(\\vec{v}) = A\\vec{v} + \\vec{b} = \\begin{bmatrix} 5 & 2\\\\ 3 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} + \\begin{bmatrix} -2\\\\ -6 \\end{bmatrix} = \\begin{bmatrix} 5\\\\ -2 \\end{bmatrix} \\end{align*} This kind of transformation is actually the basic block of linear regression, which is a core foundation for machine learning. However, these concepts are out of the scope of this tutorial. Python code is below for this transformation. Eigenvalues Let consider matrix $A$ \\begin{align*}A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} \\end{align*} Now, let multiply this matrix with vector \\begin{align*} \\vec{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{align*} We have the following: \\begin{align*}A \\cdot v = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\\\ 0 \\end{bmatrix} = 2 \\cdot v \\end{align*} That's the beautiful relationship yes? To prove this is not the only one vector, which can do this try this vector $\\vec{v} = [0\\quad 1\\quad 2]$ instead of old $v$. You should get $11\\cdot \\vec{v}$ This beautiful relationship comes from the notion of eigenvalues and eigenvectors. In this case $2$ and $11$ are eigenvalues of the matrix $A$. Let formalize the notion of eigenvalue and eigenvector: Let $A$ be an $n\\times n$ square matrix. If $\\lambda$ is a scalar and $v$ is non-zero vector in $\\mathbb{R&#94;n}$ such that $$Av = \\lambda v$$ then we say that $\\lambda$ is an eigenvalue and $v$ is eigenvector I believe you are interested in how to find eigenvalues. Consider again our matrix $A$ and follow steps to find eigenvalues. Given that our matrix $A$ is a square matrix, the condition that characterizes an eigenvalue $\\lambda$ is the existence of a nonzero vector $v$ such that $Av = \\lambda v$. We can rewrite this equation in the following way: \\begin{align*} Av = \\lambda v \\\\ Av - \\lambda v = 0 \\\\ Av - \\lambda I v = 0 \\\\ (A - \\lambda I)v = 0 \\end{align*} The final form of this equation makes it clear that $v$ is the solution of a square, homogeneous system. To have the nonzero solution(we required it in above definition), then the determinant of the coefficient matrix - $(A - \\lambda I)$ must be zero. This is achieved when the columns of the coefficient matrix are linearly dependent. In other words, to find eigenvalues we have to choose $\\lambda$ such that to solve the following equation: \\begin{align*} det(A - \\lambda I) = 0 \\end{align*} This equation is called characteristic equation For more clarity, let solve it with a particular example. We have square matrix $A$ and follow the above equation gives us: \\begin{align*}det(A - \\lambda I) = det\\Bigg( \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} - \\lambda \\cdot \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\Bigg) = det\\Bigg( \\begin{bmatrix} 2 - \\lambda & 0 & 0 \\\\ 0 & 3 - \\lambda & 4 \\\\ 0 & 4 & 9 - \\lambda \\end{bmatrix} \\Bigg) \\Rightarrow \\\\ \\\\ \\Rightarrow (2 - \\lambda)[(3 - \\lambda)(9 - \\lambda) - 16] = -\\lambda&#94;3 + 14\\lambda&#94;2 - 35\\lambda + 22 \\end{align*} The equation $-\\lambda&#94;3 + 14\\lambda&#94;2 - 35\\lambda + 22$ is called characteristic polynomial of the matrix $A$ and will be of degree $n$ if $A$ is $n\\times n$ The zeros or roots of this characteristic polynomial are the eigenvalues of the original matrix $A$. In this case the roots are $2$, $1$, and $11$. Surprise! Our matrix $A$ have three eigenvalues and two of them are already known for us from above example. Eigenvalues of a square matrix $A$ have some nice features: The determinant of $A$ eqauls to the product of the eigenvalues The trace of $A$ (The sum of the elements on the principal diagonal) equal the sum of the eigenvalues If $A$ is symmetric matrix, then all of its eigenvalues are real If $A$ is invertible (The determiannt of $A$ is not zero) and $\\lambda_{1}, \\cdots, \\lambda_{n}$ are its eigenvalues, then the eigenvalues of $A&#94;{-1}$ are $1 / \\lambda_{1}, \\cdots, 1 / \\lambda_{n}$ From first feature we have that the matrix is invertible if and only if all its eigenvalues are nonzero. Eigenvectors It's time to calculate eigenvectors, but let firstly define what is eigenvector and how it relates to the eigenvalue. Any nonzero vector $v$ which satisfies characteristic equation is said to be an eigenvector of $A$ corresponding to $\\lambda$ Continue above example and see what are eigenvectors corresponding to eigenvalues $\\lambda = 2$, $\\lambda = 1$, and $\\lambda = 11$, repectively. Eigenvector for $\\lambda = 1$ \\begin{align*}(A - 1I)\\cdot \\begin{bmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\end{bmatrix} =\\Bigg( \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\Bigg)\\cdot \\begin{bmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 4 \\\\ 0 & 4 & 8 \\end{bmatrix}\\cdot \\begin{bmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\end{bmatrix}= \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{align*} Rewrite this as a system of equations, we'll get \\begin{align*} \\begin{cases} v_{1} = 0\\\\ 2v_{2} + 4v_{3} = 0\\\\ 4v_{2} + 8v{3} = 0 \\end{cases}\\rightarrow \\begin{cases} v_{1} = 0 \\\\ v_{2} = -2v_{3}\\\\ v_{3} = 1 \\end{cases} \\rightarrow \\begin{cases} v_{1} = 0 \\\\ v_{2} = -2\\\\ v_{3} = 1 \\end{cases} \\end{align*} So, our eigenvector correcponding to eigenvalue $\\lambda = 1$ is \\begin{align*} v_{\\lambda = 1} = \\begin{bmatrix} 0 \\\\ -2 \\\\ 1 \\end{bmatrix} \\end{align*} Finding eigenvectors for $\\lambda = 2$ and $\\lambda = 11$ is up to you. Spectrum and Spectral Radius The Spectral Radius of a sqaure matrix $A$ is the largest absolute values of its eigenvalues and is denoted by $\\rho(A)$. More formally, Spectral radius of a $n \\times n$ matrix $A$ is: \\begin{equation} \\rho(A) = max \\Big\\{ \\mid \\lambda \\mid \\ : \\lambda \\ is \\ an \\ eigenvalue \\ of \\ A \\Big\\} \\end{equation} Stated otherwise, we have \\begin{equation} \\rho(A) = max \\Big\\{ \\mid \\lambda_{1} \\mid, \\cdots, \\mid \\lambda_{n} \\mid \\Big\\} \\end{equation} It's noteworthy that the set of all eigenvalues \\begin{align*} \\Big\\{ \\lambda : \\lambda \\in \\lambda(A) \\Big\\} \\end{align*} is called the Spectrum From above example we had three eivenvalues, $\\lambda = 2$, $\\lambda = 1$ and $\\lambda = 11$ which are spectrum of $A$ and spectral radius for our matrix $A$ is $\\lambda = 11$ Numerical Representation Kernel or Null Space of a Matrix In [1]: import numpy as np from scipy.linalg import null_space A = np . array ([[ 2 , 7 , 1 , 3 ], [ - 4 , - 2 , 2 , - 2 ], [ - 1 , 7 , 3 , 2 ],[ - 2 , 2 , 2 , 0 ]]) kernel_A = null_space ( A ) print ( \"Normilized Kernel\" , kernel_A , sep = ' \\n ' ) # This matrix is normilized, meaning that it has unit length # To find unnormilized kernel we have to do the following: # Import sympy from sympy import Matrix B = [[ 2 , 7 , 1 , 3 ], [ - 4 , - 2 , 2 , - 2 ], [ - 1 , 7 , 3 , 2 ],[ - 2 , 2 , 2 , 0 ]] B = Matrix ( B ) kernel_B = B . nullspace () print () print ( \"Unnormiled Kernel\" , kernel_B , sep = ' \\n ' ) # In unnormilized case, we clearly see that sympy automaticaly choose values for our free variables. # In first case x_3 = 1; x_4 = 0 and in the second case x_3 = 0; x_4 = 1 # Resulted vector(s) are basis for the null space for our matrix A Normilized Kernel [[-0.06082406 0.59096667] [ 0.41624035 -0.05669711] [-0.47706441 0.64766378] [-0.77165663 -0.47757246]] Unnormiled Kernel [Matrix([ [ 2/3], [-1/3], [ 1], [ 0]]), Matrix([ [-1/3], [-1/3], [ 0], [ 1]])] Linear Transformations In [2]: import numpy as np import matplotlib.pyplot as plt % matplotlib inline v = np . array ([ 1 , 0 ]) A = np . array ([[ 2 , 0 ], [ 0 , 2 ]]) t = A @v # dot product print ( \"Resulted vector is: t =\" , t ) # Plot v and t vecs = np . array ([ t , v ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . grid () plt . quiver ( * origin , vecs [:, 0 ], vecs [:, 1 ], color = [ 'blue' , 'green' ], scale = 10 ) plt . show () # Original vector v is green and transformed vector t is blue. # Vector t has same direction as v but greater magnitude Resulted vector is: t = [2 0] In [3]: import numpy as np import matplotlib.pyplot as plt % matplotlib inline v = np . array ([ 1 , 0 ]) A = np . array ([[ 0 , - 1 ], [ 1 , 0 ]]) t = A @v print ( \"Resulted vector is: t =\" , t ) # Plot v and t vecs = np . array ([ v , t ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . grid () plt . quiver ( * origin , vecs [:, 0 ], vecs [:, 1 ], color = [ 'green' , 'blue' ], scale = 10 ) plt . show () # Resulted vector change the direction but has the same length Resulted vector is: t = [0 1] In [4]: import numpy as np import matplotlib.pyplot as plt % matplotlib inline v = np . array ([ 1 , 0 ]) A = np . array ([[ 2 , 1 ], [ 1 , 2 ]]) t = A @v print ( \"Resulted vector is: t =\" , t ) # Plot v and t vecs = np . array ([ v , t ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . grid () plt . quiver ( * origin , vecs [:, 0 ], vecs [:, 1 ], color = [ 'green' , 'blue' ], scale = 10 ) plt . show () # Resulted vector changed the direction, as well as the length Resulted vector is: t = [2 1] In [5]: import numpy as np import matplotlib.pyplot as plt % matplotlib inline v = np . array ([ 1 , 1 ]) A = np . array ([[ 5 , 2 ], [ 3 , 1 ]]) b = np . array ([ - 2 , - 6 ]) t = A @v + b print ( \"Resulted vector is: t =\" , t ) # Plot v and t vecs = np . array ([ v , t ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . grid () plt . quiver ( * origin , vecs [:, 0 ], vecs [:, 1 ], color = [ 'green' , 'blue' ], scale = 15 ) plt . show () # The resulted vector t is blue Resulted vector is: t = [ 5 -2] Eigenvalues and Eigenvectors In [6]: import numpy as np A = np . array ([[ 2 , 0 , 0 ], [ 0 , 3 , 4 ], [ 0 , 4 , 9 ]]) eigenvalues , eigenvectors = np . linalg . eig ( A ) print ( \"Eigenvalues are: \" , eigenvalues ) print () print ( \"Eigenvectors are: \" , eigenvectors , sep = ' \\n ' ) # Note that this eigenvectors seems different from my calculation. However they are not different. # They are normiled to have unit length Eigenvalues are: [11. 1. 2.] Eigenvectors are: [[ 0. 0. 1. ] [ 0.4472136 0.89442719 0. ] [ 0.89442719 -0.4472136 0. ]] Matrix Decompositions In linear algebra, matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. Factorizing a matrix means that we want to find a product of matrices that is equal to the initial matrix. These techniques have a wide variety of uses and consequently, there exist several types of decompositions. Below, I will consider some of them, mostly applicable to machine learning or deep learning. Cholesky Decomposition The Cholesky Decomposition is the factorization of a given symmetric square matrix $A$ into the product of a lower triangular matrix, denoted by $L$ and its transpose $L&#94;{T}$. This decomposition is named after French artillery officer Andre-Louis Cholesky . The formula is: \\begin{align*}A = LL&#94;{T} \\end{align*} For rough sense, let $A$ be \\begin{align*}A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} \\end{align*} Then we can represent $A$ as \\begin{align*}A = LL&#94;{T} = \\begin{bmatrix} l_{11} & 0 & 0 \\\\ l_{21} & l_{22} & 0 \\\\ l_{31} & l_{32} & l_{33} \\end{bmatrix} \\cdot \\begin{bmatrix} l_{11} & l_{12} & l_{13} \\\\ 0 & l_{22} & l_{23} \\\\ 0 & 0 & a_{33} \\end{bmatrix} = \\begin{bmatrix} l_{11}&#94;{2} & l_{21}l_{11} & l_{31}l_{11} \\\\ l_{21}l_{11} & l_{21}&#94;{2} + l_{22}&#94;{2} & l_{31}l_{21} + l_{32}l_{22} \\\\ l_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}&#94;{2} + l_{32}&#94;{2} + l_{33}&#94;2 \\end{bmatrix} \\end{align*} The diagonal elements of matrix $L$ can be calculated by the following formulas: \\begin{align*} l_{11} = \\sqrt{a_{11}} \\quad \\quad l_{22} = \\sqrt{a_{22} - l_{21}&#94;{2}} \\quad \\quad l_{33} = \\sqrt{a_{33} - (l_{31}&#94;{2} + l_{32}{2})} \\end{align*} And in general, for diagonal elements of the matrix $L$ we have: \\begin{align*}l_{kk} = \\sqrt{a_{kk} - \\sum_{j = 1}&#94;{k - 1}l_{kj}&#94;{2}} \\end{align*} For the elements below the main diagonal, $l_{ik}$ where $i > k$, the formulas are \\begin{align*} l_{21} = \\frac{1}{l_{11}}a_{21} \\quad \\quad l_{31} = \\frac{1}{l_{11}}a_{31} \\quad \\quad l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) \\end{align*} And the general formula is \\begin{align*}l_{ik} = \\frac{1}{l_{kk}}\\Big(a_{ik} - \\sum_{j = 1}&#94;{k - 1}l_{ij}l_{kj}\\Big) \\end{align*} Messy formulas! Consider a numerical example to see what happen under the hood. We have a matrix $A$ \\begin{align*}A = \\begin{bmatrix} 25 & 15 & -5 \\\\ 15 & 18 & 0 \\\\ -5 & 0 & 11 \\end{bmatrix} \\end{align*} According to the above formulas, let find a lower triangular matrix $L$. We have \\begin{align*} l_{11} = \\sqrt{a_{11}} = \\sqrt{25} = 5 \\quad \\quad l_{22} = \\sqrt{a_{22} - l_{21}&#94;{2}} = \\sqrt{18 - 3&#94;{2}} = 3 \\quad \\quad l_{33} = \\sqrt{a_{33} - (l_{31}&#94;{2} + l_{32}&#94;{2})} = \\sqrt{11 - ((-1)&#94;{2} + 1&#94;{2})} = 3 \\end{align*} Seems, we have missing non-diagonal elements, which are \\begin{align*} l_{21} = \\frac{1}{l_{11}}a_{21} = \\frac{1}{5}15 = 3 \\quad \\quad l_{31} = \\frac{1}{l_{11}}a_{31} = \\frac{1}{5}(-5) = -1 \\quad \\quad l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{3}(0 - (-1)\\cdot 3) = 1 \\end{align*} So, our matrix $L$ is \\begin{align*}L = \\begin{bmatrix} 5 & 0 & 0 \\\\ 3 & 3 & 0 \\\\ -1 & 1 & 3 \\end{bmatrix} \\quad \\quad L&#94;{T} = \\begin{bmatrix} 5 & 3 & -1 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 3 \\end{bmatrix} \\end{align*} Multiplication of this matrices is up to you. QR Decomposition QR decomposition is another type of matrix factorization, where a given $m \\times n$ matrix $A$ is decomposed into two matrices, $Q$ which is orthogonal matrix, which in turn means that $QQ&#94;{T} = Q&#94;{T}Q = I$ and the inverse of $Q$ equal to its transpose, $Q&#94;{T} = Q&#94;{-1}$, and $R$ which is upper triangular matrix. Hence, the formula is given by \\begin{align*}A = QR \\end{align*} As $Q$ is an orthogonal matrix, there are three methods to find $Q$, one is Gramm-Schmidt Process , second is Householder Transformation , and third is Givens Rotation . These methods are out of the scope of this blog post series and hence I'm going to explain all of them in separate blog posts. Consequently, there is no calculation besides python code in numerical representation section. Eigendecomposition Here is the question. What's the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform matrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula: \\begin{align*} A = Q \\Lambda Q&#94;{-1} \\end{align*} $A$ is $n\\times n$ square matrix, $Q$ is the matrix whose columns are the eigenvectors, which in turn are linearly independent and $\\Lambda$ is diagonal matrix of eigenvalues of $A$ and these eigenvalues are not necessarily distinct. To see the detailed steps of this decomposition, consider the abovementioned example of the matrix $A$ for which we already found eigenvalues and eigenvectors. \\begin{align*}A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 4 & 9 \\end{bmatrix} \\quad Q = \\begin{bmatrix} 0 & 1 & 0 \\\\ -2 & 0 & 1 \\\\ 1 & 0 & 2 \\end{bmatrix} \\quad \\Lambda = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 11 \\end{bmatrix} \\quad Q&#94;{-1} = \\begin{bmatrix} 0 & -0.4 & 0.2 \\\\ 1 & 0 & 0 \\\\ 0 & 0.2 & 0.4 \\end{bmatrix} \\end{align*} We have all the matrices and now take matrix multiplication according to the above formula. Particularly, multiply $Q$ by $\\Lambda$ and by $Q&#94;{-1}$. We have to get original matrix $A$ Furthermore, if matrix $A$ is a real symmetric matrix, then eigendecomposition can be performed by the following formula: \\begin{align*} A = Q \\Lambda Q&#94;{T} \\end{align*} The only difference between this formula and above formula is that the matrix $A$ is $n\\times n$ real symmetric square matrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real symmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example: \\begin{align*}A = \\begin{bmatrix} 6 & 2 \\\\ 2 & 3 \\end{bmatrix} \\end{align*} The matrix is symmetric because of the original matrix equal to its transpose, $A = A&#94;{T}$ Its eigenvalues are $\\lambda_{1} = 7$ and $\\lambda_{2} = 2$ and corresponding eigenvectors are \\begin{align*}v_{\\lambda_{1}} = \\begin{bmatrix} 0.89442719 \\\\ 0.4472136 \\end{bmatrix} \\quad v_{\\lambda_{2}} = \\begin{bmatrix} -0.4472136 \\\\ 0.89442719 \\end{bmatrix} \\end{align*} And in this set up, matrices $Q$, $\\Lambda$ and $Q&#94;{T}$ are the following: \\begin{align*}Q = \\begin{bmatrix} 0.89442719 & -0.4472136 \\\\ 0.4472136 & 0.89442719 \\\\ \\end{bmatrix} \\quad \\Lambda = \\begin{bmatrix} 7 & 0 \\\\ 0 & 2 \\\\ \\end{bmatrix} \\quad Q&#94;{T} = \\begin{bmatrix} 0.89442719 & 0.4472136 \\\\ -0.4472136 & 0.89442719 \\\\ \\end{bmatrix} \\end{align*} Taking matrix product gives initial matrix $A$. To verify all of this calculation see Python code below. Eigendecomposition cannot be used for nonsquare matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices. Singular Value Decomposition Singular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition. In this context, generalization means that eigendecomposition is applicable only for square $n \\times n$ matrices, while Singular Value Decomposition (SVD) is applicable for any $m \\times n$ matrices. SVD for a $m \\times n$ matrix $A$ is computed by the following formula: \\begin{align*} A = U \\ D \\ V&#94;{T} \\end{align*} Where, $U$'s columns are left singular vectors of $A$, $V$'s columns are right singular vectors of $A$ and $D$ is a diagonal matrix, not necessarily square matrix, containing singular values of $A$ on main diagonal. Singular values of $m \\times n$ matrix $A$ are the square roots of the eigenvalues of $A&#94;{T}A$, which is a square matrix. If our initial matrix $A$ is square or $n \\times n$ then singular values coincide eigenvalues. Moreover, all of these defines the path towards eigendecomposition. Let see how this path is defined. Matrices, $U$, $D$, and $V$ can be found by transforming $A$ into a square matrix and computing eigenvalues and eigenvectors of this transformed matrix. This transformation is done by multiplying $A$ by its transpose $A&#94;{T}$. After that, matrices $U$, $D$ and $V$ are the following: $U$ corresponds to the eigenvectors of $AA&#94;{T}$ $V$ corresponds to eigenvectors of $A&#94;{T}A$ $D$ corresponds to eigenvalues, either $AA&#94;{T}$ or $A&#94;{T}A$, which are the same Theory almost always seems confusing. Consider a numerical example and Python code below for clarification. Let our initial matrix $A$ be: \\begin{align*}A = \\begin{bmatrix} 0 & 1 & 0 \\\\ \\sqrt{2} & 2 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix} \\end{align*} Here, to use SVD first we need to find $AA&#94;{T}$ and $A&#94;{T}A$. \\begin{align*}AA&#94;{T} = \\begin{bmatrix} 2 & 2 & 2 \\\\ 2 & 6 & 2 \\\\ 2 & 2 & 2 \\end{bmatrix} \\quad A&#94;{T}A = \\begin{bmatrix} 2 & 2\\sqrt{2} & 0 \\\\ 2\\sqrt{2} & 6 & 2 \\\\ 0 & 2 & 2 \\end{bmatrix} \\end{align*} In the next step, we have to find eigenvalues and eigenvectors for $AA&#94;{T}$ and $A&#94;{T}A$. The characteristic polynomial is \\begin{align*} -\\lambda&#94;{3} + 10\\lambda&#94;2 - 16\\lambda \\end{align*} with roots equal to $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$. Note that these eigenvalues are the same for the $A&#94;{T}A$. We need singular values which are square root from eigenvalues. Let denote them by $\\sigma$ such as $\\sigma_{1} = \\sqrt{8} = 2\\sqrt{2}$, $\\sigma_{2} = \\sqrt{2}$ and $\\sigma_{3} = \\sqrt{0} = 0$. We now can construct diagonal matrix of singular values: \\begin{align*}D = \\begin{bmatrix} 2\\sqrt{2} & 0 & 0 \\\\ 0 & \\sqrt{2} & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\end{align*} Now we have to find matrices $U$ and $V$. We have everything what we need. First find eigenvectors of $AA&#94;{T}$ for $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$, which are the following: \\begin{align*}U_{1} = \\begin{bmatrix} \\frac{1}{\\sqrt{6}}\\\\ \\frac{2}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}} \\end{bmatrix} \\quad U_{2} = \\begin{bmatrix} -\\frac{1}{\\sqrt{3}}\\\\ \\frac{1}{\\sqrt{3}} \\\\ -\\frac{1}{\\sqrt{3}} \\end{bmatrix} \\quad U_{3} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}}\\\\ 0 \\\\ -\\frac{1}{\\sqrt{2}} \\end{bmatrix} \\end{align*} Note that eigenvectors are normilized. As we have eigenvectors, our $U$ matrix is: \\begin{align*}U = \\begin{bmatrix} \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\ \\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\ \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} \\end{bmatrix} \\end{align*} In the same fashin, we can find matrix $V$, which is: \\begin{align*}V = \\begin{bmatrix} \\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\ \\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\ \\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2} \\end{bmatrix} \\end{align*} According to the formula we have \\begin{align*} A = U \\ D \\ V&#94;{T} = \\begin{bmatrix} \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\ \\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\ \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} \\end{bmatrix} \\cdot \\begin{bmatrix} 2\\sqrt{2} & 0 & 0 \\\\ 0 & \\sqrt{2} & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\ \\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\ \\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2} \\end{bmatrix} &#94;{T} = A \\end{align*} Inverse of a Square Full Rank Matrix Here, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition. Let's get started. If a matrix $A$ can be eigendecomposed and it has no any eigenvalue equal to zero, then this matrix has the inverse and this inverse is given by: \\begin{align*}A&#94;{-1} = Q \\Lambda&#94;{-1} Q&#94;{-1} \\end{align*} Matrices, $Q$, and $\\Lambda$ are already known for us. Consider an example: \\begin{align*}A = \\begin{bmatrix} 1 & 2 \\\\ 4 & 3 \\end{bmatrix} \\end{align*} Its eigenvalues are $\\lambda_{1} = -1$ and $\\lambda_{2} = 5$ and eigenvectors are: \\begin{align*}v_{\\lambda_{1}} = \\begin{bmatrix} -0.70710678 \\\\ 0.70710678 \\end{bmatrix} \\quad v_{\\lambda_{2}} = \\begin{bmatrix} 0.4472136 \\\\ -0.89442719 \\end{bmatrix} \\end{align*} Let calculate the onverse of $A$ \\begin{align*}A&#94;{-1} = Q \\Lambda&#94;{-1} Q&#94;{-1} = \\begin{bmatrix} -0.70710678 & -0.4472136 \\\\ 0.70710678 & -0.89442719 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 & -0 \\\\ 0 & 0.2 \\end{bmatrix} \\cdot \\begin{bmatrix} -0.94280904 & 0.47140452 \\\\ -0.74535599 & -0.74535599 \\end{bmatrix} = \\begin{bmatrix} -0.6 & 0.4 \\\\ 0.8 & -0.2 \\end{bmatrix} \\end{align*} Numerical Representation Cholesky Decomposition In [7]: import numpy as np A = np . array ([[ 25 , 15 , - 5 ], [ 15 , 18 , 0 ], [ - 5 , 0 , 11 ]]) # Cholesky decomposition, find lower triangular matrix L L = np . linalg . cholesky ( A ) # Take transpose L_T = np . transpose ( L ) # Check if it's correct A == np . dot ( L , L_T ) Out[7]: array([[ True, True, True], [ True, True, True], [ True, True, True]]) QR Decomposition In [8]: import numpy as np A = np . array ([[ 12 , - 51 , 4 ], [ 6 , 167 , - 68 ], [ - 4 , 24 , - 41 ]]) # QR decomposition Q , R = np . linalg . qr ( A ) print ( \"Q =\" , Q , sep = ' \\n ' ) print () print ( \"R =\" , R , sep = ' \\n ' ) print () print ( \"A = QR\" , np . dot ( Q , R ), sep = ' \\n ' ) Q = [[-0.85714286 0.39428571 0.33142857] [-0.42857143 -0.90285714 -0.03428571] [ 0.28571429 -0.17142857 0.94285714]] R = [[ -14. -21. 14.] [ 0. -175. 70.] [ 0. 0. -35.]] A = QR [[ 12. -51. 4.] [ 6. 167. -68.] [ -4. 24. -41.]] Eigendecomposition In [9]: import numpy as np # Eigendecomposition for nonsymmetric matrix A = np . array ([[ 2 , 0 , 0 ], [ 0 , 3 , 4 ], [ 0 , 4 , 9 ]]) eigenvalues1 , eigenvectors1 = np . linalg . eig ( A ) # Form diagonal matrix from eigenvalues L1 = np . diag ( eigenvalues1 ) # Seperate eigenvector matrix and take its inverse Q1 = eigenvectors1 inv_Q = np . linalg . inv ( Q1 ) B = np . dot ( np . dot ( Q1 , L1 ), inv_Q ) # Check if B equal to A print ( \"Decomposed matrix B:\" ) print ( B ) # Numpy produces normilized eigenvectors and don't be confused with my calculations above # Eigendecomposition for symmetric matrix C = np . array ([[ 6 , 2 ],[ 2 , 3 ]]) eigenvalues2 , eigenvectors2 = np . linalg . eig ( C ) # Eigenvalues L2 = np . diag ( eigenvalues2 ) # Eigenvectors Q2 = eigenvectors2 Q2_T = Q2 . T D = np . dot ( np . dot ( Q2 , L2 ), Q2 . T ) # Check if D equal to C print () print ( \"Decomposed matrix D:\" ) print ( D ) Decomposed matrix B: [[2. 0. 0.] [0. 3. 4.] [0. 4. 9.]] Decomposed matrix D: [[6. 2.] [2. 3.]] Singular Value Decomposition In [10]: import numpy as np np . set_printoptions ( suppress = True ) # Suppres scientific notation A = np . array ([[ 0 , 1 , 0 ], [ np . sqrt ( 2 ), 2 , 0 ], [ 0 , 1 , 1 ]]) U , D , V = np . linalg . svd ( A ) print ( \"U =\" , U ) print () print ( \"D =\" , D ) print () print ( \"V =\" , V ) B = np . dot ( U , np . dot ( np . diag ( D ), V )) print () print ( \"B =\" , B ) U = [[-0.32099833 0.14524317 -0.93587632] [-0.87192053 -0.43111301 0.23215547] [-0.36974946 0.8905313 0.26502706]] D = [2.75398408 1.09310654 0.46977627] V = [[-0.44774472 -0.8840243 -0.13425984] [-0.55775521 0.15876626 0.81467932] [ 0.69888038 -0.43965249 0.56415592]] B = [[ 0. 1. -0. ] [ 1.41421356 2. 0. ] [ 0. 1. 1. ]] Inverse of a Square Full Rank Matrix In [11]: import numpy as np A = np . array ([[ 1 , 2 ], [ 4 , 3 ]]) # Eigenvalues and Eigenvectors L , Q = np . linalg . eig ( A ) # Diagonal eigenvalues L = np . diag ( L ) # Inverse inv_L = np . linalg . inv ( L ) # Inverse of igenvector matrix inv_Q = np . linalg . inv ( Q ) # Calculate the inverse of A inv_A = np . dot ( Q , np . dot ( inv_L , inv_Q )) # Print the inverse print ( \"The inverse of A is\" ) print ( inv_A ) The inverse of A is [[-0.6 0.4] [ 0.8 -0.2]] Conclusion In conclusion, my aim was to make linear algebra tutorials which are in absence, while learning machine learning or deep learning. Particularly, existing materials either are pure mathematics books which cover lots of unnecessary(actually they are necessary) things or machine learning books which assume that you already have some linear algebra knowledge. The series starts from very basic and at the end explains some advanced topics. I can say that I tried my best to filter the materials and only explained the most relevant linear algebra topic for machine learning and deep learning. Based on my experience, these tutorials are not enough to master the concepts and all intuitions but the journey should be continuous. Meaning, that you have to practice more and more. Now, I realize that, for aspiring data scientists, besides my hard work, some topic may not be still clear. For that don't hesitate and comment below. References Vector Linear Algebra Done Right Matrix Linear Algebra Topics Matrix Decomposition Cholesky Decomposition Matrix Decomposition Introduction To Linear Algebra if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/articles/mathematics/advances-of-linear-algebra-with-python.html","loc":"https://dsfabric.org/articles/mathematics/advances-of-linear-algebra-with-python.html"},{"title":"Intermediates of Linear Algebra with Python","text":"This is the third post in blog series about linear algebra. Introduction Basics of linear algebra Intermediate linear algebra Advances in linear algebra First and second posts are here and here In this post I will introduce you to the intermediates of linear algebra, which in turn includes the following: Vector Cross Product Span Linear Independence and Dependence Numerical Representation Matrix Types of Matrices Trace of a Matrix Determinant of a Matrix Minor of a Matrix Cofactor of a Matrix Determinant of a Matrix - continuation Matrix Division Inverse of a Matrix Matrix Division - continuation Solving Systems of Equations with Matrices Elemenraty Row Operations Rank of a Matrix Power of a Matrix Norm of a Matrix Numerical Representation Conclusion References Vector Cross Product In the case of the dot product between two vectors, we saw that the result is a scalar. In the case of a cross product the result is a vector, so the cross product is also called the vector product. The resulted vector is a vector that is at right angles to both the other vectors in 3D Euclidean space. This means that the cross-product only really makes sense when working with vectors that contain three components. There are two formulas to calculate the cross product. One is algebraic and second is geometric. More precisely, the first formula catches the algebraic intuition of cross product and second catches the geometric intuition of the cross product. If we have two vectors $A$ and $B$ in such a way: \\begin{align*}A = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ a_{3} \\end{bmatrix} \\ B = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ b_{3} \\end{bmatrix} \\end{align*} The algebraic formula is: \\begin{align*} \\vec{C} = \\vec{A} \\times \\vec{B} = \\begin{bmatrix} (a_{2} \\cdot b_{3}) - (a_{3} \\cdot b_{2}) \\\\ (a_{3} \\cdot b_{1}) - (a_{1} \\cdot b_{3}) \\\\ (a_{1} \\cdot b_{2}) - (a_{2} \\cdot b_{1}) \\end{bmatrix} \\end{align*} At first glance, the formula is not easy to remember. Let try another formula which uses some advanced components. \\begin{align*} \\vec{A} \\times \\vec{B} = \\left| {\\begin{array}{*{20}{c}}{\\vec{i}}&{\\vec{j}}&{\\vec{k}}\\\\{{a_1}}&{{a_2}}&{{a_3}}\\\\{{b_1}}&{{b_2}}&{{b_3}}\\end{array}} \\right| \\end{align*} where $\\vec{i}$, $\\vec{j}$ and $\\vec{k}$ are bases vectors and is explained here . To find the cross product we have to find the determinant of above matrix in the following way: \\begin{align*} \\vec{A} \\times \\vec{B} = \\left| {\\begin{array}{*{20}{c}}{{a_2}}&{{a_3}}\\\\{{b_2}}&{{b_3}}\\end{array}} \\right|\\vec{i} - \\left| {\\begin{array}{*{20}{c}}{{a_1}}&{{a_3}}\\\\{{b_1}}&{{b_3}}\\end{array}} \\right|\\vec{j} + \\left| {\\begin{array}{*{20}{c}}{{a_1}}&{{a_2}}\\\\{{b_1}}&{{b_2}}\\end{array}} \\right|\\vec{k} \\end{align*} Suppose we have two vectors: \\begin{align*}A = \\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\end{bmatrix} \\ B = \\begin{bmatrix} 1 \\\\ 2 \\\\ -2 \\end{bmatrix} \\end{align*} Then cross product is: \\begin{align*} \\vec{C} = \\vec{A} \\times \\vec{B} = \\begin{bmatrix} (3 \\cdot -2) - (1 \\cdot 2) \\\\ (1 \\cdot 1) - (2 \\cdot -2) \\\\ (2 \\cdot 2) - (3 \\cdot 1) \\end{bmatrix} = \\begin{bmatrix} -6 - 2 \\\\ 1 - -4 \\\\ 4 - 3 \\end{bmatrix} = \\begin{bmatrix} -8 \\\\ 5 \\\\ 1 \\end{bmatrix} \\end{align*} The geometric formula is: \\begin{align*} \\vec{C} = \\vec{A} \\times \\vec{B} = \\|\\vec{A}\\|\\cdot\\|\\vec{B}\\|\\cdot\\sin{\\theta} \\cdot \\hat{n} \\end{align*} where, $\\theta$ is the angle between $\\vec{A}$ and $\\vec{B}$. Also, $\\hat{n}$ is a unit vector perpendicular to both $\\vec{A}$ and $\\vec{B}$, such that $\\vec{A}$, $\\vec{B}$ and $\\hat{n}$ form a right-handed system There is also geometric application of the cross product. If we three vectors $\\vec{a}$, $\\vec{b}$ and $\\vec{c}$ which forms the three dimensional figure such as: Then the area of the parallelogram ( two dimensional front of this object ) is given by: \\begin{align*} {\\rm{Area}} = \\left\\| {\\vec{a} \\times \\vec{b}} \\right\\| \\end{align*} This is the dot product of the result of cross product between vectors $\\vec{a}$ and $\\vec{b}$ The volume of the parallelepiped ( the whole three dimensional object ) is given by: \\begin{align*} {\\rm{Volume}} = \\left| {\\vec{a} \\centerdot \\left( {\\vec{b} \\times \\vec{c}} \\right)} \\right| \\end{align*} here, absolute value bars are necessary since the result could be negative and volume must be positive. Span suppose we have set of vectors: \\begin{align*} {\\alpha _1},...,{\\alpha _n} \\in A\\ \\end{align*} then we can define the space $S$ spaned by: \\begin{align*} {\\alpha _1},...,{\\alpha _n} \\end{align*} as \\begin{align*} S\\left( {{\\alpha _1},...,{\\alpha _n}} \\right) = \\left\\{ {\\sum_{i = 1}&#94;{n} {{c_i}{\\alpha _i}\\;|\\;{c_i} \\in \\mathbb{R}} } \\right\\} \\end{align*} which is the set of all linear combinations of the vectors in this subspace. The set is a subspace of $A$: \\begin{align*} S\\left( {{\\alpha _1},...,{\\alpha _n}} \\right) \\subset A \\end{align*} In other words, if we have set of vectors \\begin{align*} A := \\{a_1, \\ldots, a_k\\} \\ \\text{in} \\ \\mathbb R&#94;n \\end{align*} it's natural to think about the new vectors we can create from these vectors by performing linear operations. New vectors created in this way are called linear combinations of $A$. Particularly, $X \\in \\mathbb{R}&#94;n$ is a linear combination of $A$ if \\begin{align*} X = \\beta_1 a_1 + \\cdots + \\beta_k a_k \\text{ for some scalars } \\beta_1, \\ldots, \\beta_k \\end{align*} here, $\\beta_1, \\ldots, \\beta_k$ are called coefficients of the linear combination. The set of linear combinations of $A$ is the span of $A$ and is written as $Span(A)$ For example, if \\begin{align*} \\vec{A} = \\{e_1, e_2, e_3\\} \\ \\text{such that} \\ e_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} , \\ e_2 := \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} , \\ e_3 := \\ \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{align*} then the span of $A$ is all of $\\mathbb{R}&#94;3$, because, for any $X=(x_1, x_2, x_3)\\in \\mathbb{R}&#94;3$, we can write \\begin{align*} X = x_1 e_1 + x_2 e_2 + x_3 e_3 \\end{align*} This means that by using $A$ or vectors $e_1$, $e_2$ and $e_3$ we can generate any vector in $\\mathbb{R}&#94;3$ by performing linear operations. Linear Independence and Dependence Above, I mentioned a linear combination. In order to define linear dependence and independence let farther clarify what is a linear combination. If we have a set of vectors \\begin{align*} \\vec{A} = \\{a_1, \\ldots, a_k\\} \\end{align*} which all have the same dimension, then A linear combination of the vectors in $A$ is any vector of the form $c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k}$, where $c_{1}, c_{2}, \\ldots, c_{k}$ are arbitrary scalars For example, if $A = \\{[1, 2], [2, 1]\\}$, then \\begin{align*} 2 a_{1} - a_{2} = 2 ([1, 2]) - [2, 1] = [0, 3] \\end{align*} is linear combination of the vectors in $A$. A set $A$ of m-dimensional vectros is linearly independent if the only linear combination of vectors in $A$ that equals $0$ is the trivial linear combination This formal definition seems a little bit confusing. Let consider the example to catch the idea. The set of vectors \\begin{align*} A = \\{[1, 0], [0, 1]\\} \\end{align*} is linearly independent. Let prove this claim. We need to find constants $c_{1}$ and $c_{2}$ satisfying \\begin{align*} c_{1} ([1, 0]) + c_{2} ([0, 1]) = [0, 0] \\end{align*} solvoing this system of equations gives that $[c_{1}, c_{2}] = [0, 0] \\rightarrow c_{1} = c_{2} = 0$, in turn this implies that $A$ is linearly independent. The following statements are equivalent to a linear independence of $A$: No vector in $A$ can be formed as a linear combination of the other vectors. This means that if we have three vectors in $A$, any of them cannot be expressed as the linear combination of the other two. If $c_{1} a_{1} + c_{2} a_{2} + \\cdots + c_{k} a_{k} = 0$, then $c_{1} = c_{2} = \\cdots = c_{k} = 0$ A set $A$ of m-dimensional vectors is linearly dependent if there is a nontrivial linear combination of the vectors in $A$ that adds up to $0$ The set of vectors \\begin{align*} A = \\{[1, 2], [2, 4]\\} \\end{align*} is linearly dependent set of vectors. Let see how. \\begin{align*} c_{1} ([1, 2]) + c_{2} ([2, 4]) = [0, 0] \\end{align*} There is a nontrivial linear combination with $c_{1} = 2$ and $c_{2} = 1$ that yields $0$. This implies $A$ is the linearly dependent set of vectors. It's easy in this case to spot linear dependence by first glance, as the second vector is 2 times the first vector, which indicates linear dependence. Numerical Representation Cross Product In [1]: import numpy as np A = np . array ([ 2 , 3 , 1 ]) B = np . array ([ 1 , 2 , - 2 ]) print ( 'A =' , A ) print ( 'B = ' , B ) print () print ( 'Cross Product is:' ) C = np . cross ( A , B ) print ( C ) A = [2 3 1] B = [ 1 2 -2] Cross Product is: [-8 5 1] This figure below shows the span of $A = \\{a_1, a_2\\}$ in $\\mathbb{R}&#94;3$. The span is a 2 dimensional plane passing through these two points and the origin In [2]: import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D from scipy.interpolate import interp2d % matplotlib inline fig = plt . figure ( figsize = ( 10 , 8 )) ax = fig . gca ( projection = '3d' ) x_min , x_max = - 5 , 5 y_min , y_max = - 5 , 5 a , b = 0.2 , 0.1 ax . set ( xlim = ( x_min , x_max ), ylim = ( x_min , x_max ), zlim = ( x_min , x_max ), xticks = ( 0 , ), yticks = ( 0 , ), zticks = ( 0 , )) gs = 3 z = np . linspace ( x_min , x_max , gs ) y = np . zeros ( gs ) x = np . zeros ( gs ) ax . plot ( x , y , z , 'k-' , lw = 2 , alpha = 0.5 ) ax . plot ( z , x , y , 'k-' , lw = 2 , alpha = 0.5 ) ax . plot ( y , z , x , 'k-' , lw = 2 , alpha = 0.5 ) # Linear function to generate plane def f ( x , y ): return a * x + b * y # Set vector coordinates x_coords = np . array (( 3 , 3 )) y_coords = np . array (( 4 , - 4 )) z = f ( x_coords , y_coords ) for i in ( 0 , 1 ): ax . text ( x_coords [ i ], y_coords [ i ], z [ i ], f '$a_{i+1}$' , fontsize = 14 ) # We need to draw lines from origin to the vectors for i in ( 0 , 1 ): x = ( 0 , x_coords [ i ]) y = ( 0 , y_coords [ i ]) z = ( 0 , f ( x_coords [ i ], y_coords [ i ])) ax . plot ( x , y , z , 'b-' , lw = 1.5 , alpha = 0.6 ) # As we already draw axes and vectors, it's time to plot the plane grid_size = 50 xr2 = np . linspace ( x_min , x_max , grid_size ) yr2 = np . linspace ( y_min , y_max , grid_size ) x2 , y2 = np . meshgrid ( xr2 , yr2 ) z2 = f ( x2 , y2 ) ax . plot_surface ( x2 , y2 , z2 , rstride = 1 , cstride = 1 , cmap = cm . jet , linewidth = 0 , antialiased = True , alpha = 0.2 ) plt . show () Linear Independence and Dependence In [3]: # Numpy has not a straightforward rule to find linearly independent vectors in some set, so we can use another Python module Sympy # Here, I'll show you how to find linearly independent vectors using Sympy and other implementations are up to you import sympy import numpy as np matrix = np . array ([[ 1 , 2 ],[ 2 , 1 ]]) # this is our set of vectors _ , inds = sympy . Matrix ( matrix ) . T . rref () print ( inds ) (0, 1) This says that the vectors at index 0 and 1 are linearly independent. Let consider a linearly dependent set of vectors to see the result of the above code clearly. In [4]: matrix = np . array ([[ 0 , 1 , 0 , 0 ],[ 0 , 0 , 1 , 0 ],[ 0 , 1 , 1 , 0 ],[ 1 , 0 , 0 , 1 ]]) # this is our set of vectors _ , inds = sympy . Matrix ( matrix ) . T . rref () print ( inds ) (0, 1, 3) This says that vectors at index 0, 1, and 3 are linearly independent, while vector at index 2 is linearly dependent. Matrix Types of Matrices During years of linear algebra evolution, there appeared different types of matrices. Some of them were fundamentals, some of them appeared lately. In this part, I will introduce some basic types of matrices and give you reference to find some other useful ones. Previously, I talked about the identity matrix, which operates as number 1 in matrix multiplication and is denoted by capital letter $I$. A square matrix is a matrix with the same number of rows and columns. \\begin{align*}A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} \\end{align*} A diagonal matrix is a matrix in which the entries on principal diagonal are non-zero and all the others are zeros. \\begin{align*}A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix} \\end{align*} Scalar multiple of the identity matrix is called scalar matrix that is also diagonal. This means on the main diagonal all elements are equal. \\begin{align*}A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix} \\end{align*} A square matrix is called triangular matrix if all of its elements above the main diagonal are zero ( lower triangular matrix ) or all of its elements below the main diagonal are zero ( upper triangular matrix ). \\begin{align*}A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 4 & 5 & 0 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 5 & 6 \\\\ 0 & 0 & 9 \\end{bmatrix} \\end{align*} These matrices are lower and upper triangular matrices, respectively. A null or zero matrix is a matrix with all elements equal to zero. \\begin{align*}A = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\end{align*} A matrix of ones is where all elements equal to 1. \\begin{align*}A = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix} \\end{align*} Symmetric matrix is a square matrix that is equal to its own transpose or $A = A&#94;T$. For example, \\begin{align*}A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix} \\end{align*} is a symmetric matrix. Furthermore, matrix elements are symmetric with respect to main diagonal or are equal. A skew-symmetric matrix is a square matrix whose transpose equals its negative or $A&#94;T = -A$. For example, \\begin{align*}A = \\begin{bmatrix} 0 & 3 & 4 \\\\ -3 & 0 & 7 \\\\ -4 & -7 & 0 \\end{bmatrix} \\ A&#94;T = \\begin{bmatrix} 0 & -3 & -4 \\\\ 3 & 0 & -7 \\\\ 4 & 7 & 0 \\end{bmatrix} \\ -A = \\begin{bmatrix} 0 & -3 & -4 \\\\ 3 & 0 & -7 \\\\ 4 & 7 & 0 \\end{bmatrix} \\end{align*} Involutory matrix is a square matrix that is equal to its own inverse. More precisely, it is the matrix whose square is the identity matrix. \\begin{align*}A = \\begin{bmatrix} -5 & -8 & 0 \\\\ 3 & 5 & 0 \\\\ 1 & 2 & -1 \\end{bmatrix} \\end{align*} then \\begin{align*}A&#94;2 = \\begin{bmatrix} -5 & -8 & 0 \\\\ 3 & 5 & 0 \\\\ 1 & 2 & -1 \\end{bmatrix} \\cdot \\begin{bmatrix} -5 & -8 & 0 \\\\ 3 & 5 & 0 \\\\ 1 & 2 & -1 \\end{bmatrix} \\ = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\ = I \\end{align*} A square matrix is called idempotent matrix, if multiplied by itself yields itself. Equivalently, $A \\cdot A = A$ \\begin{align*}A = \\begin{bmatrix} 2 & -2 & -4 \\\\ -1 & 3 & 4 \\\\ 1 & -2 & -3 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 & -2 & -4 \\\\ -1 & 3 & 4 \\\\ 1 & -2 & -3 \\end{bmatrix} \\ = \\begin{bmatrix} 2 & -2 & -4 \\\\ -1 & 3 & 4 \\\\ 1 & -2 & -3 \\end{bmatrix} \\end{align*} A nildepotent matirix is such that $A&#94;k = 0$ for some positive integer $k$. This means for some positive $k$, multipling matrix $A$ by $k$ times gives zero matrix. For matrix $A$ and for $k=2$ we have: \\begin{align*}A = \\begin{bmatrix} 5 & -3 & 2 \\\\ 15 & -9 & 6 \\\\ 10 & -6 & 4 \\end{bmatrix} \\end{align*}\\begin{align*}A&#94;2 = \\begin{bmatrix} 5 & -3 & 2 \\\\ 15 & -9 & 6 \\\\ 10 & -6 & 4 \\end{bmatrix} \\cdot \\begin{bmatrix} 5 & -3 & 2 \\\\ 15 & -9 & 6 \\\\ 10 & -6 & 4 \\end{bmatrix} \\ = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\end{align*} So, as I said there are much much more matrices, but I restricted here due to limited space. If you think you need more, definitely check this wikipedia page . Trace of a Matrix The trace of $n\\times n$ square matrix $A$ is the sum of all elements on the main diagonal. It is defined only for square matrices and the formula is: \\begin{align*} tr(A)=\\sum_{i=1}&#94;{n}a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn} \\end{align*} Where $a_{ii}$ denotes the entry on $i$-th row and $j$-th column of matrix A. For example, Let $A$ be a matrix, \\begin{align*}A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\end{align*} Then trace is: \\begin{align*} tr(A)=\\sum_{i=1}&#94;{3}a_{ii} = a_{11} + a_{22} + a_{33} = 1 + 5 + 9 = 15 \\end{align*} Determinant of a Matrix There is not plain English definition of determinant but I'll try to explain it by examples to catch the main idea behind of that special number. However, we can consider determinant as a function which as an input accepts $n \\times n$ matrix and output real or a complex number, that is called determinant of input matrix and is denoted by $det(A)$ or $|A|$. For any $2 \\times 2$ square matrix $A$ determinant is calculated by: \\begin{align*}A = \\begin{bmatrix} a & b \\\\ c & d \\\\ \\end{bmatrix} \\end{align*}\\begin{align*} det(A) = ad - bc \\end{align*} It seems easy to calculate the determinant of any $2 \\times 2$ matrix right? Now think about how to calculate determinant for higher dimensional matrices...did you find a way? If no let me explain it step by step. If we have, say $3 \\times 3$ matrix $A$ and want to calculate determinant we need some other notions such as minors and cofactors of that matrix. Minor of a Matrix A minor of matrix $A$ is the determinant of some smaller square matrix. Precisely, the minor $M_{i,j}$ is the determinant of matrix $A$ with row $i$ and column $j$ omitted. Minor of matrix $A$ is denoted by $M_{ij}$, where $i$ and $j$ denotes element of $i$-th row and $j$-th column. Let have general matrix $A$: \\begin{align*}A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} \\end{align*} We can take rows or columns to find all minors. It's up to you which one you take, rows or columns. Let take the columns. We take the first element of our matrix $a_{11}$ and delete row and column along it. As the first element is $a_{11}$, we have to delete first row and first column. After that, we take the second element of the first column which is $a_{21}$ and do same or delete second row and first column. After that, we take the third element of the first column $a_{31}$ and delete third row and first column. We have to do these for three columns. After all of that, we have: \\begin{align*}M_{11} = \\begin{bmatrix} \\square & \\square & \\square \\\\ \\square & a_{22} & a_{23} \\\\ \\square & a_{32} & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{bmatrix} \\ = a_{22}a_{33} - a_{23}a_{32} \\end{align*}\\begin{align*}M_{21} = \\begin{bmatrix} \\square & a_{12} & a_{13} \\\\ \\square & \\square & \\square \\\\ \\square & a_{32} & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} a_{12} & a_{13} \\\\ a_{32} & a_{33} \\end{bmatrix} \\ = a_{12}a_{33} - a_{13}a_{32} \\end{align*}\\begin{align*}M_{31} = \\begin{bmatrix} \\square & a_{12} & a_{13} \\\\ \\square & a_{22} & a_{23} \\\\ \\square & \\square & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{12} & a_{13} \\\\ a_{22} & a_{23} \\end{bmatrix} \\ = a_{12}a_{23} - a_{13}a_{22} \\end{align*}\\begin{align*}M_{12} = \\begin{bmatrix} \\square & \\square & \\square \\\\ a_{21} & \\square & a_{23} \\\\ a_{31} & \\square & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} a_{21} & a_{23} \\\\ a_{31} & a_{33} \\end{bmatrix} \\ = a_{21}a_{33} - a_{23}a_{31} \\end{align*}\\begin{align*}M_{22} = \\begin{bmatrix} a_{11} & \\square & a_{13} \\\\ \\square & \\square & \\square \\\\ a_{31} & \\square & a_{33} \\end{bmatrix} \\ = \\begin{bmatrix} a_{11} & a_{13} \\\\ a_{31} & a_{33} \\end{bmatrix} \\ = a_{11}a_{33} - a_{13}a_{31} \\end{align*}\\begin{align*}M_{32} = \\begin{bmatrix} a_{11} & \\square & a_{13} \\\\ a_{21} & \\square & a_{23} \\\\ \\square & \\square & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{11} & a_{13} \\\\ a_{21} & a_{13} \\end{bmatrix} \\ = a_{11}a_{13} - a_{13}a_{21} \\end{align*}\\begin{align*}M_{13} = \\begin{bmatrix} \\square & \\square & \\square \\\\ a_{21} & a_{22} & \\square \\\\ a_{31} & a_{32} & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{bmatrix} \\ = a_{21}a_{32} - a_{22}a_{31} \\end{align*}\\begin{align*}M_{23} = \\begin{bmatrix} a_{11} & a_{12} & \\square \\\\ \\square & \\square & \\square \\\\ a_{31} & a_{32} & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{31} & a_{32} \\end{bmatrix} \\ = a_{11}a_{32} - a_{12}a_{31} \\end{align*}\\begin{align*}M_{33} = \\begin{bmatrix} a_{11} & a_{12} & \\square \\\\ a_{21} & a_{22} & \\square \\\\ \\square & \\square & \\square \\end{bmatrix} \\ = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} \\ = a_{11}a_{22} - a_{12}a_{21} \\end{align*} These nine scalars are minors of matrix $A$. Once again, minor is not smaller matrix, it is determinant of a smaller matrix. Cofactor of a Matrix We left one more step to compute determinant of $3 \\times 3$ matrix $A$. This step is cofactor of matrix $A$. The cofactor of matrix $A$ is the minor, multiplied by $(-1)&#94;{i+j}$ and is denoted by $C_{ij}$ \\begin{align*}C_{ij} = (-1)&#94;{i+j} \\cdot M_{ij} \\end{align*} where $i$ is the number of row and $j$ is the number of column of matrix $A$. In the above case our cofactors are: \\begin{align*}C_{11} = (-1)&#94;{1+1} \\cdot M_{11} \\end{align*}\\begin{align*}C_{21} = (-1)&#94;{2+1} \\cdot M_{21} \\end{align*}\\begin{align*}C_{31} = (-1)&#94;{3+1} \\cdot M_{31} \\end{align*}\\begin{align*}C_{12} = (-1)&#94;{1+2} \\cdot M_{12} \\end{align*}\\begin{align*}C_{22} = (-1)&#94;{2+2} \\cdot M_{22} \\end{align*}\\begin{align*}C_{32} = (-1)&#94;{3+2} \\cdot M_{32} \\end{align*}\\begin{align*}C_{13} = (-1)&#94;{1+3} \\cdot M_{13} \\end{align*}\\begin{align*}C_{23} = (-1)&#94;{2+3} \\cdot M_{23} \\end{align*}\\begin{align*}C_{33} = (-1)&#94;{3+3} \\cdot M_{33} \\end{align*} So, the sum of $i$ and $j$ in the power of $(-1)$ switch the sign of every minor. Determinant of a Matrix - continuation We are ready to compute the determinant of our $3 \\times 3$ matrix $A$. We need to expand this matrix along one of the row or one of the column to compute the determinant. It's up to you which one you take, row or column. Let take the first column. Now, what does expansion means? We have to fix either $i$ if we choose a row, or $j$ if we choose column. At first glance it seems confusing but an example will make sense. This expansion is called Laplace Expansion and is used to compute the determinant of any $n \\times n$ matrix. \\begin{align*} det(A) = \\sum_{j\\prime=1}&#94;{n}a_{ij\\prime}C_{ij\\prime} \\ = \\sum_{i\\prime=1}&#94;{n}a_{i\\prime j}C_{i\\prime j} \\end{align*} where $i\\prime$ means we fixed index $i$ or row and we change only column index. In case of $j\\prime$ we fixed index $j$ or columns and change the only row. So, when $i$ is fixed it is called row expansion and when $j$ is fixed it's called column expansion. $C_{ij}$ is our cofactor. To continue the above example, let expand our initial matrix $A$ by the first column, meaning I fix $j$ to be 1 and only change row index $i$ from 1 to 3. In this particular case above formula is: \\begin{align*} det(A) = \\sum_{j\\prime=1}&#94;{3}a_{ij\\prime}C_{ij\\prime} \\ = a_{11}C_{11} + a_{21}C_{21} + a_{31}C_{31} \\end{align*} Instead, if I choose first row, I have to fix row index $i$ and change column index $j$ from 1 to 3 and determinant formula is: \\begin{align*} det(A) = \\sum_{i\\prime=1}&#94;{3}a_{i\\prime j}C_{i\\prime j} \\ = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} \\end{align*} Below, you will see numerical example. Matrix Division We can't actually divide matrix by a matrix; but when we want to divide matrices, we can take advantage of the fact that division by a given number is the same as multiplication by the reciprocal of that number. For matrix division, we use a related idea; we multiply matrix by the inverse of a matrix. If we have two matrices $A$ and $B$, we can do: \\begin{align*} A \\div B = A \\cdot B&#94;{-1} \\end{align*} Here, $B&#94;{-1}$ is the inverse of matrix $B$. As taking inverse of a matrix requires computaions and is not easy, let explain it below and then return here. Inverse of a Matrix The matrix is said to be invertible if: \\begin{align*} A \\cdot A&#94;{-1} = I \\end{align*} where $I$ is the identity matrix and $A&#94;{-1}$ is the inverse of $A$. Generally, matrix inverse is only defined for square matrices, but there still exist ways to take the inverse of non-square matrices but this is out of the scope of this blog series and I will not consider. For $2 \\times 2$ matrix $A$ \\begin{align*}A = \\begin{bmatrix} a & b \\\\ c & d \\\\ \\end{bmatrix} \\end{align*} Inverse of $A$ is: \\begin{align*}A&#94;{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\\\ \\end{bmatrix} \\end{align*} What happened there? I swapped the positions of a and d I changed the signs of b and c I multiplied the resulting matrix by 1 over the determinant of the matrix $A$ For example, \\begin{align*}A = \\begin{bmatrix} 6 & 2 \\\\ 1 & 2 \\\\ \\end{bmatrix} \\quad A&#94;{-1} = \\frac{1}{(6 \\times 2) - (2 \\times 1)} \\begin{bmatrix} 2 & -2 \\\\ -1 & 6 \\\\ \\end{bmatrix} \\ = \\begin{bmatrix} 0.2 & -0.2 \\\\ -0.1 & 0.6 \\\\ \\end{bmatrix} \\end{align*} to check if this is really the inverse of $A$, multiply $A$ by its inverse in order to get an identity matrix. Now let take the inverse of $3 \\times 3$ matrix. This process is long and involves taking minors, cofactors and determinant. After that, above-mentioned operations should be understandable. It has to be mentioned that there are several ways to take matrix inverse but as I started here explaining minors, cofactors and determinant I use this technique to find inverse. We can calculate the inverse by: step 1: Calculate the matrix of minors step 2: Turn the matrix of minors into the matrix of cofactors step 3: Transpose the matrix of cofactors step 4: Multiply transpose of cofactor by 1/determinant Let have matrix: \\begin{align*}A = \\begin{bmatrix} 4 & 2 & 2 \\\\ 6 & 2 & 4 \\\\ 2 & 2 & 8 \\end{bmatrix} \\end{align*} Step 1: Calculate the matrix of minors \\begin{align*}M_{11} = \\begin{bmatrix}\\color{blue}4 & \\color{lightgray}2 & \\color{lightgray}2\\\\\\color{lightgray}6 & \\color{red}2 & \\color{red}4\\\\\\color{lightgray}2 & \\color{red}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(2\\times8) - (4\\times2) = 8\\;\\;\\;\\;\\begin{bmatrix}8 & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}? \\end{bmatrix} \\end{align*}\\begin{align*}M_{12} = \\begin{bmatrix}\\color{lightgray}4 & \\color{blue}2 & \\color{lightgray}2\\\\\\color{red}6 & \\color{lightgray}2 & \\color{red}4\\\\\\color{red}2 & \\color{lightgray}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(6\\times8) - (4\\times2) = 40\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} \\end{align*}\\begin{align*}M_{13} = \\begin{bmatrix}\\color{lightgray}4 & \\color{lightgray}2 & \\color{blue}2\\\\\\color{red}6 & \\color{red}2 & \\color{lightgray}4\\\\\\color{red}2 & \\color{red}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(6\\times2) - (2\\times2) = 8\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} \\end{align*}\\begin{align*}M_{21} = \\begin{bmatrix}\\color{lightgray}4 & \\color{red}2 & \\color{red}2\\\\\\color{blue}6 & \\color{lightgray}2 & \\color{lightgray}4\\\\\\color{lightgray}2 & \\color{red}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(2\\times8) - (2\\times2) = 12\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & \\color{lightgray}? & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} \\end{align*}\\begin{align*}M_{22} = \\begin{bmatrix}\\color{red}4 & \\color{lightgray}2 & \\color{red}2\\\\\\color{lightgray}6 & \\color{blue}2 & \\color{lightgray}4\\\\\\color{red}2 & \\color{lightgray}2 & \\color{red}8\\end{bmatrix}\\;\\;\\;\\;(4\\times8) - (2\\times2) = 28\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & \\color{lightgray}?\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} \\end{align*}\\begin{align*}M_{23} = \\begin{bmatrix}\\color{red}4 & \\color{red}2 & \\color{lightgray}2\\\\\\color{lightgray}6 & \\color{lightgray}2 & \\color{blue}4\\\\\\color{red}2 & \\color{red}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(4\\times2) - (2\\times2) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\\\color{lightgray}? & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} \\end{align*}\\begin{align*}M_{31} = \\begin{bmatrix}\\color{lightgray}4 & \\color{red}2 & \\color{red}2\\\\\\color{lightgray}6 & \\color{red}2 & \\color{red}4\\\\\\color{blue}2 & \\color{lightgray}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(2\\times4) - (2\\times2) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & \\color{lightgray}? & \\color{lightgray}?\\end{bmatrix} \\end{align*}\\begin{align*}M_{32} = \\begin{bmatrix}\\color{red}4 & \\color{lightgray}2 & \\color{red}2\\\\\\color{red}6 & \\color{lightgray}2 & \\color{red}4\\\\\\color{lightgray}2 & \\color{blue}2 & \\color{lightgray}8\\end{bmatrix}\\;\\;\\;\\;(4\\times4) - (2\\times6) = 4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & 4 & \\color{lightgray}?\\end{bmatrix} \\end{align*}\\begin{align*}M_{33} = \\begin{bmatrix}\\color{red}4 & \\color{red}2 & \\color{lightgray}2\\\\\\color{red}6 & \\color{red}2 & \\color{lightgray}4\\\\\\color{lightgray}2 & \\color{lightgray}2 & \\color{blue}8\\end{bmatrix}\\;\\;\\;\\;(4\\times2) - (2\\times6) = -4\\;\\;\\;\\;\\begin{bmatrix}8 & 40 & 8\\\\12 & 28 & 4\\\\4 & 4 & -4\\end{bmatrix} \\end{align*} Our matrix of minors is: \\begin{align*}M = \\begin{bmatrix} 8 & 40 & 8 \\\\ 12 & 28 & 4 \\\\ 4 & 4 & -4 \\end{bmatrix} \\end{align*} Note that I used rows to find minors, in contrast to columns in the previous example. Step 2: Turn the matrix of minors into the matrix of cofactors To turn minors matrix into cofactor matrix, we just need to change the sign of elements in minors matrix according to the rule proposed above section. Cofactor matrix is: \\begin{align*}C = \\begin{bmatrix} 8 & -40 & 8 \\\\ -12 & 28 & -4 \\\\ 4 & -4 & -4 \\end{bmatrix} \\end{align*} Step 3: Transpose the matrix of cofactors We need to take the transpose of the cofactor matrix. In other words, swap their positions over the main diagonal (the main diagonal stays the same). \\begin{align*}C&#94;{T}= \\begin{bmatrix}8 & \\color{green}-\\color{green}1\\color{green}2 & \\color{orange}4\\\\\\color{green}-\\color{green}4\\color{green}0 & 28 & \\color{purple}-\\color{purple}4\\\\\\color{orange}8 & \\color{purple}-\\color{purple}4 & -4\\end{bmatrix} \\end{align*} This matrix is called Adjugate or Adjoint , which is simple the transpose of the cofactor matrix. Step 4: Multiply transpose of cofactor by 1/determinant As we did all the necessary operations to have determinant let compute it firstly and then multiply the adjoint matrix by 1/determinant. Using formula: \\begin{align*} det(A) = \\sum_{i\\prime=1}&#94;{3}a_{i\\prime j}C_{i\\prime j} \\ = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} \\end{align*} We have: \\begin{align*} det(A) = (4 \\times 8) + (2 \\times (-40)) + (2 \\times 8) = -32 \\end{align*} Now the inverse is: \\begin{align*}A&#94;{-1} = \\frac{1}{-32} \\cdot \\begin{bmatrix} 8 & -40 & 8 \\\\ -12 & 28 & -4 \\\\ 4 & -4 & -4 \\end{bmatrix} \\ = \\begin{bmatrix} -0.25 & 0.375 & -0.125 \\\\ 1.25 & 0.875 & 0.125 \\\\ -0.25 & 0.125 & 0.125 \\end{bmatrix} \\end{align*} Let's verify that the original matrix multiplied by the inverse results in an identity matrix: \\begin{align*}A \\cdot A&#94;{-1} = \\begin{bmatrix}4 & 2 & 2\\\\6 & 2 & 4\\\\2 & 2 & 8\\end{bmatrix} \\cdot \\begin{bmatrix}-0.25 & 0.375 & -0.125\\\\1.25 & -0.875 & 0.125\\\\-0.25 & 0.125 & 0.125\\end{bmatrix} \\end{align*}\\begin{align*} = \\begin{bmatrix}(4\\times-0.25)+(2\\times1.25)+(2\\times-0.25) & (4\\times0.375)+(2\\times-0.875)+(2\\times0.125) & (4 \\times-0.125)+(2 \\times-0.125)+(2 \\times0.125)\\\\ (6 \\times-0.25)+(2\\times1.25)+(4\\times-0.25) & (6\\times0.375)+(2 \\times-0.875)+(4\\times0.125) & (6 \\times-0.125)+(2 \\times-0.125)+(4 \\times0.125)\\\\ (2\\times-0.25)+(2\\times1.25)+(8\\times-0.25) & (2 \\times0.375)+(2 \\times-0.875)+(8 \\times0.125) & (2 \\times-0.125)+(2 \\times-0.125)+(8 \\times0.125)\\end{bmatrix} \\end{align*}\\begin{align*} = \\begin{bmatrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1\\end{bmatrix} = I \\end{align*} Do you see how challenging can be finding the inverse of $4 \\times 4$ matrix? That's why we use calculators or computer program to compute it. Matrix Division - continuation As we already know how to compute the inverse of a matrix, the division is easy now. If we have two matrices: \\begin{align*}A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\end{align*} and \\begin{align*}B = \\begin{bmatrix} 4 & 2 & 2 \\\\ 6 & 2 & 4 \\\\ 2 & 2 & 8 \\end{bmatrix} \\end{align*} then $A$ devided by $B$ is \\begin{align*}A \\cdot B&#94;{-1} \\ = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\cdot \\begin{bmatrix} -0.25 & 0.375 & -0.125 \\\\ 1.25 & 0.875 & 0.125 \\\\ -0.25 & 0.125 & 0.125 \\end{bmatrix} \\ = \\begin{bmatrix} 1.5 & -1 & 0.5 \\\\ 3.75 & -2.125 & 0.875 \\\\ 6 & -3.25 & 1.25 \\end{bmatrix} \\ \\equiv A \\div B \\end{align*} Solving System of Equations with Matrices In the previous blog, I talked about the system of linear equations and we solved this system graphically and algebraically. One of the great things about matrices, is that they can help us solve systems of equations. For example, consider the following system of equations: \\begin{align*} \\begin{cases} 2x + 4y = 18 \\\\ 6x + 2y = 34 \\end{cases} \\end{align*} We can write this in matrix form, like this: \\begin{align*} \\begin{bmatrix} 2 & 4 \\\\ 6 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\ = \\begin{bmatrix} 18 \\\\ 34 \\end{bmatrix} \\end{align*} If we calculate the dot product between the matrix and vector on the left side, we can see clearly that this represents the original equations. Now let rename our matrices: \\begin{align*}A = \\begin{bmatrix} 2 & 4 \\\\ 6 & 2 \\end{bmatrix} \\;\\;\\;\\; X = \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\;\\;\\;\\; B = \\begin{bmatrix} 18 \\\\ 34 \\end{bmatrix} \\end{align*} This can be represented as $AX = B$ and we know that to find $X$ we have to solve this: $B \\div A$. Since we cannot divide matrices in this way, we have to use the previous technique. Find the inverse of $A$ and multiply by $B$. The inverse of $A$: \\begin{align*}A&#94;{-1} = \\begin{bmatrix} -0.1 & 0.2 \\\\ 0.3 & -0.1 \\end{bmatrix} \\end{align*}\\begin{align*}X = \\begin{bmatrix} -0.1 & 0.2 \\\\ 0.3 & -0.1 \\end{bmatrix} \\cdot \\begin{bmatrix} 18 \\\\ 34 \\end{bmatrix} \\ = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} \\end{align*} Now, instead of $x$ and $y$ in the original equation put $5$ and $2$ and this will make equality true. \\begin{align*}10 + 8 = 18\\end{align*}\\begin{align*}30 + 4 = 34\\end{align*} Elementary Row Operations Elementary row operations (ERO) play an important role in many matrix algebra applications, such as finding the inverse of a matrix and solving simultaneous linear equations. These topics are covered here . An ERO transforms a given matrix $A$ into a new matrix $A&#94;{'}$ via one of the following operations: Interchange two rows (or columns) Multiply each element in a row (or column) by a non-zero number Multiply a row (or column) by a non-zero number and add the result to another row (or column) To catch the idea behind this operations let do the example. We have a matrix $A$ such that \\begin{align*}A = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 1 & 3 & 5 & 6 \\\\ 0 & 1 & 2 & 3 \\end{bmatrix} \\end{align*} Type 1 ERO that interchange rows 1 and 3 of $A$ would yield \\begin{align*}A&#94;{'} = \\begin{bmatrix} 0 & 1 & 2 & 3 \\\\ 1 & 3 & 5 & 6 \\\\ 1 & 2 & 3 & 4 \\end{bmatrix} \\end{align*} Type 2 ERO that multiplies row 2 of $A$ by 3 would yield \\begin{align*}A&#94;{'} = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 3 & 9 & 15 & 18 \\\\ 0 & 1 & 2 & 3 \\end{bmatrix} \\end{align*} Type 3 ERO that multiplies row 2 of $A$ by 4 and replace row 3 of $A$ by $4 \\times (\\text{row 2 of A}) + \\text{row 3 of A}$ would yiled row 3 of $A&#94;{'}$ to be $ 4 \\times [1 \\ 3 \\ 5 \\ 6] + [0 \\ 1 \\ 2 \\ 3] = [4 \\ 13 \\ 22 \\ 27]$ and \\begin{align*}A&#94;{'} = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 1 & 3 & 5 & 6 \\\\ 4 & 13 & 22 & 27 \\end{bmatrix} \\end{align*} Except this, to perform an elementary row operation on the matrix $A$, first we can perform the operation on the corresponding identity matrix to obtain an elementary matrix, then multiply $A$ on the left by this elementary matrix. More precisely this means that we take one ERO, whichever we want and perform this operation on corresponding identity matrix of $A$. If $A$ has $m \\times n$ dimension we have identity matrix $I_{m \\times n}$. After that, we multiply $A$ by this identity matrix. If we denote the elementary matrix by $E$ then, we multiply $A$ by $E$ in the following way: \\begin{equation}E_{1} \\cdot A \\end{equation} where $E_{1}$ is ERO one performed on identity matrix. Rank of a Matrix The maximum number of linearly independent rows in a matrix $A$ is called the row rank of $A$ and the maximum number of linearly independent columns in $A$ is called the column rank of $A$. If $A$ is $n \\times m$ matrix, that is if matrix $A$ has $m$ rows and $n$ columns then the following inequality holds: $$ \\text{row rank of} \\ A \\leq m \\\\ \\text{column rank of} \\ A \\leq n $$ Furthermore, for any matrix $A$ $$ \\text{row rank of} \\ A = \\text{column rank of} \\ A $$ From the above inequality it follows that $$ Rank(A) \\leq min(m, n) $$ This means that if a matrix has, for example, 3 rows and 5 columns, its rank cannot be more than 3. The rank of a matrix would be zero if and only if the matrix had no elements. If a matrix had even one element, its minimum rank would be one. When all of the vectors in a matrix are linearly independent, the matrix is said to be full rank . To calculate the rank of a matrix, we have to compute the determinant. It turns out that the rank of a matrix $A$, denoted by $Rank(A)$ is the size of the largest non-zero $m \\times m$ submatrix with non-zero determinant. To simplify further, if the determinant of $4 \\times 4$ matrix $A$ is zero and any $3 \\times3$ submatrix of original matrix $A$ has non-zero determinant then the rank of the original matrix $A$ is $3$. So we can say that rank shows the \"nondegenerateness\" of the matrix $A$. Actually, there is no only one way to compute the rank. I provided one more way in the advanced tutorial here Power of a Matrix We can rise a square matrix $A$ in any nonnegative power just like any number. This is defined as the product of $A$ by itself $n$ times. If matrix $A$ has inverse, then $A&#94;{-n} = (A&#94;{-1})&#94;{n}$ or take inverse of $A$ and multiply by itself $n$ times. For example, if \\begin{align*}A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\end{align*} then \\begin{align*}A&#94;{2} = A A \\ = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\ = \\begin{bmatrix} 7 & 10 \\\\ 15 & 22 \\end{bmatrix} \\end{align*} Norm of a Matrix In the previous post I talked about vector norms but did not mentioned matrix norm. There are three types of matrix norms: Matrix norms induced by vector norms Entrywise matrix norms Schatten norms Here, I will introduce only the first two types of matrix norm and depict one example of each to give the general idea of matrix norms. Induced norms usually are denoted by: $$\\|A\\|_p$$ In the special cases of $p = 1, 2, \\infty$, the induced matrix norms can be computed by: \\begin{align*} \\|A\\|_1 = max_{1\\leq j \\leq m} \\sum_{i = 1}&#94;{m}|a_{ij}| \\end{align*} Which is the maximum absolute column sum of the matrix $A$ \\begin{align*} \\|A\\|_2 = \\|A\\|_F = \\sigma_{max}(A) \\end{align*} Where $\\|A\\|_F$ is Frobenius Norm, which will be discussed below and $\\sigma_{max}(A)$ is the spectral norm. The later will be discussed in the next post. \\begin{align*} \\|A\\|_{\\infty} = max_{1\\leq i \\leq m} \\sum_{j = 1}&#94;{n}|a_{ij}| \\end{align*} which is the maximum absolute row sum of the matrix $A$. To clarify this farther, let consider the following example: \\begin{align*}A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\end{align*}\\begin{align*} \\|A\\|_1 = max(|1| + |4| + |7|; |2| + |5| + |8|; |3| + |6| + |9|) = max(12; 15; 18) = 18 \\end{align*}\\begin{align*} \\|A\\|_{\\infty} = max(|1| + |2| + |3|; |4| + |5| + |6|; |7| + |8| + |9|) = max(6; 15; 24) = 24 \\end{align*} Imagine, we have a vector whose elements are matrices instead of scalars. Then norm defined here is entrywise matrix norm. The general formula for entrywise matrix norm is: \\begin{align*} \\|A\\|_{p,q} = \\left(\\sum_{j=1}&#94;{n}\\left(\\sum_{i=1}&#94;{m}|a_{ij}&#94;{p}\\right)&#94;\\frac{q}{p}\\right)&#94;\\frac{1}{q} \\end{align*} where $p,q \\geq 1$ When $p=q=2$ we have Frobenius Norm or Frobenius Inner Product: \\begin{align*} \\|A\\|_{F} = \\sqrt{\\sum_{i=1}&#94;m \\sum_{j=1}&#94;{n}|a_{ij}|&#94;2} \\end{align*} and when $p=q=\\infty$, we have Max Norm: \\begin{align*} \\|A\\|_{max} = max_{ij}|a_{ij}| \\end{align*} If we have matrix $A$ such that: \\begin{align*}A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\end{align*} Then Frobenius Norm is: \\begin{align*} \\|A\\|_{F} = \\sqrt{(|1|&#94;2 + |2|&#94;2 + |3|&#94;2 + |4|&#94;2 + |5|&#94;2 + |6|&#94;2 + |7|&#94;2 + |8|&#94;2 + |9|&#94;2)} = \\sqrt{1 + 4 + 9 + 16 + 25 + 36 + 49 + 64 + 81} = \\sqrt{285} \\approx 16.89 \\end{align*} For two matrices $A$ and $B$ we have Frobenius Inner Product: \\begin{align*} \\langle A,B \\rangle_{F} = \\sum_{i,j}\\overline{A_{i,j}}B_{i,j} = tr \\left(\\overline{A&#94;T}B \\right) \\end{align*} Where overline denotes the complex conjugate of a matrix. If \\begin{align*}A = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\\\ \\end{bmatrix} \\end{align*} And \\begin{align*}B = \\begin{bmatrix} 8 & -3 \\\\ 4 & 1 \\\\ \\end{bmatrix} \\end{align*} Then \\begin{align*} \\langle A,B \\rangle_{F} = 2 \\cdot 8 + 0 \\cdot (-3) + 1 \\cdot 4 + 1 \\cdot 1 = 21 \\end{align*} Numerical Representation Types of Matrices In [5]: import numpy as np # Diagonal Matrix diagonal = np . diag ([ 1 , 2 , 3 ]) print ( 'Diagonal Matrix' ) print ( diagonal ) print () # Lower Triangular Matrix low_triang = np . tril ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( 'Lower Triangular Matrix' ) print ( low_triang ) print () # Upper Triangular Matrix upper_triang = np . triu ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( 'Upper Triangular Matrix' ) print ( upper_triang ) print () # Matrix of Zeros zeros = np . zeros (( 3 , 3 ), dtype = int ) # \"dtype=int\" is necessary not to get float values print ( 'Matrix of Zeros' ) print ( zeros ) print () # Matrix of Ones ones = np . ones (( 3 , 3 ), dtype = int ) print ( 'Matrix of Ones' ) print ( ones ) print () # Identity Matrix identity = np . eye ( 3 , dtype = int ) print ( 'Identity Matrix' ) print ( identity ) Diagonal Matrix [[1 0 0] [0 2 0] [0 0 3]] Lower Triangular Matrix [[1 0 0] [4 5 0] [7 8 9]] Upper Triangular Matrix [[1 2 3] [0 5 6] [0 0 9]] Matrix of Zeros [[0 0 0] [0 0 0] [0 0 0]] Matrix of Ones [[1 1 1] [1 1 1] [1 1 1]] Identity Matrix [[1 0 0] [0 1 0] [0 0 1]] Trace of a Matrix In [6]: import numpy as np A = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) trace = np . trace ( A ) print ( trace ) 15 Determinant of a Matrix In [7]: import numpy as np A = np . array ([[ 4 , 2 , 2 ], [ 6 , 2 , 4 ], [ 2 , 2 , 8 ]]) determinant = np . linalg . det ( A ) print ( determinant ) -32.000000000000014 Inverse of a Matrix In [8]: import numpy as np A = np . array ([[ 4 , 2 , 2 ], [ 6 , 2 , 4 ], [ 2 , 2 , 8 ]]) inverse = np . linalg . inv ( A ) print ( inverse ) [[-0.25 0.375 -0.125] [ 1.25 -0.875 0.125] [-0.25 0.125 0.125]] Matrix Division In [9]: import numpy as np A = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) B = np . array ([[ 4 , 2 , 2 ], [ 6 , 2 , 4 ], [ 2 , 2 , 8 ]]) # A divided by B is dot product btw A and inverse of B inv_B = np . linalg . inv ( B ) X = np . dot ( A , inv_B ) print ( X ) [[ 1.5 -1. 0.5 ] [ 3.75 -2.125 0.875] [ 6. -3.25 1.25 ]] Solving System of Equations with Matrices In [10]: import numpy as np A = np . array ([[ 2 , 4 ], [ 6 , 2 ]]) B = np . array ([[ 18 ], [ 34 ]]) A_inverse = np . linalg . inv ( A ) print ( 'The inverse of A is' , A_inverse , sep = ' \\n ' ) print () X = np . dot ( A_inverse , B ) print ( 'X =' , X , sep = ' \\n ' ) The inverse of A is [[-0.1 0.2] [ 0.3 -0.1]] X = [[5.] [2.]] Rank of a Matrix In [11]: import numpy as np A = np . array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ,], [ 1 , - 3 , 5 ]]) rank_A = np . linalg . matrix_rank ( A ) print ( 'Rank(A) = ' , rank_A ) print () # matrix B has full rank B = np . array ([[ 2 , 2 , - 1 ], [ 4 , 0 , 2 ], [ 0 , 6 , - 3 ]]) rank_B = np . linalg . matrix_rank ( B ) print ( 'Rank(B) = ' , rank_B ) Rank(A) = 2 Rank(B) = 3 Power of a Matrix In [12]: import numpy as np A = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) A_square = np . linalg . matrix_power ( A , 2 ) print ( A_square ) [[ 7 10] [15 22]] Norm of a Matrix In [13]: import numpy as np A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) frobenius_norm = np . linalg . norm ( A , ord = 'fro' ) column_max_norm = np . linalg . norm ( A , ord = 1 ) row_max_norm = np . linalg . norm ( A , ord = np . inf ) # same as infinity norm # Frobenius Inner Product A = np . array ([[ 2 , 0 ], [ 1 , 1 ]]) B = np . array ([[ 8 , - 3 ], [ 4 , 1 ]]) frobenius_inner_product = np . vdot ( A , B ) # numpy.vdot function flattens high dimensional arrays and take dot product print ( 'Frobenius Norm is:' , frobenius_norm ) print ( 'Column Max Norm is:' , column_max_norm ) print ( 'Row Max Norm is:' , row_max_norm ) print ( 'Frobenius Inner Product is:' , frobenius_inner_product ) Frobenius Norm is: 16.881943016134134 Column Max Norm is: 18.0 Row Max Norm is: 24.0 Frobenius Inner Product is: 21 Conclusion To sum up, we've covered a lot of materials. Some of them seemed easy, while some of them at the first glance seemed complex, but I do hope a little more practice and reading this tutorial 2 times will help you to master all of these intuitions further. For any questions, suggestions, corrections and/or short speeches comment below. References Vector Linear Algebra Done Right Linear Algebra Topics Matrix Introduction To Linear Algebra Linear Algebra Topics Khan Academy - Linear Algebra if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/articles/mathematics/intermediates-of-linear-algebra-with-python.html","loc":"https://dsfabric.org/articles/mathematics/intermediates-of-linear-algebra-with-python.html"},{"title":"Basics of Linear Algebra with Python","text":"This is the second post in blog series about linear algebra. Introduction Basics of linear algebra Intermediate linear algebra Advances in linear algebra Fisrt post is here In this post I will introduce you to the basics of linear algebra, which in turn includes the following: One Variable Equation Two Variable Equation The Systems of Equations Vector Vectors and Vector Notation Dimensions of Vector Operations on Vectors Vector Length Unit Vector Scalar Product of Two Vectors Numerical Representation Matrix Matrices and Matrix Notation Dimension of Matrix Matrix Operations Matrix Transpose Identity Matrix Numerical Representation Conclusion References One Variable Equation Generally, equations state that two things are equal. They contain one or more variables and solving them means to find the value of those variable to make equality true. This value is known as a solution. Consider the following equation: \\begin{align*} 2x + 5 = 15 \\end{align*} In this case, our variable is $x$ and the solution is $10$. In [1]: x = 5 2 * x + 5 == 15 Out[1]: True Two Variable Equation Equations with two variables are known as linear equations. Consider the following equation: \\begin{align*} 2y + 3 = 2x - 1 \\end{align*} This equation includes two different variables, $x$, and $y$. These variables depend on one another. The value of $x$ is determined in part by the value of $y$ and vice-versa. So we can't solve this equation as in the case of one variable equation. However, we can express $y$ in terms of $x$ and obtain a result that describes a relative relationship between the variables. For example, let's solve this equation for $y$. First, rearrage equation in a way to have following: \\begin{align*}2y = 2x - 4 \\Rightarrow \\\\ \\Rightarrow y = x - 2 \\end{align*} Note that this is not linear function, this is an affine function . Below we will see the solution of the above equation for various values of $y$. It's also good practice to plot the graph to visualize the solutions. In [2]: import pandas as pd # Create a dataframe with a column x, containing values from -10 to 10 df = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) # Add column y, by applying the solved equation to x df [ 'y' ] = df [ 'x' ] - 2 # Display the dataframe #df Above table shows valid solutions for values of $x$ in range $(-10, 10)$. Besides numerical solution, let see the graphical solution. In [3]: import matplotlib.pyplot as plt % matplotlib inline plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df [ 'x' ], df [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . show () The solution of the above equation lies on the blue line, for any value pairs $(x,y)~\\in~\\mathbb{R}$ When we use a linear equation to plot a line, we can easily see where the line intersects the X and Y axes of the plot. These points are known as intercepts . The x-intercept is where the line intersects the X (horizontal) axis, and the y-intercept is where the line intersects the Y (horizontal) axis. The x-intercept is the point where the line crosses the $X$ axis, and at this point, the value for $y$ is always 0. Similarly, the y-intercept is where the line crosses the $Y$ axis, at which $x$ value is 0. So to find the intercepts, we need to solve the equation for $x$ when $y$ is 0 and for $y$ when $x$ is 0. For the x-intercept, we have: \\begin{align*} y = x - 2 = 0 \\Rightarrow x = 2 \\end{align*} For y-intercept, we have: \\begin{align*} y = x - 2 \\Rightarrow y = 0 - 2 \\Rightarrow y = -2 \\end{align*} In [4]: import matplotlib.pyplot as plt % matplotlib inline plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df [ 'x' ], df [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . annotate ( 'x-intercept' ,( 2 , 0 ), color = 'red' , fontsize = 12 ) plt . annotate ( 'y-intercept' ,( 0 , - 2 ), color = 'red' , fontsize = 12 ) plt . show () It is natural to ask, what if we move one unit along the $x$ axis, how the value for the $y$ change? The answer to this question is the notion of slope . Slope is defined as \\begin{align*} m = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}} \\end{align*} This means that for any given two ordered pairs of $x$ and $y$, how a change in $x$ affect $y$. For example: (5, 3) (6, 4) So, according to our formula, slope equal to: \\begin{align*} m = \\frac{4 - 3}{6 - 5} = 1 \\end{align*} So what does that actually mean? Well, if we start from any point on the blue line and move one unit to the right (along with the $X$ axis), we'll need to move 1 unit up (along with the $Y$ axis) to get back to the blue line. The Systems of Equations To have the system of equations means that we have two or more linear equations together and we have to solve them simultaneously to make the equality true. There are three possible solutions of the linear system. One solution, No solution or system is inconsistent and infinitely many solutions. Generally, the linear system can have two or more variables and two or more equations. There, I will consider two variable and two-equation system with three solutions, in order to depict the intuition. It's up to you to delve deeper. The system with one solution, meaning two lines intersect \\begin{align*} \\begin{cases} 2x-y = 2 \\\\ x+y = -2 \\end{cases} \\end{align*} If we divide these equations we'll get $x=-2$ and $y=0$. This is the solution. Now let see it graphically. In [5]: import pandas as pd import matplotlib.pyplot as plt % matplotlib inline df_1 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) # Add column y, by applying the solved equation to x df_1 [ 'y' ] = 2 * df_1 [ 'x' ] - 2 df_2 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) df_2 [ 'y' ] = - 1 * df_2 [ 'x' ] - 2 plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df_1 [ 'x' ], df_1 [ 'y' ]) plt . plot ( df_2 [ 'x' ], df_2 [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . annotate ( '2x - y = 2' , ( 7.5 , 18 ), weight = 'bold' ) plt . annotate ( 'x + y = -2' , ( - 10 , 10 ), weight = 'bold' ) plt . annotate ( 'Solution (x = -2 ; y = 0)' ,( 0 , - 3 )) # I put coordinates(0,-3) intentionally to make annotation look clear plt . show () The system with no solution or inconsistent system, meaning two lines are parallel \\begin{align*} 3x+2y = 3 \\\\ 3x+2y = -4 \\end{align*} The system is inconsistent. There is no solution. In [6]: import pandas as pd import matplotlib.pyplot as plt % matplotlib inline # Dataframe for first equation df_1 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) df_1 [ 'y' ] = ( - 3 / 2 ) * df_1 [ 'x' ] + 3 / 2 # Dataframe for second equation df_2 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) df_2 [ 'y' ] = ( - 3 / 2 ) * df_2 [ 'x' ] - 2 plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df_1 [ 'x' ], df_1 [ 'y' ]) plt . plot ( df_2 [ 'x' ], df_2 [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . annotate ( '3x + 2y = 3' , ( - 5 , 10 ), weight = 'bold' ) plt . annotate ( '3x + 2y = -4' , ( - 10 , 7.5 ), weight = 'bold' ) plt . show () The system with infinitely many solutions, meaning two lines coincide \\begin{align*} x-y = -3 \\\\ 2x-2y = -6 \\end{align*} The system has infinitely many solutions, as one of them is a linear combination of another. In this case, the second equation is scaled by 2 version of the first equation. In [7]: import pandas as pd import matplotlib.pyplot as plt % matplotlib inline # Dataframe for first equation df_1 = pd . DataFrame ({ 'x' : range ( - 10 , 11 )}) df_1 [ 'y' ] = df_1 [ 'x' ] - 3 # Dataframe for second equation df_2 = pd . DataFrame ({ 'x' : range ( - 5 , 6 )}) df_2 [ 'y' ] = df_2 [ 'x' ] - 3 plt . figure ( num = None , figsize = ( 8 , 6 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) plt . plot ( df_1 [ 'x' ], df_1 [ 'y' ]) plt . plot ( df_2 [ 'x' ], df_2 [ 'y' ]) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . grid () plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . annotate ( '2x - 2y = -6' , ( 5 , 5 ), weight = 'bold' ) plt . annotate ( 'x - y = -3' , ( - 5 , - 5 ), weight = 'bold' ) plt . show () Vectors Vectors and Vector Notation In plain English, the vector is a directed arrow. Mathematically, the vector is an object that has magnitude and direction. Magnitude is the length of the vector and direction is from its tail to its end. In other words, imagine vector as the line which connects two points in the Cartesian Coordinate System. A vector of length $n$ is a sequence or array of $n$ numbers, which we can write as: \\begin{align*} \\vec{X} = (x_1, x_2, x_3...x_n) \\end{align*} or \\begin{align*} \\vec{X} = [x_1, x_2, x_3, ... x_n] \\end{align*} Horizontally represented vector is a row vector, while vertically represented vector is a column vector. Let see how they look graphically. In [8]: import matplotlib.pyplot as plt fig , ax = plt . subplots ( figsize = ( 10 , 8 )) # Set the axes through the origin for spine in [ 'left' , 'bottom' ]: ax . spines [ spine ] . set_position ( 'zero' ) for spine in [ 'right' , 'top' ]: ax . spines [ spine ] . set_color ( 'none' ) ax . set ( xlim = ( - 6 , 6 ), ylim = ( - 6 , 6 )) ax . grid () vecs = (( 2 , 4 ), ( - 3 , 3 )) # These are vectors for v in vecs : ax . annotate ( '' , xy = v , xytext = ( 0 , 0 ), arrowprops = dict ( facecolor = 'blue' , shrink = 0 , alpha = 0.7 , width = 0.5 )) ax . text ( 1.1 * v [ 0 ], 1.1 * v [ 1 ], str ( v )) plt . show () Dimensions of Vector The dimension of vector is the number of elements in it. For example, the above vector is row vector with dimension $1\\times n$, but if we take it as a column vector its dimension will be $n\\times 1$. \\begin{align*} \\vec{X} = [x_1,x_2,...x_n]_{1\\times~n} \\end{align*} and \\begin{align*} \\vec{X} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}_{n\\times1} \\end{align*} Operations on Vectors The most common operations on vectors are vector addition/subtraction and scalar multiplication. If we have two vectors, $\\vec{X}$ and $\\vec{Y}$, we can add them up in the following way: \\begin{align*} \\vec{X} + \\vec{Y} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}_{n\\times1} + \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}_{n\\times1} = \\begin{bmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{bmatrix}_{n\\times1} \\end{align*} Multiplying vector by a scalar $\\alpha$, gives \\begin{align*} \\alpha \\vec{X} = \\begin{bmatrix} \\alpha x_1 \\\\ \\alpha x_2 \\\\ \\vdots \\\\ \\alpha x_n \\end{bmatrix}_{n\\times1} \\end{align*} For example: \\begin{align*} \\vec{X} + \\vec{Y} = \\begin{bmatrix} 1 \\\\ 2 \\\\ \\vdots \\\\ 100 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 4 \\\\ \\vdots \\\\ 400 \\end{bmatrix} = \\begin{bmatrix} 1 + 3 \\\\ 2 + 4 \\\\ \\vdots \\\\ 100 + 400 \\end{bmatrix} \\end{align*} For scalar multiplication, if we have vector $X$ and scalar $\\alpha = 5$ then alpha times $X$ is: \\begin{align*} 5 * \\vec{X} = \\begin{bmatrix} 5 * 1 \\\\ 5 * 2 \\\\ \\vdots \\\\ 5 * 100 \\end{bmatrix} \\end{align*} Vector Length The vector length or magnitude is calculated by the following formula: \\begin{align*} \\|\\vec{X}\\| = \\sqrt{x_1&#94;2 + x_2&#94;2 + x_3&#94;2 + ... + x_n&#94;2} = \\sqrt{\\sum_{i=1}&#94;n x_i&#94;2} \\end{align*} We can link notion of vector length to the Euclidean distance. If our vector $\\vec{X}$ has tail at origin, $\\vec{0} = [0_1, 0_2, 0_3, ... , 0_n]$ and point at $\\vec{X} = [x_1,x_2,...x_n]$, then Euclidean distance between tail and point is the length of $\\vec{X}$ by the formula: \\begin{align*} d(\\vec{0},\\vec{X}) = \\sqrt{(0_1 - x_1)&#94;2 + (0_2 - x_2)&#94;2 + ... + (0_n - x_n)&#94;2} = \\sqrt{\\sum_{i=1}&#94;n (0_i - x_i)&#94;2} \\end{align*} For example, we have vector $X$ \\begin{align*} \\vec{X} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\end{align*} then its length is \\begin{align*} \\|\\vec{X}\\| = \\sqrt{1&#94;2 + 2&#94;2 + 3&#94;2} = \\sqrt{14} \\end{align*} Unit Vector What if the length of a vector equal to 1? This kind of vector is known as unit vector and it plays a very important role in different calculations and formulae. We'll see it in later posts, but here unit vector is defined as \\begin{align*} \\hat{X} = \\frac{X}{\\|X\\|} \\end{align*} where $\\hat{X}$ is a unit vector, the numerator is vector $\\vec{X}$ and denominator is the norm of vector $\\vec{X}$. We can use vector $X$ from above example. We already calculated it length which is $\\|\\vec{X}\\| = \\sqrt{14}$. So, we can construct unit vector $\\hat{X}$ in the following way: \\begin{align*} \\hat{X} = \\frac{X}{\\|X\\|} = \\frac{1}{\\sqrt{14}}; \\frac{2}{\\sqrt{14}}; \\frac{3}{\\sqrt{14}} \\Rightarrow [0.26726124; \\ 0.53452248; \\ 0.80178373] \\end{align*} Scalar Product of Two Vectors Multiplication of two vectors is known as dot product, scalar product, or inner product and is defined by: \\begin{align*} \\langle\\, \\vec{X},\\vec{Y}\\rangle~=~\\vec{X}\\cdot\\vec{Y}~=~x_1\\times y_1 + x_2\\times y_2 + ... + x_n\\times y_n~=~\\sum_{i=1}&#94;n x_i\\cdot y_i \\end{align*} The inner product is defined only when the dimensions of two vectors coincide. Another formula of inner product is: \\begin{align*} \\vec{X}\\cdot\\vec{Y}~=~\\|\\vec{X}\\|\\cdot\\|\\vec{Y}\\|\\cdot\\cos{\\theta} \\end{align*} where $\\cos{\\theta}$ is an angle between the vectors $\\vec{X}$ and $\\vec{Y}$. Numerical Representation In [9]: import numpy as np # It's more convenient to represent the vector as a Numpy ndarray, rather than Python tuple. X = np . array ([ 1 , 2 , 3 ]) Y = np . array (( 2 , 4 , 6 )) print ( 'X is:' , type ( X )) print ( 'The dimension of X is:' , X . shape ) X is: The dimension of X is: (3,) Vector Addition In [10]: print ( X + Y ) [3 6 9] In [11]: # I use two dimensional vectors to show graphically the vector addition v = np . array (( 2 , 1 )) w = np . array (( - 3 , 2 )) V = np . array ([ w , v , v + w ]) origin = [ 0 ], [ 0 ] plt . axis ( 'equal' ) plt . axhline ( color = 'black' ) plt . axvline ( color = 'black' ) plt . grid () plt . ticklabel_format ( style = 'sci' , axis = 'both' , scilimits = ( 0 , 0 )) plt . quiver ( * origin , V [:, 0 ], V [:, 1 ], color = [ 'r' , 'b' , 'g' ], scale = 10 ) plt . show () Red is vector $\\vec{V}$, blue is $\\vec{W}$ and green is the sum of these two vectors Scalar Multiplication In [12]: import numpy as np X = np . array ([ 1 , 2 , 3 ]) print ( 5 * X ) [ 5 10 15] Vector Length In [13]: import numpy as np X = np . array ([ 1 , 2 , 3 ]) # In numpy, there are two ways to compute vector length print ( np . sqrt ( np . sum ( X ** 2 ))) # More verbose method print () print ( np . linalg . norm ( X )) 3.7416573867739413 3.7416573867739413 Unit Vector In [14]: import numpy as np X = np . array ([ 1 , 2 , 3 ]) n = X / np . linalg . norm ( X ) print ( 'Vector n =' , n ) Vector n = [0.26726124 0.53452248 0.80178373] In [15]: # If we take length of vector n, we'll get 1 print ( 'The length of vector n is:' , np . linalg . norm ( n )) The length of vector n is: 1.0 Scalar Product In [16]: import numpy as np # It's more convenient to represent the vector as a Numpy ndarray, rather than Python tuple. X = np . array ([ 1 , 2 , 3 ]) Y = np . array (( 2 , 4 , 6 )) # Numpy have two possible ways to compute vector inner product scalar_prod = np . sum ( X * Y ) dot_prod = np . dot ( X , Y ) print ( 'Scalar Product is:' , dot_prod ) # Compute angle between vector X and vector Y cosine_theta = np . dot ( X , Y ) / ( np . linalg . norm ( X ) * np . linalg . norm ( Y )) # Angle theta theta = np . arccos ( cosine_theta ) print ( 'Cosine theta is:' , cosine_theta ) print ( 'Angle theta is:' , theta ) Scalar Product is: 28 Cosine theta is: 1.0 Angle theta is: 0.0 Matrix Matrices and Matrix Notation Matrix is a rectangular array of numbers and/or expressions that are arranged into rows and columns. These rows and columns can be considered as row and column vectors. So, the matrix is the rectangular array which contains either row or column vectors. Generally, capital letters are used to denote matrix and lower case letters to denote each element of that matrix and I will follow this convention. A matrix arranges numbers into rows and columns, like this: \\begin{align*}A = \\begin{bmatrix} a_{1,1} & a_{1,2}\\\\ a_{2,1} & a_{2,2} \\end{bmatrix} \\end{align*} Here, matrix $A$ has four elements, denoted by lower letter $a$, where subscripts denote row and column number. For example, $a_{2,1}$ denotes element at the cross of the second row and first column. Dimension of Matrices If a matrix $A$ has $n$ rows and $m$ columns, we call $A$ an $n\\times m$ matrix and it is read as \"n by m matrix\" . A typical $n\\times m$ matrix $A$ can be written as: \\begin{align*} A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1m} \\\\ a_{21} & a_{22} & \\cdots & a_{2m} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nm} \\end{bmatrix}_{n \\times m} \\end{align*} When $n=m$ we have square matrix. To link this matrix to the vector we can rewrite it by the following way: \\begin{align*} A = \\begin{bmatrix} [a_{11} & a_{12} & \\cdots & a_{1m}] \\\\ [a_{21} & a_{22} & \\cdots & a_{2m}] \\\\ [\\vdots & \\vdots & & \\vdots] \\\\ [a_{n1} & a_{n2} & \\cdots & a_{nm}] \\end{bmatrix}_{n \\times m} \\end{align*} Matrix Operations We add two matrices elementwise. To exist this addition we require that the dimensions of the two matrices coincide. If we have two matrices, $A$ and $B$, addition is defined by: \\begin{align*} A + B = \\begin{bmatrix} a_{11} & \\cdots & a_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{n1} & \\cdots & a_{nm} \\\\ \\end{bmatrix} + \\begin{bmatrix} b_{11} & \\cdots & b_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ b_{n1} & \\cdots & b_{nm} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} + b_{11} & \\cdots & a_{1m} + b_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{n1} + b_{n1} & \\cdots & a_{nm} + b_{nm} \\\\ \\end{bmatrix} \\end{align*} Matrix subtraction is defined in the same fashion as the addition. The nagative of a matrix, is just a matrix with the sign of each element reversed: \\begin{align*} A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1m} \\\\ a_{21} & a_{22} & \\cdots & a_{2m} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nm} \\end{bmatrix} \\end{align*}\\begin{align*} -A = \\begin{bmatrix} -a_{11} & -a_{12} & \\cdots & -a_{1m} \\\\ -a_{21} & -a_{22} & \\cdots & -a_{2m} \\\\ \\vdots & \\vdots & & \\vdots \\\\ -a_{n1} & -a_{n2} & \\cdots & -a_{nm} \\end{bmatrix} \\end{align*} Multiplying matrices is a little more complex than the operations we've seen so far. There are two cases to consider. One is scalar multiplication (multiplying a matrix by a single number), and second is matrix multiplication (multiplying a matrix by another matrix). If we have some scalar or number $\\gamma$ and matrix $A$, scalar multiplication is: \\begin{align*} \\gamma A = \\gamma \\begin{bmatrix} a_{11} & \\cdots & a_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{n1} & \\cdots & a_{nm} \\\\ \\end{bmatrix} = \\ \\begin{bmatrix} \\gamma a_{11} & \\cdots & \\gamma a_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ \\gamma a_{n1} & \\cdots & \\gamma a_{nm} \\\\ \\end{bmatrix} \\end{align*} To multiply two matrices, we take inner product of $i$-th row of the matrix $A$ and $j$-th columns of matrix $B$ . If we have two matrices $A$ is $n \\times k$ and $B$ is $j \\times m$, then to multiply $A$ and $B$, we require $k=j$, and resulting matrix $AB$ is $n \\times m$. \\begin{align*} A \\cdot B = \\begin{bmatrix} a_{11} & \\cdots & a_{1k} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{n1} & \\cdots & a_{nk} \\\\ \\end{bmatrix}_{n\\times k} \\cdot \\begin{bmatrix} b_{11} & \\cdots & b_{1m} \\\\ \\vdots & \\vdots & \\vdots \\\\ b_{j1} & \\cdots & b_{jm} \\\\ \\end{bmatrix}_{j \\times m} = \\ \\begin{bmatrix} (a_{11} \\times b_{11} & +~\\cdots~+ & a_{1k} \\times b_{j1}),~\\cdots~,(a_{11} \\times b_{1m} & +~\\cdots~+ & a_{1k} \\times b_{jm}) \\\\ \\vdots & \\vdots & \\vdots \\\\ (a_{n1} \\times b_{11} & +~\\cdots~+ & a_{nk} \\times b_{j1}),~\\cdots~,(a_{n1} \\times b_{1m} & +~\\cdots~+ & a_{nk} \\times b_{jm})\\\\ \\end{bmatrix}_{n \\times m} \\end{align*} If you did not catch the idea of matrix multiplication don't worry. In numerical representation section I will provide simpler example and some extra source to take a look. Note that, in matrix multiplication, $A \\cdot B$ is not same as $B \\cdot A$. Matrix Transpose In the above example, we saw that the matrix is the collection of vectors. We also know that vectors can be horizontal as well as vertical, or row and column vectors. Now, what if we change in any matrix row vectors into column vectors? This operation is known as transposition. The idea of this operation is to change matrix rows into matrix columns or vice versa, and is denoted by the superscript $T$. \\begin{align*} A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1m} \\\\ a_{21} & a_{22} & \\cdots & a_{2m} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nm} \\end{bmatrix}_{n \\times m} \\end{align*} Then \\begin{align*} A&#94;{T} = \\begin{bmatrix} a_{11} & a_{21} & \\cdots & a_{n1} \\\\ a_{12} & a_{22} & \\cdots & a_{n2} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{1m} & a_{2m} & \\cdots & a_{nm} \\end{bmatrix}_{m \\times n} \\end{align*} Identity Matrix There are several different types of matrices. In this post, we will introduce only identity matix . Future post will introduce other types of matrices. An identity matrix (usually indicated by a capital $I$) is the equivalent in matrix terms of the number 1. It always has the same number of rows as columns, and it has the value 1 in the diagonal element positions I 1,1 , I 2,2 , etc; and 0 in all other element positions. Here's an example of a $3 \\times 3$ identity matrix: \\begin{align*} I = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}_{3 \\times 3} \\end{align*} Multiplying any matrix by an identity matrix is the same as multiplying a number by 1; the result is the same as the original value: Numerical Representation Dimension of a Matrix In [17]: import numpy as np # 3x3 matrix in Numpy A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) print ( 'Matrix A is' , A , sep = ' \\n ' ) print () print ( 'Dimensions of A is:' , A . shape ) Matrix A is [[1 2 3] [4 5 6] [7 8 9]] Dimensions of A is: (3, 3) Matrix Addition and Subtraction In [18]: import numpy as np A = np . array ([[ 1 , 5 , 3 ],[ 4 , 2 , 8 ],[ 3 , 6 , 9 ]]) B = np . array ([[ 1 , 1 , 3 ],[ 1 , 2 , 8 ],[ 0 , 5 , 3 ]]) A + B A - B print ( 'Matrix A is:' , A , sep = ' \\n ' ) print () print ( 'Matrix B is:' , B , sep = ' \\n ' ) print () print ( 'The sum of them is:' , A + B , sep = ' \\n ' ) print () print ( 'The difference of them is:' , A - B , sep = ' \\n ' ) Matrix A is: [[1 5 3] [4 2 8] [3 6 9]] Matrix B is: [[1 1 3] [1 2 8] [0 5 3]] The sum of them is: [[ 2 6 6] [ 5 4 16] [ 3 11 12]] The difference of them is: [[0 4 0] [3 0 0] [3 1 6]] Nergative Matrix In [19]: import numpy as np C = np . array ([[ - 5 , - 3 , - 1 ],[ 1 , 3 , 5 ]]) - C print ( 'Matrix C is:' , C , sep = ' \\n ' ) print () print ( 'The negative of C is:' , - C , sep = ' \\n ' ) Matrix C is: [[-5 -3 -1] [ 1 3 5]] The negative of C is: [[ 5 3 1] [-1 -3 -5]] Matrix - Scalar Multiplication In [20]: import numpy as np A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) scalar = 2 scalar * A print ( 'Matrix A is:' , A , sep = ' \\n ' ) print () print ( 'Scalar is:' , scalar , sep = ' \\n ' ) print () print ( 'Matrix multiplied by the scalar is:' , scalar * A , sep = ' \\n ' ) Matrix A is: [[1 2 3] [4 5 6] [7 8 9]] Scalar is: 2 Matrix multiplied by the scalar is: [[ 2 4 6] [ 8 10 12] [14 16 18]] As I mentioned earlier, matrix multiplication seems to be tricky at a first glance. Let's look at an example: \\begin{align*} \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix} 9 & 8 \\\\ 7 & 6 \\\\ 5 & 4 \\end{bmatrix} \\end{align*} Note that the first matrix is $2\\times 3$, and the second matrix is $3\\times 2$. The important thing here is that the first matrix has two rows, and the second matrix has two columns. To perform the multiplication, we first take the dot product of the first row of the first matrix (1,2,3) and the first column of the second matrix (9,7,5): \\begin{align*} (1,2,3) \\cdot (9,7,5) = (1 \\times 9) + (2 \\times 7) + (3 \\times 5) = 38 \\end{align*} In our resulting matrix (which will always have the same number of rows as the first matrix, and the same number of columns as the second matrix), we can enter this into the first row and first column element: \\begin{align*} \\begin{bmatrix} 38 & ?\\\\? & ? \\end{bmatrix} \\end{align*} Now we can take the dot product of the first row of the first matrix and the second column of the second matrix: \\begin{align*} (1,2,3) \\cdot (8,6,4) = (1 \\times 8) + (2 \\times 6) + (3 \\times 4) = 32 \\end{align*} Let's add that to our resulting matrix in the first row and second column element: \\begin{align*} \\begin{bmatrix} 38 & 32\\\\? & ? \\end{bmatrix} \\end{align*} Now we can repeat this process for the second row of the first matrix and the first column of the second matrix: \\begin{align*} (4,5,6) \\cdot (9,7,5) = (4 \\times 9) + (5 \\times 7) + (6 \\times 5) = 101 \\end{align*} Which fills in the next element in the result: \\begin{align*} \\begin{bmatrix} 38 & 32\\\\101 & ? \\end{bmatrix} \\end{align*} Finally, we get the dot product for the second row of the first matrix and the second column of the second matrix: \\begin{align*} (4,5,6) \\cdot (8,6,4) = (4 \\times 8) + (5 \\times 6) + (6 \\times 4) = 86 \\end{align*} Giving us: \\begin{align*} \\begin{bmatrix} 38 & 32\\\\101 & 86 \\end{bmatrix} \\end{align*} If this is not enough to catch the idea, take a look this explanation Matrix - Matrix Multiplication In [21]: # In Numpy matrix multiplication can be done with no effort import numpy as np A = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) B = np . array ([[ 9 , 8 ], [ 7 , 6 ], [ 5 , 4 ]]) print ( 'A * B is:' , np . dot ( A , B ), sep = ' \\n ' ) print () print ( 'B * A is:' , np . dot ( B , A ), sep = ' \\n ' ) A * B is: [[ 38 32] [101 86]] B * A is: [[41 58 75] [31 44 57] [21 30 39]] Matrix Transpose In [22]: import numpy as np A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) A . T print ( 'Matix A is:' , A , sep = ' \\n ' ) print () print ( 'Transpose of A is:' , A . T , sep = ' \\n ' ) Matix A is: [[1 2 3] [4 5 6] [7 8 9]] Transpose of A is: [[1 4 7] [2 5 8] [3 6 9]] Matrix - Identity Matrix Multiplication In [23]: # We have two ways to define identity matrix in Numpy. # First is to define by hand, like above examples, and second is to use Numpy's buildin function import numpy as np I_1 = np . array ([[ 1. , 0. , 0. ],[ 0. , 1. , 0. ],[ 0. , 0. , 1. ]]) I_2 = np . identity ( 3 ) print ( I_1 ) print () print ( I_2 ) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]] [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]] In [24]: # Matrix identity multiplication A = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]]) I = np . identity ( 3 ) print ( 'A = ' , A , sep = ' \\n ' ) print ( 'A * I = ' , np . dot ( A , I ), sep = ' \\n ' ) A = [[1 2 3] [4 5 6] [7 8 9]] A * I = [[1. 2. 3.] [4. 5. 6.] [7. 8. 9.]] Conclusion In this post, I tried to cover the basics of linear algebra. I depicted some theory with examples solved by hand as well as with Numpy. I do hope, this blog will help you to grab the necessary knowledge in linear algebra basics and further gives you the direction where to dig deeper. I did not provide here further resources due to not to confuse the reader and give freedom to look for some other materials. If some part of the blog is someway unclear to you, please comment below. References Vector Vector ) Matrix Matrix ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/articles/mathematics/basics-of-linear-algebra-with-python.html","loc":"https://dsfabric.org/articles/mathematics/basics-of-linear-algebra-with-python.html"},{"title":"Introduction to Linear Algebra with Python","text":"These are the series of linear algebra mainly based on Numpy and Scipy . The series will follow the sequence: Introduction Basics of linear algebra Intermediate linear algebra Advances in linear algebra Concluding remarks Linear algebra is one of the building block of data science among others. Its importance is huge, as all supervised, unsupervised and semi-supervised algorithms use it with some degree. One great example is Google's famous Page Rank algorithm, which heavily relies on it. And that is not all. We can find as many usage cases of linear algebra as many individuals exist in the data science field. The purpose of this blog series is to introduce you the ways how to use linear algebra in data science. More precisely, by this series, I intend to help aspiring data scientist to refresh their linear algebra knowledge with Python and gain some hands-on experience. Moreover, this may serve you as a starting point to dig deeper into an amazing world of linear algebra with Python. Let start by defining what is linear algebra Linear algebra is a branch of mathematics that is concerned with mathematical structures, closed under addition and scalar multiplication operations and that includes the theory of systems of linear equations, matrices, determinants, vector spaces, and linear transformations. Let's start explaining word by word the definition of linear algebra. Loosely speaking, mathematical structure is a set, together with a family of operations and relations defined on that set. Now divide \"Closed under addition and scalar multiplication\" into two parts. First is \"closed under addition\" which means that a set is \"closed under addition\" if the sum of any two members of this set belongs to this set again. For example, imagine the set of even integers. Then, take any to integer and add them up. The result is an even integer belonging to the initial set. Here is a mathematical definition. \\begin{align*} A=\\{x \\in \\mathbb{Z}~\\vert~mod~2 =0\\} \\end{align*} Second is \"closed under scalar multiplication\". This means that the product of any member of the set and any scalar $\\alpha$ such that $\\alpha~\\in \\mathbb{R}$ is also in the set. The above-mentioned set is also closed under scalar multiplication. Generally, the sets $\\mathbb{N}, ~ \\mathbb{Z}, ~ \\mathbb{Q}$ and $\\mathbb{R}$ are closed under both addition and multiplication. The set \\begin{align*} A = (0,1) \\end{align*} is closed under multiplication, but not addition. $(0.6 + 0.7 = 1.3 > 1)$ The set of all half integers \\begin{align*} \\frac{\\mathbb{Z}}{2}=\\{x : \\exists~{y}~\\in~\\mathbb{Z}~(x = \\frac{y}{2})\\} \\end{align*} is closed under addition, but not under multiplication. $(0.5 * 0.5 = 0.25~\\notin~\\frac{\\mathbb{Z}}{2})$ The system of linear equations is a collection of two or more linear equations involving the same set of variables. The example is the following: \\begin{align*} \\begin{cases} 3x + 2y - z = 1 \\\\\\ 2x - 2y + 4z = -2 \\\\\\ -x + \\frac{1}{2}y - z = 0 \\end{cases} \\end{align*} Matrix ) is just a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. There is no plain English definition of the determinant, but forthcoming blogs in this series will cover it in detail. Vector space is a set of objects (vectors) closed under finite vector addition and scalar multiplication. Linear transformation or linear map is a mapping between two vector spaces that preserves the addition and scalar multiplication rule. More mathematically, a linear transformation between two vector space $V$ and $W$ is a map $T : V~\\rightarrow~W$ such that the following hold: $T(v_1 + v_2) = T(v_1) + T(v_2)$ for any vectors $v_1$ and $v_2$ in $V$ $T(\\alpha v_1) = \\alpha T(v_1)$ for any scalar $\\alpha$ Conclusion To sum up, this post is an introduction towards linear algebra series, where I will introduce you linear algebra concepts intuitively and programmatically in Python. The main idea of this series is to feel comfortable in the field and to give you the direction where to dig deeper. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"https://dsfabric.org/articles/mathematics/introduction-to-linear-algebra-with-python.html","loc":"https://dsfabric.org/articles/mathematics/introduction-to-linear-algebra-with-python.html"},{"title":"Economics - Test - Test II","text":"Written with StackEdit . To do this project as I want I need to plan it carefully.","tags":"Economics","url":"https://dsfabric.org/articles/economics/economics-test-test-ii.html","loc":"https://dsfabric.org/articles/economics/economics-test-test-ii.html"},{"title":"StackEdit","text":"Written with StackEdit . Churn Rate Modeling import pandas as pd df = pd . read_excel ( 'nodo.xlsx' ) from sklearn import preprocessing This is my new definition of data science This is the simplest project description file for my project, which will be reedited to access the changes To do this project as I want I need to plan it carefully. Project steps Business Understanding Data Understanding Data Acquisition Modeling Evaluation Deployment Result Presentation Description of each step Business Understanding Data Understanding Data Acquisition Modeling Evaluation Deployment Result Presentation SmartyPants SmartyPants converts ASCII punctuation characters into \"smart\" typographic punctuation HTML entities. For example: ASCII HTML Single backticks 'Isn't this fun?' 'Isn't this fun?' Quotes \"Isn't this fun?\" \"Isn't this fun?\" Dashes -- is en-dash, --- is em-dash -- is en-dash, --- is em-dash KaTeX You can render LaTeX mathematical expressions using KaTeX : The Gamma function satisfying \\(\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N\\) is via the Euler integral $$ \\Gamma(z) = \\int_0&#94;\\infty t&#94;{z-1}e&#94;{-t}dt\\,. $$ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"General","url":"https://dsfabric.org/articles/general/stackedit.html","loc":"https://dsfabric.org/articles/general/stackedit.html"}]};